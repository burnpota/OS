======================<br><b>생성계정 : 삼성생명 타임게이트</b><br><b>생성날짜 : 2017-11-30T05:03:44Z</b><br><b>마지막 답변자 : Jay Shin</b><br><b>마지막 수정 일자 : 2017-12-14T22:30:23Z</b><br><b>id : 500A000000Z4bIDIAZ</b><br>======================<br><br><b><font size=15>
제목  : 물리메모리가 충분한 상황에서도 스왑이 발생되는 원인에 대한 문의
</font></b><br><br>======================<br><actionPlan>https://wiki.scn.sap.com/wiki/display/SAPHANA/Linux+Operating+System+with+SAP+HANA+Reference+Guide<br>  https://apps.support.sap.com/sap/support/knowledge/preview/en/1999997<br><br>  2009879 - SAP HANA Guidelines for Red Hat Enterprise Linux (RHEL) Operating System<br>  https://launchpad.support.sap.com/#/notes/0002009879<br><br>  2247020 - SAP HANA DB: Recommended OS settings for RHEL 6.7<br>  https://launchpad.support.sap.com/#/notes/2247020<br><br> Suggested to set vm.swappiness=1<br> Suggested to disable numa then see if issue is resolved.</actionPlan><b>사전문의<br></b><br>안녕하세요?<br>궁금한 사항이 있어 아래와 같이 문의 드립니다.<br><br>- 환경 : 메모리 4TB 장비(swap 30GB) , RHEL 6.7<br>- 현상 : 스왑 사용율 100% 수시 발생<br>※    실제 메모리 사용율은 85% 미만인 상태이며, 특정 서버 한대에서만 발생하고 있어서 원인이 좀 궁금한 상황입니다.<br> <br><br>[문의사항]<br>- cpu에서 사용중인 numa node 메모리에 여유량이 적은 경우에도 이처럼 swap 이 발생가능한건지,<br>  numa 환경에서 swap 발생 동작 확인 부탁드립니다.<br>  <br>1. 혹시 그렇다면, 현재 노드별로 메모리 점유하고 있는 세부 내용을 어떤 식으로 알수 있을까요? <br> <br>2. 다른 원인으로 physical 메모리가 충분한 경우에도 swap 이 발생하는 경우가 있는지 궁금합니다.<br> <br> <br>참고로 아래는 현재 numactl 로 확인한 메모리 상태 및 swap 상태입니다. (현재 스왑 100% 사용중)<br>root@pbwshl11sl /root # date;numactl -H<br>Wed Nov 29 17:47:56 KST 2017<br>available: 8 nodes (0-7)<br>node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161<br>node 0 size: 524098 MB<br>node 0 free: 245671 MB<br>node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179<br>node 1 size: 524287 MB<br>node 1 free: 38131 MB<br>node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197<br>node 2 size: 524288 MB<br>node 2 free: 9 MB ----------------&gt; 메모리 사용<br>node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215<br>node 3 size: 524288 MB<br>node 3 free: 9 MB ----------------&gt; 메모리 사용<br>node 4 cpus: 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233<br>node 4 size: 524288 MB<br>node 4 free: 69280 MB<br>node 5 cpus: 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251<br>node 5 size: 524288 MB<br>node 5 free: 152806 MB<br>node 6 cpus: 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269<br>node 6 size: 524288 MB<br>node 6 free: 89619 MB<br>node 7 cpus: 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287<br>node 7 size: 524288 MB<br>node 7 free: 92739 MB<br>node distances:<br>node   0   1   2   3   4   5   6   7<br>  0:  10  16  30  30  30  30  30  30<br>  1:  16  10  30  30  30  30  30  30<br>  2:  30  30  10  16  30  30  30  30<br>  3:  30  30  16  10  30  30  30  30<br>  4:  30  30  30  30  10  16  30  30<br>  5:  30  30  30  30  16  10  30  30<br>  6:  30  30  30  30  30  30  10  16<br>  7:  30  30  30  30  30  30  16  10 <br> <br>### SAR 결과<br>Linux 2.6.32-573.42.1.el6.x86_64 (pbwshl11sl)   11/29/17        _x86_64_        (288 CPU)<br>17:40:04    kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit<br>17:41:02    684186300 3550893008     83.84    508092   5326264 3553506824     83.28<br>17:42:03    689585776 3545493532     83.72    412908   3618816 3553286808     83.27<br>17:43:02    695953396 3539125912     83.57    413844   2496328 3553528728     83.28<br>17:44:02    696563452 3538515856     83.55    414400   2543292 3553454244     83.28<br>17:45:01    697430044 3537649264     83.53    414864   2552900 3553551480     83.28<br>17:46:03    701861356 3533217952     83.43    415924   2573272 3553421016     83.28<br>17:47:01    701026004 3534053304     83.45    415952   2573504 3553488648     83.28<br>17:48:02    703843112 3531236196     83.38    415988   2573812 3553484640     83.28<br>17:49:01    705240488 3529838820     83.35    416236   2609156 3552552848     83.25<br>17:50:01    751901508 3483177800     82.25    416288   2693404 3516612948     82.41<br>17:40:04    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad<br>17:41:02      7218592  24781208     77.44   4495752     18.14<br>17:42:03      3820608  28179192     88.06   4129800     14.66<br>17:43:02        20428  31979372     99.94   6154756     19.25<br>17:44:02        88944  31910856     99.72   6177660     19.36<br>17:45:01       126996  31872804     99.60   6178960     19.39<br>17:46:03       154524  31845276     99.52   6183356     19.42<br>17:47:01       177452  31822348     99.45   6178252     19.41<br>17:48:02       212972  31786828     99.33   6181108     19.45<br>17:49:01       232576  31767224     99.27   6181676     19.46<br>17:50:01       251312  31748488     99.21   6186044     19.48<br> <br> <br> <br>이상입니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.7</b><br><b>타입  : Other</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><hostname>pbwshl11sl</hostname><br><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-14T22:30:20Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-14T22:30:20Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>본 기술 문의의 상태를 처리완료 상태로 변경하고자 합니다.<br><br>만약 본 기술 문의와 관련하여 추가 질문이 있으시다면 언제든지 본 기술 문의를 *재개*하실 수 있습니다.<br><br>기술 문의가 처리 완료되면 *고객 설문조사* 메일이 발송됩니다. 고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해, 소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>만약 본 이슈와 관련하여 추가적인 문의 사항이 있으실 경우 연락 주시기 바랍니다.<br><br>감사합니다.<br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>GSS, Asia Pacific, Red Hat Inc<br><br><publishedDate>2017-12-14T22:30:20Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LC388IAD"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-11T05:06:21Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-11T05:06:21Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 입니다.<br><br>본 케이스와 연관되어 더 궁금한 사항이 있거나 추가적인 지원이 필요하신가요?<br><br>본 안내 후 일주일 이내에 별다른 답변이 없다면 자동으로 종료 상태가 된다는 것을 알려 드립니다.<br><br>만약 추가적인 지원이 필요하다면, 연락 부탁드립니다.<br><br>감사합니다.<br><br>Jay Shin<br>Technical Support Engineer<br>GSS, Asia Pacific, Red Hat Inc<br><br><publishedDate>2017-12-11T05:06:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LBT5ZIAX"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-07T06:29:05Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-07T06:29:05Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>sar29 일 데이터를 확인해보니, 05시 42분 경 block write 작업이 높아지면서 다량의 페이지가 요구되었으며,<br>요청한 페이지를 할당하는 과정에 pgscank/s 페이지 스캔 빈도가 높아졌으며, 이를 통해 페이지가 100% 할당되지 못해<br>스왑 사용이 증가한 것으로 확인됩니다.<br><br>Linux 2.6.32-573.42.1.el6.x86_64 (pbwshl11sl)   11/29/2017      _x86_64_        (288 CPU)<br><br>12:00:02 AM     CPU      %usr     %nice      %sys   %iowait    %steal      %irq     %soft    %guest     %idle<br>05:42:03 PM     all      6.39      0.00      1.69      0.04      0.00      0.00      0.17      0.00     91.71<br><br><br>12:00:02 AM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff<br>05:41:02 PM      2.80     42.16  31519.85      0.14 136344.05      0.00      0.00      0.00      0.00<br>05:42:03 PM   1306.78  37022.68  32824.98     33.17 165092.13  49038.28      0.00  20758.21     42.33  &lt;&lt;------ 페이지 스캔 비율이 높으며, 회수된 비율도 높지 않음.<br>05:43:02 PM   3398.44  61878.43  40002.61    116.80 174306.31  27013.09      0.00   8443.87     31.26<br><br><br>12:00:02 AM       tps      rtps      wtps   bread/s   bwrtn/s<br>05:41:02 PM     10.33      1.30      9.03     16.81    252.94<br>05:42:03 PM  14795.77    311.59  14484.18   7845.02 222251.37 &lt;&lt;----- 블럭 쓰기 작업 증가<br>05:43:02 PM  18402.35   1096.32  17306.04  20396.75 371149.90<br>05:44:02 PM    734.87    560.96    173.91  14038.74 152639.51<br><br><br>12:00:02 AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit<br>05:40:04 PM 688339436 3546739872     83.75    508092   5326120 3553601320     83.28<br>05:41:02 PM 684186300 3550893008     83.84    508092   5326264 3553506824     83.28<br>05:42:03 PM 689585776 3545493532     83.72    412908   3618816 3553286808     83.27  &lt;&lt;---- 캐시 반환<br>05:43:02 PM 695953396 3539125912     83.57    413844   2496328 3553528728     83.28<br><br><br>12:00:02 AM kbswpfree kbswpused  %swpused  kbswpcad   %swpcad<br>05:41:02 PM   7218592  24781208     77.44   4495752     18.14<br>05:42:03 PM   3820608  28179192     88.06   4129800     14.66 &lt;&lt;--- 스왑 사용량 증가가<br>05:43:02 PM     20428  31979372     99.94   6154756     19.25<br><br>12:00:02 AM   runq-sz  plist-sz   ldavg-1   ldavg-5  ldavg-15<br>05:40:04 PM        20      9205     18.69     17.91     19.26<br>05:41:02 PM        29      9203     19.63     18.27     19.29<br>05:42:03 PM        21      9203     22.41     19.42     19.63<br>05:43:02 PM        36      9220     24.55     20.57     20.01<br>05:44:02 PM        22      9203     23.06     20.91     20.16<br><br>12:00:02 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s<br>05:37:03 PM     bond2  61971.29  47007.86 479767.24 213876.09      0.00      0.00      0.00<br>05:38:02 PM     bond2  61975.48  50085.42 469681.73 250600.53      0.00      0.00      0.00<br>05:39:01 PM     bond2  56286.00  42813.12 434805.69 194538.61      0.00      0.00      0.00<br>05:40:04 PM     bond2  54610.08  42235.84 419789.24 194498.62      0.00      0.00      0.00<br>05:41:02 PM     bond2  59744.17  49099.28 448900.37 244901.51      0.00      0.00      0.00<br>05:41:02 PM  p12565p1  59744.17  49099.28 448900.37 244901.51      0.00      0.00      0.00<br><br>Linux 2.6.32-573.42.1.el6.x86_64 (pbwshl11sl)   11/30/2017      _x86_64_        (288 CPU)<br><br>12:00:01 AM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff<br>01:44:02 PM  89883.34   7209.54 101123.27      9.52 772102.82  18496.19      0.00  10754.37     58.14 &lt;&lt;--- 스캔 비율 증가 및 페이지 회수율 58%<br>01:45:01 PM 102163.43     46.33 118791.26      2.99 180622.38    860.01      0.00    838.73     97.53<br><br>12:00:01 AM       tps      rtps      wtps   bread/s   bwrtn/s<br>01:43:01 PM     85.50     73.71     11.80  75304.02   2219.52<br>01:44:02 PM   4620.80    609.29   4011.51 539300.03  43257.24<br>01:45:01 PM    658.67    645.94     12.72 613016.64    278.41<br><br>12:00:01 AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit<br>01:43:01 PM 544662864 3690416444     87.14    167324   7612408 3550733656     83.21<br>01:44:02 PM 690490864 3544588444     83.70    109736   4945004 3410434712     79.92 &lt;&lt;--- 캐시 반환<br>01:45:01 PM 685514460 3549564848     83.81     90616   2385952 3423096144     80.22<br>01:46:02 PM 673237952 3561841356     84.10     92328   2425388 3435819388     80.52<br><br>12:00:01 AM kbswpfree kbswpused  %swpused  kbswpcad   %swpcad<br>01:42:02 PM    988580  31011220     96.91   6378984     20.57<br>01:43:01 PM    988612  31011188     96.91   6378984     20.57<br>01:44:02 PM         0  31999800    100.00   6985148     21.83 &lt;&lt;--- 스왑 증가<br>01:45:01 PM      3256  31996544     99.99   6982012     21.82<br><br><br>ETHTOOL<br>  Interface Status:<br>    bond0                   link=up 10000Mb/s full (autoneg=N)  rx ring UNKNOWN    drv bonding v3.7.1 / fw 2<br>    bond1                   link=up 10000Mb/s full (autoneg=N)  rx ring UNKNOWN    drv bonding v3.7.1 / fw 2<br>    bond2                   link=up 10000Mb/s full (autoneg=N)  rx ring UNKNOWN    drv bonding v3.7.1 / fw 2<br>    p12561p1  0000:01:00.0  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12561p2  0000:01:00.1  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12563p1  0000:21:00.0  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12563p2  0000:21:00.1  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12565p1  0000:41:00.0  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12565p2  0000:41:00.1  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12567p1  0000:61:00.0  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12567p2  0000:61:00.1  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12817p1  0000:11:00.0  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12817p2  0000:11:00.1  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12819p1  0000:31:00.0  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12819p2  0000:31:00.1  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12821p1  0000:51:00.0  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12821p2  0000:51:00.1  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12823p1  0000:71:00.0  link=DOWN                           rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>    p12823p2  0000:71:00.1  link=up 10000Mb/s full (autoneg=Y)  rx ring 4078/4078  drv bnx2x v1.714.0 / fw bc 7.13.75<br>  Interface Errors:<br>    p12561p1  rx_brb_discard: 4363<br>    - - - - - - - - - - - - - - - - - - -<br>    p12565p1  [0]: rx_discards: 31   &lt;&lt;-------------------- active bond<br>              [1]: rx_discards: 345<br>              [2]: rx_discards: 1438<br>              [3]: rx_discards: 563<br>              [4]: rx_discards: 386<br>              [5]: rx_discards: 417<br>              [6]: rx_discards: 361<br>              [7]: rx_discards: 172<br>              rx_discards: 3713<br>              rx_brb_discard: 23959<br><br>$ cat sos_commands/networking/ethtool_-g_p12565p1 <br>Ring parameters for p12565p1:<br>Pre-set maximums:<br>RX:<br><br>4078<br>RX Mini:<br>0<br>RX Jumbo:<br>0<br>TX:<br><br>4078<br>Current hardware settings:<br>RX:<br><br>4078<br>RX Mini:<br>0<br>RX Jumbo:<br>0<br>TX:<br><br>4078<br><br>$ cat sos_commands/networking/ethtool_-k_p12565p1|grep offload<br>tcp-segmentation-offload: on<br>udp-fragmentation-offload: off [fixed]<br>generic-segmentation-offload: on<br>generic-receive-offload: on<br>large-receive-offload: on<br>rx-vlan-offload: on [fixed]<br>tx-vlan-offload: on<br><br><br>sar 분석 결과, 급작스런 major fault 에 의한 페이지 요청으로 페이지를 스캔하였으나 요청한 페이지를 적절하게 할당하지 못하고<br>스왑이 사용된 것으로 확인됩니다.<br><br>여유 공간이 있는데도 불구하고 스왑이 사용된 이유에 대해서는 앞서 설명드린바 있습니다.<br><br>요청드린 vm.swappiness = 1 설정 이후에도 문제가 지속되는지 확인이 필요하며,<br>문제가 지속된다면, 시스템 점검 기간에 numa 설정을 비활성화하여 테스트 진행이 가능할지도 확인부탁드립니다.<br><br>아울러, 해당 현상은 SAP 애플리케이션의 메모리 할당 정책과 관련이 있을 것으로 생각되며, SAP 측에도 해당 증상을 리포트하여<br>SAP 튜닝 등 관련 지원을 받아보시기 바랍니다.<br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-12-07T06:29:05Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LBS61IAH"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-12-07T04:07:20Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-12-07T04:07:20Z</b><br><br>해당 내용은 고객에게 전달하였고 확인하여 추가 답변드리겠습니다<br><br>추가로 요청주셨던 sar데이터 파일첨부드립니다<br><br>감사합니다<br><br><publishedDate>2017-12-07T04:07:20Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LBPjkIAH"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-06T22:42:19Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-06T22:42:19Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>앞서 안내해드린바와 같이 아래 설정이 도움이 될 수 있는지 확인해주시기 바랍니다.<br><br># echo 1 &gt; /proc/sys/vm/swappiness<br><br>시스템 점검기간에 numa 설정을 비활성화할 수 있는지 비활성화 이후에 메모리 사용에 문제가 없는지도 테스트해주시기 바랍니다.<br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-12-06T22:42:19Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L6yTWIAZ"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-06T06:24:02Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-06T06:24:01Z</b><br><br>&gt;&gt;&gt; COLLABORATION REQUEST &lt;&lt;&lt;<br> <br>ENVIRONMENT:<br>  Vend: HP    Vers: Bundle    Date: 03/08/2017    BIOS Rev: 8.7<br>      Prod: Superdome2 16s x86<br>  CPU:<br>    8 of 8 CPU sockets populated, 18 cores/36 threads per CPU<br>    144 total cores, 288 total threads<br>    Vers: Intel(R) Xeon(R) CPU E7-8880 v3 @ 2.30GHz<br>    Total: 4194304 MiB (4096 GiB)<br>    Transparent Hugepages : disabled<br>    io scheduler : noop<br>    numa_hit 271511505791<br>    numa_miss 81871112519<br>    tune-profiles/active-profile : sap-hana<br><br>    4038.9 GiB total ram<br>    3435.8 GiB (85%) used<br>    3425.5 GiB (85%) used excluding Buffers/Cached<br>    0.04 GiB (0%) dirty<br>  HugePages:<br>    No ram pre-allocated to HugePages<br><br>  LowMem/Slab/PageTables/Shmem:<br>    2.95 GiB (0%) of total ram used for Slab<br>    9.07 GiB (0%) of total ram used for PageTables<br>    0.1 GiB (0%) of total ram used for Shmem<br><br>SPECIFICS:<br>  High order were consumed and low order was not used mostly.<br>  Customer is concerning Swap was fully consumed even though there is lots of free memory.<br><br>RESEARCH:<br>  https://wiki.scn.sap.com/wiki/display/SAPHANA/Linux+Operating+System+with+SAP+HANA+Reference+Guide<br>  https://apps.support.sap.com/sap/support/knowledge/preview/en/1999997<br><br>  2009879 - SAP HANA Guidelines for Red Hat Enterprise Linux (RHEL) Operating System<br>  https://launchpad.support.sap.com/#/notes/0002009879<br><br>  2247020 - SAP HANA DB: Recommended OS settings for RHEL 6.7<br>  https://launchpad.support.sap.com/#/notes/2247020<br><br>STEPS TAKEN: <br> Provided to set vm.swappiness=1<br> Requesting sar data.<br> Disabling numa then see if issue is resolved.<br><br>SUPPORTING DATA:<br>  Attached to Case: sosreport <br><br>  Elsewhere:<br> <br>REQUESTED ACTION:<br>  Customer has raised a ticket : 592968<br><br><publishedDate>2017-12-06T06:24:01Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L6yRGIAZ"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-12-06T06:20:23Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-12-06T06:20:23Z</b><br><br>안녕하십니까 타임게이트 오선우입니다<br><br>SAP 진행 티켓번호는 아래와 같습니다.<br>- 번호 : 592968<br><br>sar데이터는 곧 업로드 하도록 하겠습니다<br><br>감사합니다<br><br><publishedDate>2017-12-06T06:20:23Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000L6vV2IAJ"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-05T23:55:45Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-05T23:55:45Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>남아있는 페이지들을 합산한 결과 603 GB 의 여유공간이 있는 것으로 확인됩니다.<br>$ $ buddyfrag.sh |grep Total | awk '{sum+=$2} END {print sum*4096/1024/1024/1024 &quot;GB&quot;}'<br>603.163GB<br><br>하지만, 앞선 분석 결과에서 나온 것처럼, 높은 용량의 페이지들이 대부분 모두 소진되었고, 낮은 용량의 페이지들은 대부분 사용되지 않고 있습니다.<br>이는 애플리케이션의 페이지 요청에 기인하는 것으로, SAP 애플리케이션에서 구현된 페이지 요청 사이즈가 대용량 페이지를 요청하는 것으로 생각할 수 있습니다.<br><br>높은 용량의 페이지를 다량 요청하였을 경우, 해당 용량의 페이지가 가용하지 않는 경우 순차적으로 낮은 용량*2 의 용량을 찾게되며, 이는 buddy 알고리즘을<br>이용한 커널의 메모리 관리 방식입니다. 허나, 연속적인 페이지가 존재하지 않는 경우 다시 낮은 용량*2 의 용량을 찾게되며, 순차적으로 낮은 용량을 찾아가는 동안<br>스왑 사용에 대한 계산식에서 높은 distress 를 산출하는 요인이 됩니다.<br><br>swap_tendency 값이 100이 되는 순간부터 스왑 사용이 시작되며, 이 값은 커널이 실시간으로 자동 계산하게 되며, 사용자가 변경할 수 있는 값은,<br>vm.swappiness 값을 설정하는 것만 가능합니다.<br><br>What does swappiness do and how does it affect swap_tendency? <br>https://access.redhat.com/solutions/103833<br><br>swap_tendency = mapped_ratio/2 + distress + vm_swappiness;<br><br>현재 해당 값이 60(os 설치시 기본값)으로 설정되어있으며, 이 값을 1로 설정하여 스왑 사용을 자제하게 할 수 있습니다.<br><br>다음은 numa 입니다만, 아래 /proc/zoneinfo 에서 확인한 값을 보면, hit 대비 miss 가 상당히 많은 것으로 확인됩니다.<br>많게는 99% hit, 적게는 33% hit 입니다.<br><br>numa_hit<br>59066668261<br>82.2<br>numa_miss<br>12798034566<br>17.8<br>numa_hit<br>22694300916<br>88.8<br>numa_miss<br>2859662272<br>11.2<br>numa_hit<br>22643829551<br>89.6<br>numa_miss<br>2632678361<br>10.4<br>numa_hit<br>19098114625<br>91.3<br>numa_miss<br>1808597788<br>8.7<br>numa_hit<br>88757849089<br>99.0<br>numa_miss<br>862142155<br>1.0<br>numa_hit<br>15703586274<br>33.9<br>numa_miss<br>30649178465<br>66.1<br>numa_hit<br>19988676212<br>55.4<br>numa_miss<br>16077557992<br>44.6<br>numa_hit<br>23558049303<br>62.4<br>numa_miss<br>14183260917<br>37.6<br><br>$ cat sos_commands/numa/numactl_--hardware <br>available: 8 nodes (0-7)<br>node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161<br>node 0 size: 524098 MB<br>node 0 free: 143232 MB<br>node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179<br>node 1 size: 524287 MB<br>node 1 free: 45741 MB<br>node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197<br>node 2 size: 524288 MB<br>node 2 free: 179225 MB<br>node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215<br>node 3 size: 524288 MB<br>node 3 free: 9489 MB<br>node 4 cpus: 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233<br>node 4 size: 524288 MB<br>node 4 free: 65767 MB<br>node 5 cpus: 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251<br>node 5 size: 524288 MB<br>node 5 free: 144254 MB<br>node 6 cpus: 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269<br>node 6 size: 524288 MB<br>node 6 free: 13502 MB<br>node 7 cpus: 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287<br>node 7 size: 524288 MB<br>node 7 free: 16195 MB<br>node distances:<br>node   0   1   2   3   4   5   6   7 <br>  0:  10  16  30  30  30  30  30  30 <br>  1:  16  10  30  30  30  30  30  30 <br>  2:  30  30  10  16  30  30  30  30 <br>  3:  30  30  16  10  30  30  30  30 <br>  4:  30  30  30  30  10  16  30  30 <br>  5:  30  30  30  30  16  10  30  30 <br>  6:  30  30  30  30  30  30  10  16 <br>  7:  30  30  30  30  30  30  16  10<br><br>앞서 보내주신 numa_ctl 과 위 sosreport 데이터에서의 numa_ctl 에서는 <br>특정 노드에 편향된 메모리 사용이 보여지고 있는 것처럼 보여집니다.<br><br>따라서, numa 를 비활성화하고도 문제가 지속되는지도 확인이 필요할 것으로 판단됩니다.<br><br>How to disable NUMA in Red Hat Enterprise Linux system? <br>https://access.redhat.com/solutions/23216<br><br>또한, 아래 문서들을 통해서 튜닝 설정을 점검해볼 수도 있습니다.<br><br>  https://apps.support.sap.com/sap/support/knowledge/preview/en/1999997<br><br>  2009879 - SAP HANA Guidelines for Red Hat Enterprise Linux (RHEL) Operating System<br>  https://launchpad.support.sap.com/#/notes/0002009879<br><br>  2247020 - SAP HANA DB: Recommended OS settings for RHEL 6.7<br>  https://launchpad.support.sap.com/#/notes/2247020<br><br>참고로, SAP 시스템에 hugepages 설정은 권장되지 않는 것으로 확인됩니다.<br><br>SAP 측에 티켓 번호를 올려주시면 내부적으로 SAP 와 연계하여 기술 지원이 가능할지도 확인해보도록 하겠습니다.<br><br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-12-05T23:55:45Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L6dRfIAJ"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-05T03:59:55Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-05T22:36:35Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>일단 올려주신 sosreport 를 확인한 결과, 말씀하신대로 메모리 여유가 있는데도 불구하고 swap을 사용중인 것으로 확인됩니다.<br>SAP 애플리케이션에서 3.6T 의 메모리를 사용중이며, 32GB 의 swap 이 거의 모두 소진 된 것으로 확인됩니다.<br>대부분 128KB 에서 high order 인 4MB 까지 블럭이 소모되었으며, 낮은 블럭에서는 거의 소모가 되지 않았습니다.<br><br><br>DMIDECODE<br>  BIOS:<br>    Vend: HP    Vers: Bundle    Date: 03/08/2017    BIOS Rev: 8.7<br>  System:<br>    Prod: Superdome2 16s x86<br>  CPU:<br>    8 of 8 CPU sockets populated, 18 cores/36 threads per CPU<br>    144 total cores, 288 total threads<br>    Vers: Intel(R) Xeon(R) CPU E7-8880 v3 @ 2.30GHz<br>  Memory:<br>    Total: 4194304 MiB (4096 GiB)<br><br>OS<br>  Hostname: pbwshl11sl<br>  Distro:   [redhat-release] Red Hat Enterprise Linux Server release 6.7 (Santiago)<br>  Kernel:<br>    Booted kernel:  2.6.32-573.42.1.el6.x86_64<br>    Booted kernel cmdline:<br>      ro root=UUID=b03b45c7-1d10-4e64-89ea-e4f1fc9040ac rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD <br>      SYSFONT=latarcyrheb-sun16 crashkernel=832M log_buf_len=2M  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb <br>      quiet biosdevname=1 console=ttyS0,115200n8 nosoftlockup intel_idle.max_cstate=0 <br>      transparent_hugepage=never elevator=noop nmi_watchdog=0 processor.max_cstate=0 mce=ignore_ce<br>    Taint-check: 1  (see https://access.redhat.com/solutions/40594)<br>       0  PROPRIETARY_MODULE: Proprietary module has been loaded<br>    - - - - - - - - - - - - - - - - - - -<br>  Sys time:  Thu Nov 30 13:23:39 KST 2017<br>  Boot time: 2017. 09. 27. (수) 09:38:55 KST  (epoch: 1506472735)<br>  Uptime:    64 days,  3:44,  3 users<br>  LoadAvg:   [288 CPU] 2.70 (1%), 2.62 (1%), 3.32 (1%)<br>  /proc/stat:<br>    procs_running: 6   procs_blocked: 0    processes [Since boot]: 98149076<br>    cpu [Utilization since boot]:<br>      us 2%, ni 0%, sys 0%, idle 97%, iowait 0%, irq 0%, sftirq 0%, steal 0%<br><br><br>KDUMP CONFIG<br>  kexec-tools rpm version:<br>    kexec-tools-2.0.0-286.el6.x86_64<br>  Service enablement:<br>    kdump  0:off  1:off  2:on  3:on  4:on  5:on  6:off<br>  kdump initrd/initramfs:<br>    9201756 Jul 23 15:55 initrd-2.6.32-573.42.1.el6.x86_64kdump.img<br>  Memory reservation config:<br>    /proc/cmdline { crashkernel=832M }<br>  Actual memory reservation per /proc/iomem:<br>      04000000-37ffffff : Crash kernel<br>  kdump.conf:<br>    path /var/crash<br>    core_collector makedumpfile -c --message-level 1 -d 31<br>    blacklist RedCastle<br>  kdump.conf &quot;path&quot; available space:<br>    System MemTotal (uncompressed core size) { 4038.89 GiB }<br>    Available free space on target path's fs { 347.78 GiB }  (fs=/)<br>  Panic sysctls:<br>    kernel.sysrq [bitmask] =  &quot;1&quot;  (all SysRqs allowed)<br>    kernel.panic [secs] =  30  (secs til autoreboot after panic)<br>    kernel.hung_task_panic =  0<br>    kernel.panic_on_oops =  1<br>    kernel.panic_on_io_nmi =  1<br>    kernel.panic_on_unrecovered_nmi =  1<br>    kernel.panic_on_stackoverflow =  <br>    kernel.softlockup_panic =  0<br>    kernel.unknown_nmi_panic =  1<br>    kernel.nmi_watchdog =  0<br>    vm.panic_on_oom [0-2] =  0  (no panic)<br><br>MEMORY<br>  Stats graphed as percent of MemTotal:<br>    MemUsed    ▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊▊.......  85.1%<br>    Buffers    ..................................................   0.0%<br>    Cached     ..................................................   0.3%<br>    HugePages  ..................................................   0.0%<br>    Dirty      ..................................................   0.0%<br>  RAM:<br>    4038.9 GiB total ram<br>    3435.8 GiB (85%) used<br>    3425.5 GiB (85%) used excluding Buffers/Cached<br>    0.04 GiB (0%) dirty<br>  HugePages:<br>    No ram pre-allocated to HugePages<br>  LowMem/Slab/PageTables/Shmem:<br>    2.95 GiB (0%) of total ram used for Slab<br>    9.07 GiB (0%) of total ram used for PageTables<br>    0.1 GiB (0%) of total ram used for Shmem<br>  Swap:<br>    29.6 GiB (97%) used of 30.5 GiB total<br><br>  vm.<br>    dirty_ratio =  &quot;3&quot;  (% of total system memory)<br>    dirty_bytes =  &quot;0&quot;  (disabled -- check dirty_ratio)<br>    dirty_background_ratio =  &quot;1&quot;  (% of total system memory)<br>    dirty_background_bytes =  &quot;0&quot;  (disabled -- check dirty_background_ratio)<br>    dirty_expire_centisecs =  &quot;3000&quot;<br>    dirty_writeback_centisecs =  &quot;500&quot;<br>    nr_hugepages [2-MiB pages] =  &quot;0&quot;<br>    nr_overcommit_hugepages [2-MiB pages] =  &quot;0&quot;<br>    overcommit_memory [0-2] =  &quot;0&quot;  (heuristic overcommit)<br>    overcommit_ratio =  &quot;50&quot;<br>    oom_kill_allocating_task [bool] =  &quot;0&quot;  (scan tasklist)<br>    panic_on_oom [0-2] =  &quot;0&quot;  (no panic)<br>    swappiness [0-100] =  &quot;60&quot;<br><br>PS CHECK<br>  Total number of threads/processes: <br>    16928 / 7633<br>  Top users of CPU &amp; MEM: <br>    USER    %CPU     %MEM   RSS <br>    pbwadm  1051.3%  83.8%  3390.01 GiB<br>    root    115.9%   0.0%   0.33 GiB<br>    sapadm  2.0%     0.0%   0.11 GiB<br>    zabbix  0.3%     0.0%   0.03 GiB<br>  Uninteruptible sleep threads/processes (0/0): <br>    [None]<br>  Defunct zombie threads/processes (0/0): <br>    [None]<br>  Top CPU-using processes: <br>    USER      PID     %CPU  %MEM  VSZ-MiB  RSS-MiB  TTY    STAT  START  TIME       COMMAND  <br>    pbwadm    109783  1048  83.8  3522237  3467129  ?      Sl    Nov12  275129:53  hdbindexserver <br>    root      118858  73.0  0.0   13       1        pts/1  R     13:24  0:00       ps auxwww <br>    root      105521  18.2  0.0   648      48       ?      Sl    Nov12  4782:37    ./ontuned -c -x <br>    root      111194  17.7  0.0   249      40       pts/1  S+    13:22  0:23       /usr/bin/python /usr/sbin/sosreport -n <br>    root      81607   3.3   0.0   141      6        pts/2  S+    10:37  5:38       python /usr/bin/dstat -cdnm <br>    sapadm    35982   2.0   0.0   714      108      ?      Ssl   Sep27  1924:10    /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/hostctrl/exe/host_profile -D <br>    root      36036   1.7   0.0   51       28       ?      Ss    Sep27  1576:57    /usr/sap/hostctrl/exe/saposcol -l -w60 <br>    pbwadm    98040   1.3   0.0   15540    1833     ?      Sl    Nov12  359:03     hdbnameserver <br>    pbwadm    108994  1.1   0.0   6698     1335     ?      Sl    Nov12  309:38     hdbpreprocessor <br>    root      35723   0.9   0.0   1750     10       ?      Sl    Sep27  857:36     /usr/openv/netbackup/bin/nbsl <br>  Top MEM-using processes: <br>    USER      PID     %CPU  %MEM  VSZ-MiB  RSS-MiB  TTY    STAT  START  TIME       COMMAND  <br>    pbwadm    109783  1048  83.8  3522237  3467129  ?      Sl    Nov12  275129:53  hdbindexserver <br>    pbwadm    98040   1.3   0.0   15540    1833     ?      Sl    Nov12  359:03     hdbnameserver <br>    pbwadm    108994  1.1   0.0   6698     1335     ?      Sl    Nov12  309:38     hdbpreprocessor <br>    pbwadm    108987  0.9   0.0   6480     1068     ?      Sl    Nov12  261:49     hdbcompileserver <br>    sapadm    35982   2.0   0.0   714      108      ?      Ssl   Sep27  1924:10    /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/hostctrl/exe/host_profile -D <br>    root      28525   0.1   0.0   1768     74       ?      Sl    Sep27  127:16     /opt/opsware/agent/bin/python /opt/opsware/agent/pylibs/shadowbot/daemonbot.pyc --conf <br>    root      105521  18.2  0.0   648      48       ?      Sl    Nov12  4782:37    ./ontuned -c -x <br>    root      111194  17.7  0.0   249      40       pts/1  S+    13:22  0:23       /usr/bin/python /usr/sbin/sosreport -n <br>    root      36036   1.7   0.0   51       28       ?      Ss    Sep27  1576:57    /usr/sap/hostctrl/exe/saposcol -l -w60 <br>    zabbix    35849   0.3   0.0   37       21       ?      S     Sep27  291:44     /Tools/zabbix/sbin/zabbix_agentd: collector [idle <br>  Top thread-spawning processes: <br>    #     USER      PID     %CPU  %MEM  VSZ-MiB  RSS-MiB  TTY    STAT  START  TIME       COMMAND <br>    1328  pbwadm    109783  1048  83.8  3522237  3467129  ?      -     Nov12  275129:56  hdbindexserver <br>    72    pbwadm    108994  1.1   0.0   6698     1335     ?      -     Nov12  309:38     hdbpreprocessor <br>    71    pbwadm    98040   1.3   0.0   15540    1833     ?      -     Nov12  359:03     hdbnameserver <br>    61    pbwadm    108987  0.9   0.0   6480     1068     ?      -     Nov12  261:49     hdbcompileserver <br>    30    root      26388   0.3   0.0   1902     11       ?      -     Sep27  338:03     /sbin/multipathd <br>    24    root      35676   0.0   0.0   1851     9        ?      -     Sep27  18:40      /usr/openv/netbackup/bin/nbrmms <br>    22    root      35723   0.9   0.0   1750     10       ?      -     Sep27  857:36     /usr/openv/netbackup/bin/nbsl <br>    13    root      28525   0.1   0.0   1768     74       ?      -     Sep27  127:16     /opt/opsware/agent/bin/python /opt/opsware/agent/pylibs/shadowbot/daemonbot.pyc --conf <br>    10    sapadm    35982   2.0   0.0   714      108      ?      -     Sep27  1924:10    /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/hostctrl/exe/host_profile -D <br>    9     root      105521  18.2  0.0   648      48       ?      -     Nov12  4782:38    ./ontuned -c -x <br><br>$ cat free<br>             total       used       free     shared    buffers     cached<br>Mem:    4235079308 3602658928  632420380     107876     128836    4217672<br>-/+ buffers/cache: 3598312420  636766888 <br>Swap:     31999800   31011664     988136 <br><br>$ cat ps | awk '{ m[$11]+=$6/1024/1024 } END { for (item in m) { printf &quot;%40s %10s GiB\n&quot;, item, m[item] } }' | sort -k 2 -r -n|head<br>                          hdbindexserver    3385.87 GiB<br>                           hdbnameserver    1.79016 GiB<br>                         hdbpreprocessor    1.30403 GiB<br>                        hdbcompileserver    1.04261 GiB<br>       /usr/sap/hostctrl/exe/sapstartsrv   0.105278 GiB<br>           /opt/opsware/agent/bin/python  0.0740929 GiB<br>                               ./ontuned  0.0472488 GiB<br>                         /usr/bin/python  0.0421562 GiB<br>          /usr/sap/hostctrl/exe/saposcol  0.0276031 GiB<br>       /Tools/zabbix/sbin/zabbix_agentd:  0.0233955 GiB<br><br><br>$ cat ps | awk '{Total+=$6} END {print Total/1024/1024&quot; GB&quot;}'<br>3390.49 GB<br><br><br>Node:0, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>    108568 <br>  108568 <br>     0 %<br>  1 <br>     2 <br>    335995 <br>  671990 <br>     0 %<br>  2 <br>     4 <br>   1194013 <br> 4776052 <br>     2 %<br>  3 <br>     8 <br>   1992318 <br> 15938544 <br>    15 %<br>  4 <br>    16 <br>    726113 <br> 11617808 <br>    58 %<br>  5 <br>    32 <br>    105314 <br> 3370048 <br>    90 %<br>  6 <br>    64 <br>      1849 <br>  118336 <br>    99 %<br>  7 <br>   128 <br>        38 <br>    4864 <br>    99 %<br>  8 <br>   256 <br>         4 <br>    1024 <br>    99 %<br>  9 <br>   512 <br>         3 <br>    1536 <br>    99 %<br> 10 <br>  1024 <br>         4 <br>    4096 <br>    99 %<br>TotalFreePages: 36612866<br>Overall Fragmentation: 60 %<br><br>Node:1, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>   3348741 <br> 3348741 <br>     0 %<br>  1 <br>     2 <br>   1417358 <br> 2834716 <br>    28 %<br>  2 <br>     4 <br>    907013 <br> 3628052 <br>    52 %<br>  3 <br>     8 <br>    216323 <br> 1730584 <br>    83 %<br>  4 <br>    16 <br>     10705 <br>  171280 <br>    98 %<br>  5 <br>    32 <br>         0 <br>       0 <br>   100 %<br>  6 <br>    64 <br>         0 <br>       0 <br>   100 %<br>  7 <br>   128 <br>         0 <br>       0 <br>   100 %<br>  8 <br>   256 <br>         0 <br>       0 <br>   100 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 11713373<br>Overall Fragmentation: 78 %<br><br>Node:2, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>      7068 <br>    7068 <br>     0 %<br>  1 <br>     2 <br>     17182 <br>   34364 <br>     0 %<br>  2 <br>     4 <br>   1290324 <br> 5161296 <br>     0 %<br>  3 <br>     8 <br>   1810690 <br> 14485520 <br>    11 %<br>  4 <br>    16 <br>   1125712 <br> 18011392 <br>    42 %<br>  5 <br>    32 <br>    242212 <br> 7750784 <br>    82 %<br>  6 <br>    64 <br>      6602 <br>  422528 <br>    99 %<br>  7 <br>   128 <br>        64 <br>    8192 <br>    99 %<br>  8 <br>   256 <br>        26 <br>    6656 <br>    99 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 45887800<br>Overall Fragmentation: 57 %<br><br>Node:3, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>     74903 <br>   74903 <br>     0 %<br>  1 <br>     2 <br>    131569 <br>  263138 <br>     3 %<br>  2 <br>     4 <br>    113159 <br>  452636 <br>    13 %<br>  3 <br>     8 <br>     48397 <br>  387176 <br>    32 %<br>  4 <br>    16 <br>     78824 <br> 1261184 <br>    48 %<br>  5 <br>    32 <br>         0 <br>       0 <br>    99 %<br>  6 <br>    64 <br>         1 <br>      64 <br>    99 %<br>  7 <br>   128 <br>         0 <br>       0 <br>    99 %<br>  8 <br>   256 <br>         1 <br>     256 <br>    99 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 2439357<br>Overall Fragmentation: 62 %<br><br>Node:4, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>   1956772 <br> 1956772 <br>     0 %<br>  1 <br>     2 <br>   4541029 <br> 9082058 <br>    11 %<br>  2 <br>     4 <br>   1347438 <br> 5389752 <br>    65 %<br>  3 <br>     8 <br>     50759 <br>  406072 <br>    97 %<br>  4 <br>    16 <br>         1 <br>      16 <br>    99 %<br>  5 <br>    32 <br>         3 <br>      96 <br>    99 %<br>  6 <br>    64 <br>         2 <br>     128 <br>    99 %<br>  7 <br>   128 <br>         1 <br>     128 <br>    99 %<br>  8 <br>   256 <br>         0 <br>       0 <br>   100 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 16835022<br>Overall Fragmentation: 79 %<br><br>Node:5, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>     66031 <br>   66031 <br>     0 %<br>  1 <br>     2 <br>   1888361 <br> 3776722 <br>     0 %<br>  2 <br>     4 <br>   3824094 <br> 15296376 <br>    10 %<br>  3 <br>     8 <br>   1720060 <br> 13760480 <br>    51 %<br>  4 <br>    16 <br>    245180 <br> 3922880 <br>    89 %<br>  5 <br>    32 <br>      3043 <br>   97376 <br>    99 %<br>  6 <br>    64 <br>        52 <br>    3328 <br>    99 %<br>  7 <br>   128 <br>        21 <br>    2688 <br>    99 %<br>  8 <br>   256 <br>        13 <br>    3328 <br>    99 %<br>  9 <br>   512 <br>         3 <br>    1536 <br>    99 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 36930745<br>Overall Fragmentation: 67 %<br><br>Node:6, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>    777006 <br>  777006 <br>     0 %<br>  1 <br>     2 <br>    529321 <br> 1058642 <br>    22 %<br>  2 <br>     4 <br>    404065 <br> 1616260 <br>    53 %<br>  3 <br>     8 <br>       578 <br>    4624 <br>    99 %<br>  4 <br>    16 <br>         7 <br>     112 <br>    99 %<br>  5 <br>    32 <br>         2 <br>      64 <br>    99 %<br>  6 <br>    64 <br>         0 <br>       0 <br>   100 %<br>  7 <br>   128 <br>         0 <br>       0 <br>   100 %<br>  8 <br>   256 <br>         0 <br>       0 <br>   100 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 3456708<br>Overall Fragmentation: 79 %<br><br>Node:7, zone:Normal<br>Order <br> Block <br> FreePages <br> EachSum <br> Frag [%]<br>  0 <br>     1 <br>     50312 <br>   50312 <br>     0 %<br>  1 <br>     2 <br>     26875 <br>   53750 <br>     1 %<br>  2 <br>     4 <br>    539126 <br> 2156504 <br>     2 %<br>  3 <br>     8 <br>    238682 <br> 1909456 <br>    54 %<br>  4 <br>    16 <br>         0 <br>       0 <br>   100 %<br>  5 <br>    32 <br>         0 <br>       0 <br>   100 %<br>  6 <br>    64 <br>         0 <br>       0 <br>   100 %<br>  7 <br>   128 <br>         0 <br>       0 <br>   100 %<br>  8 <br>   256 <br>         0 <br>       0 <br>   100 %<br>  9 <br>   512 <br>         0 <br>       0 <br>   100 %<br> 10 <br>  1024 <br>         0 <br>       0 <br>   100 %<br>TotalFreePages: 4170022<br>Overall Fragmentation: 68 %<br><br>$ cat proc/sys/vm/zone_reclaim_mode <br>0<br>$ cat proc/sys/vm/vfs_cache_pressure <br>100<br>$ cat proc/sys/vm/min_free_kbytes <br>65536<br><br>sar 데이터가 sosreport 에 포함되지 않아 별도로 파일을 올려주시기 바랍니다.<br><br>또한, 본 기술 문의는 SAP 엔지니어에 요청하여 분석을 의뢰해보도록 하겠습니다.<br><br>그 전까지 vm.swappiness = 1 로 설정을 낮춰서 swap 사용을 자제해보시기 바랍니다.<br>또한, hugepage 를 별도로 설정할 수 있는지도 내부적으로 확인해보도록 하겠습니다.<br><br>만약 본 이슈와 관련하여 추가적인 문의 사항이 있으실 경우 연락 주시기 바랍니다.<br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-12-05T03:59:55Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L6duDIAR"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-05T05:06:53Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-05T05:06:52Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>SAP 측에도 기술 문의를 열어주시고 티켓 넘버를 저희에게 알려주시기 바랍니다.<br>아래 문서에 튜닝 설정에 대한 부분도 참고해주시기 바랍니다.<br><br>https://wiki.scn.sap.com/wiki/display/SAPHANA/Linux+Operating+System+with+SAP+HANA+Reference+Guide<br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-12-05T05:06:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L6MlAIAV"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-12-04T08:04:29Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-12-04T08:04:29Z</b><br><br>sosreport파일 업로드 진행하였습니다<br><br>해당 케이스 진행중이시던 담당자분이 교육이 있어서 제가 대신 해당 케이스 진행하게 되었습니다<br><br>필요하신 부분 있으시면 연락 부탁드립니다<br><br>타임게이트 / 오선우 / 책임  / 010-4630-3171 / sw.oh@time-gate.com<br><br>감사합니다<br><br><publishedDate>2017-12-04T08:04:29Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000L6JGHIA3"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-12-03T23:09:55Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-12-03T23:09:55Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 입니다.<br><br>본 케이스와 연관되어 더 궁금한 사항이 있거나 추가적인 지원이 필요하신가요?<br><br>본 안내 후 일주일 이내에 별다른 답변이 없다면 자동으로 종료 상태가 된다는 것을 알려 드립니다.<br><br>만약 추가적인 지원이 필요하다면, 연락 부탁드립니다.<br><br>감사합니다.<br><br>Jay Shin<br>Technical Support Engineer<br>GSS, Asia Pacific, Red Hat Inc<br><br><publishedDate>2017-12-03T23:09:55Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000L5gMRIAZ"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2017-11-30T06:15:44Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2017-11-30T06:15:44Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>문제가 있는 시스템의 sosreport 와 문제가 발생하지 않는 시스템의 sosreport 를 올려주시면,<br>물리메모리가 충분한 상황에서도 스왑이 발생되는 원인에 대한 분석을 진행해보도록 하겠습니다.<br><br>감사합니다.<br><br><br>Jay Shin, Technical Support Engineer<br><br>Office: +82 2 3490 5200 - ext 3 (기술문의)<br>Email: jaeshin@redhat.com <br><br>Red Hat, Asia-Pacific Pty Ltd <br>Website: www.redhat.com<br><br><publishedDate>2017-11-30T06:15:44Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>