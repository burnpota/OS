======================<br><b>생성계정 : 삼성생명 타임게이트</b><br><b>생성날짜 : 2017-11-13T01:50:10Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2018-01-22T11:02:11Z</b><br><b>id : 500A000000Z2cKMIAZ</b><br>======================<br><br><b><font size=15>
제목  : nfs마운트시 문제관련
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하십니까 타임게이트 오선우(타임게이트/책임/ 010-4630-3171 / sw.oh@time-gate.com)입니다<br>아래와 같이 자기 자신의 디렉토리르 nfs를 통해서 공유하고 해당 디렉토리를 <br>마운트 하여 사용하는 시스템에서<br><br>root@PDOCAL01SL /root # cat /etc/exports<br>/views_hs localhost(insecure,rw,root_squash,sync)<br>/views_raw localhost(insecure,rw,root_squash,sync)<br><br>아래와 같이 마운트가 되지 않는 증상이  있었습니다<br><br>root@PDOCAL01SL /root # showmount -e localhost<br>clnt_create: RPC: Unable to send<br><br>별다른 특이사항을 찾을수 없어 nfs데몬을 리스타트 진행하였고 정상적으로 showmount하는 것을 확인하였습니다  원인이 무엇인지 확인 부탁드립니다<br><br>dropbox에 파일명 20171113_PDOCAL01SL.tar 로 업로드 진행하였습니다<br><br>감사합니다<br><br>------------------------------<br>root@PDOCAL01SL /root # df -h<br>Filesystem            Size  Used Avail Use% Mounted on<br>/dev/sda2              30G  9.8G   19G  35% /<br>tmpfs                  95G     0   95G   0% /dev/shm<br>/dev/sda1             976M  119M  807M  13% /boot<br>/dev/sda3              20G  2.4G   17G  13% /var<br>/dev/mapper/vg0-home  9.8G   97M  9.2G   2% /home<br>/dev/mapper/vg0-sysadmin<br>                       20G   11G  8.6G  55% /sysadmin<br>/dev/mapper/vg9-CRASH<br>                      194G   12G  173G   7% /CRASH<br>/dev/mapper/LRPDOCAPPVG-OPENTEXT<br>                       30G  1.2G   27G   5% /OPENTEXT<br>/dev/mapper/LRPDOCAPPVG-OPENTEXTLOG<br>                       20G  171M   19G   1% /OPENTEXTLOG<br>/dev/mapper/LRPDOCAPPVG-DISKBUFFER1<br>                       30G   44M   28G   1% /DISKBUFFER1<br>/dev/mapper/LRPDOCAPPVG-DISKBUFFER2<br>                      178G  7.6G  161G   5% /DISKBUFFER2<br>/dev/mapper/LRPDOCAPPVG-BURNBUFFER<br>                      9.8G   23M  9.2G   1% /BURNBUFFER<br>/dev/mapper/LRPDOCAPPVG-DP<br>                      4.8G   10M  4.6G   1% /DP<br>/dev/mapper/LRPDOCAPPVG-TOMCAT<br>                      9.8G   31M  9.2G   1% /TOMCAT<br>/dev/mapper/LRPDOCAPPVG-FileStore<br>                      330G  176G  138G  57% /FileStore<br>/dev/mapper/LRPDOCAPPVG-APP<br>                       30G   12G   16G  43% /APP<br>root@PDOCAL01SL /root # showmount -e<br>clnt_create: RPC: Program not registered<br>root@PDOCAL01SL /root # cat /etc/exports<br>/views_hs localhost(insecure,rw,root_squash,sync)<br>/views_raw localhost(insecure,rw,root_squash,sync)<br>root@PDOCAL01SL /root # showmount -e localhost<br>clnt_create: RPC: Unable to send<br>root@PDOCAL01SL /root # chkconfig | grep nfs<br>nfs             0:off   1:off   2:on    3:on    4:on    5:on    6:off<br>nfs-rdma        0:off   1:off   2:off   3:off   4:off   5:off   6:off<br>nfslock         0:off   1:off   2:off   3:off   4:off   5:off   6:off<br>root@PDOCAL01SL /root # chkconfig nfslock on<br>root@PDOCAL01SL /root # chkconfig | grep nfs<br>nfs             0:off   1:off   2:on    3:on    4:on    5:on    6:off<br>nfs-rdma        0:off   1:off   2:off   3:off   4:off   5:off   6:off<br>nfslock         0:off   1:off   2:on    3:on    4:on    5:on    6:off<br>root@PDOCAL01SL /root #<br>root@PDOCAL01SL /root # service nfslock restart<br>Stopping NFS locking:                                      [  OK  ]<br>Stopping NFS statd:                                        [  OK  ]<br>root@PDOCAL01SL /root #<br>root@PDOCAL01SL /root #<br>root@PDOCAL01SL /root # showmount -e localhost<br>clnt_create: RPC: Unable to send<br>root@PDOCAL01SL /root # service nfs restart<br>Shutting down NFS daemon:                                  [  OK  ]<br>Shutting down NFS mountd:                                  [  OK  ]<br>Shutting down NFS quotas:                                  [  OK  ]<br>Shutting down NFS services:                                [  OK  ]<br>Shutting down RPC idmapd:                                  [  OK  ]<br>Starting NFS services:                                     [  OK  ]<br>Starting NFS quotas:                                       [  OK  ]<br>Starting NFS mountd:                                       [  OK  ]<br>Starting NFS daemon:                                       [  OK  ]<br>Starting RPC idmapd:                                       [  OK  ]<br>root@PDOCAL01SL /root # showmount -e localhost<br>Export list for localhost:<br>/views_raw localhost<br>/views_hs  localhost<br>root@PDOCAL01SL /root #<br>root@PDOCAL01SL /root #<br>root@PDOCAL01SL /root # service nfs status<br>rpc.svcgssd is stopped<br>rpc.mountd (pid 43576) is running...<br>nfsd (pid 43592 43591 43590 43589 43588 43587 43586 43585) is running...<br>rpc.rquotad (pid 43571) is running...<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.7</b><br><b>타입  : Other</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><hostname>PDOCAL01SL</hostname><br><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2018-01-04T07:34:57Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2018-01-04T07:34:57Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>1. rpcbind는 해당 버그랑 연관성이 있어보이는데요 glibc도 함께 업데이트가 필요한가요?<br><br>버그질라 확인시 고객님께서 사용중인 rpcbind 버전에서 glibc 업그레이드로 이슈 해소가 되는 사례가 있어<br>요청드린 겁니다.<br><br>2. 대상장비가 EUS로 구성되어 있고 일반으로 업데이트된 패키지들이 있긴하지만 EUS를 유지했으면 하는데요<br>rpcbind 의 경우 EUS가 끝난거 같습니다 맞는건가요?<br>glibc의 경우 마지막 빌드버전이 2.12-1.166.el6_7.8 버전이 있는것 같은데요 해당 패키지로 진행해도 해당 증상을  해결할수 있는것 인가요?<br><br>만일 EUS를 사용중이시다면 안내해드린 버전은 다운로드가 되지 않는것으로 보입니다.<br>버그질라를 살펴보면 rpcbind 버전을 0.2.0-13.el6으로 다운그레드 시에도 이슈가 재현되지 <br>않는다고 하시는데 혹시 다운그레드 작업은 가능하신지요?<br><br>3.  필요하다면 EUS를 포기하고라도 일반으로 업데이트 진행할 생각인데요 <br>말씀해주신 하기 패키지를 설치하는 것이 현재로써는 최선의 방법인가요 <br><br>rpcbind-0.2.0-13.el6_9.1<br>glibc-2.12-1.209.el6_9.2<br><br>의존성이 많이 걸릴꺼 같은데요 추가로 필요한 패키지가 어떤것이 있나요? 아니면 제가 직접 의존성을 확인할수 있는 방법이 있을까요?(외부통신이 안된다는 가정으로 패키지를 다운받아서 전달해야 하는 상황입니다)<br><br>업그레이드 혹은 다운그레드 작업을 고려해 보실수 있으십니다.<br>의존성 이슈는 서버에 설치된 패키지들의 버전에 따라 다르기 때문에 안내해 드리기 어려운데<br>오프라인 상황이시다면 최신버전 iso 파일을 다운로드 받으셔서 local repo를 생성하신후<br>별도로 rpcbind, glibc로 시작하는 패키지들의 최신버전을 별도로 다운받으신후 &quot;yum localinstall &lt;패키지명&gt;&quot;<br>명령어로 설치하시는것도 방법인것으로 보입니다.<br><br>Need to set up yum repository for locally-mounted DVD on Red Hat Enterprise Linux 5<br>https://access.redhat.com/solutions/328843<br><br>Yum Repository Configuration Helper<br>https://access.redhat.com/labs/yumrepoconfighelper/<br><br>감사합니다.<br><br><publishedDate>2018-01-04T07:34:57Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LKL8YIAX"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2018-01-04T04:14:54Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2018-01-04T04:14:54Z</b><br><br>안녕하십니까<br><br>1. rpcbind는 해당 버그랑 연관성이 있어보이는데요 glibc도 함께 업데이트가 필요한가요?<br><br>2. 대상장비가 EUS로 구성되어 있고 일반으로 업데이트된 패키지들이 있긴하지만 EUS를 유지했으면 하는데요<br>rpcbind 의 경우 EUS가 끝난거 같습니다 맞는건가요?<br>glibc의 경우 마지막 빌드버전이 2.12-1.166.el6_7.8 버전이 있는것 같은데요 해당 패키지로 진행해도 해당 증상을  해결할수 있는것 인가요?<br><br>3.  필요하다면 EUS를 포기하고라도 일반으로 업데이트 진행할 생각인데요 <br>말씀해주신 하기 패키지를 설치하는 것이 현재로써는 최선의 방법인가요 <br><br>rpcbind-0.2.0-13.el6_9.1<br>glibc-2.12-1.209.el6_9.2<br><br>의존성이 많이 걸릴꺼 같은데요 추가로 필요한 패키지가 어떤것이 있나요? 아니면 제가 직접 의존성을 확인할수 있는 방법이 있을까요?(외부통신이 안된다는 가정으로 패키지를 다운받아서 전달해야 하는 상황입니다)<br><br><publishedDate>2018-01-04T04:14:54Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LK2TyIAL"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2018-01-03T03:03:04Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2018-01-03T03:03:04Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>확인시 이슈 분석에는 반드시 디버깅 정보가 필요합니다. <br>다면 현재 관련 버그 확인중에 아래와 같이 rpcbind가 알수 없는 원인으로 죽는 버그가 발견되었습니다.<br><br>Bug 1458240 - rpcbind crash on start [rhel-6.9.z] (edit)<br>https://bugzilla.redhat.com/show_bug.cgi?id=1458240<br><br>해당 이슈는 rpcbind, glibc 버전을 최신 버전으로 올리시면 해소 가능한것으로 확인됩니다.<br>rpcbind-0.2.0-13.el6_9.1<br>glibc-2.12-1.209.el6_9.2<br><br>혹시 가능하시다면 rpcbind, glibc 버전을 최신 버전으로 올리시고 이슈가 재현 되는지 확인 부탁 드립니다.<br><br>감사합니다.<br><br><publishedDate>2018-01-03T03:03:04Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LJqhSIAT"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2018-01-02T07:06:39Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2018-01-02T07:06:39Z</b><br><br>안녕하십니까 타임게이트 오선우입니다<br>답변 감사합니다 <br><br> 한가지 문의 사항이 있습니다<br><br>해당 상황이 약 2달 즉 언제 발생할지 모르는 상황에서 디버깅 모드로 운영중인 서버에서 사용하는 것이 어려움이 있어서요<br><br>혹시 해당 증상 발생시 크래쉬덤프를 강제로 떨어트려서 원인 분석이 된다거나 비슷한 버그나 알려진 이슈가 있는지 번거롭지만 한번만 더 확인 부탁드립니다<br><br>감사합니다<br><br><publishedDate>2018-01-02T07:06:39Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LFI2GIAX"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-12-28T07:45:37Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-12-28T07:45:37Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>Dec 26 12:05:01 PDOCAL01SL Archive Server[92768]: ERROR :  Input/output error: server WORM not responding<br><br>상기 메시지는 읽기쓰기 작업에 실패 되었음을 알립니다. 이는 rpcbind 프로세스가 dead되었기때문에 발생된 현상으로 보실수 있습니다.<br>다만 이와 같은 메시지는 rpcbind가 dead된후 발생되는 메시지이며 rpcbind가 dead된 원인을 분석할수 없습니다.<br>로그를 보시다시피 현재 디폴트 설정으로는 이슈 발생시 아무런 정보를 기록하지 않습니다.<br>따라서 아쉽게도 원인 분석에는 반드시 디버깅 활성화 및 정보 수집이 필요합니다.<br><br>감사합니다.<br><br><publishedDate>2017-12-28T07:45:37Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LFHlZIAX"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-12-28T07:00:53Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-12-28T07:00:53Z</b><br><br>안녕하십니까 타임게이트 오선우입니다<br><br>해당 서버는 현재 운영과 서비스중인 서버입니다<br><br>해당 디버깅 모드를 설정하여 로그가 많아진다면 같은 증상이 언제 발생할지 모르는 상황에서<br><br>디버깅 모드로 관리하는 것은 많이 부담스러운 상황인데요<br><br>다른 방법은 없는것인가요?<br><br>하기 로그는 특별히 이슈될만한 부분은 아닌지요?<br><br>Dec 26 12:05:01 PDOCAL01SL Archive Server[92768]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 13:05:01 PDOCAL01SL Archive Server[98448]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 14:05:01 PDOCAL01SL Archive Server[104518]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 15:05:02 PDOCAL01SL Archive Server[110561]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 17:05:02 PDOCAL01SL Archive Server[2738]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 18:05:02 PDOCAL01SL Archive Server[8650]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 19:05:01 PDOCAL01SL Archive Server[14682]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 20:05:02 PDOCAL01SL Archive Server[20424]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 21:05:02 PDOCAL01SL Archive Server[26144]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 22:05:01 PDOCAL01SL Archive Server[31840]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 23:05:01 PDOCAL01SL Archive Server[37540]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 00:05:01 PDOCAL01SL Archive Server[43851]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 01:05:02 PDOCAL01SL Archive Server[50992]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 02:30:01 PDOCAL01SL Archive Server[62390]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:01:01 PDOCAL01SL Archive Server[65421]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:15:01 PDOCAL01SL Archive Server[66263]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:30:01 PDOCAL01SL Archive Server[68771]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:00:01 PDOCAL01SL Archive Server[72099]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:01:01 PDOCAL01SL Archive Server[72243]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:10:01 PDOCAL01SL Archive Server[73194]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:10:07 PDOCAL01SL Archive Server[73204]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 07:05:01 PDOCAL01SL Archive Server[92433]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 08:05:01 PDOCAL01SL Archive Server[98233]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 09:05:01 PDOCAL01SL Archive Server[103996]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 10:05:01 PDOCAL01SL Archive Server[109757]: ERROR :  Input/output error: server WORM not responding<br><br><publishedDate>2017-12-28T07:00:53Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LFHLsIAP"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-12-28T06:07:59Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-12-28T06:07:59Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>이슈 발생시점 로그를 확인해 보았지만 아무런 메시지가 기록되지 않았습니다.<br><br>해당 서버에서 아래와 같은 디버깅을 활성화하지 않은것으로 보입니다. <br>원인 분석에는 이와 같은 디버깅 정보가 필요합니다.<br>아래와 같은 옵션은 이슈가 발생되는 서버에서 추가해 주어 이슈 발생시<br>디버깅 정보를 기록하여 어떤 원인으로 rpcbind 서비스가 dead 되었는지를<br>확인해 볼수 있습니다.<br><br>rpcdebug -m nfs -s all<br>rpcdebug -m nfsd -s all<br>rpcdebug -m rpc -s all<br>rpcdebug -m nlm -s all<br><br>또한 아래와 같이 서버에서 rpcbind 프로세스이 gdb 코어를 수집하셔서 어떤 원인으로<br>죽었는지를 분석해 보실수 있습니다. <br><br>참고문서:<br>The rpcbind service periodically segfaults subsequently stopping NFS mounts from functioning.<br>https://access.redhat.com/solutions/236273<br><br>상기 작업은 전부 이슈 발생전에 실행해 주셔야 하며 이슈가 발생시 수집된 로그들을<br>저희쪽으로 보내주셔야 합니다. 상기 디버깅을 활성화 하시면 많은 로그가 생성되기 때문에<br>상용서버에서는 사용하지 마시기 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2017-12-28T06:07:59Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LF7aFIAT"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-12-27T07:34:49Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-12-27T07:34:49Z</b><br><br>안녕하십니까 타임게이트 오선우입니다<br><br>nfs가 끈겨버리는 증상이 다시 발생하여 케이스 다시 오픈합니다<br><br>sosreport는 dropbox에 업로드 하였으며<br>파일명은 sosreport-PDOCAL01SL-20171227153108.tar.xz 입니다<br><br>이슈발생은 약 25일쯤을 생각되며 어제 오전 11시경에 1차 조치하여 현재는 이상이 없는 상태입니다<br><br>조치 전에 상황<br><br>root@PDOCAL01SL /root # service rpcbind status<br>rpcbind (pid  XXXXX) is dead...<br> <br>root@PDOCAL01SL /root # grep WORM /var/log/messages<br>Dec 26 12:05:01 PDOCAL01SL Archive Server[92768]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 13:05:01 PDOCAL01SL Archive Server[98448]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 14:05:01 PDOCAL01SL Archive Server[104518]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 15:05:02 PDOCAL01SL Archive Server[110561]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 17:05:02 PDOCAL01SL Archive Server[2738]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 18:05:02 PDOCAL01SL Archive Server[8650]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 19:05:01 PDOCAL01SL Archive Server[14682]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 20:05:02 PDOCAL01SL Archive Server[20424]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 21:05:02 PDOCAL01SL Archive Server[26144]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 22:05:01 PDOCAL01SL Archive Server[31840]: ERROR :  Input/output error: server WORM not responding<br>Dec 26 23:05:01 PDOCAL01SL Archive Server[37540]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 00:05:01 PDOCAL01SL Archive Server[43851]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 01:05:02 PDOCAL01SL Archive Server[50992]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 02:30:01 PDOCAL01SL Archive Server[62390]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:01:01 PDOCAL01SL Archive Server[65421]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:15:01 PDOCAL01SL Archive Server[66263]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 03:30:01 PDOCAL01SL Archive Server[68771]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:00:01 PDOCAL01SL Archive Server[72099]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:01:01 PDOCAL01SL Archive Server[72243]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:10:01 PDOCAL01SL Archive Server[73194]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 04:10:07 PDOCAL01SL Archive Server[73204]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 07:05:01 PDOCAL01SL Archive Server[92433]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 08:05:01 PDOCAL01SL Archive Server[98233]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 09:05:01 PDOCAL01SL Archive Server[103996]: ERROR :  Input/output error: server WORM not responding<br>Dec 27 10:05:01 PDOCAL01SL Archive Server[109757]: ERROR :  Input/output error: server WORM not responding<br><br><br>조치후 <br><br>root@PDOCAL01SL /root # service rpcbind status<br>rpcbind (pid  87518) is running...<br>root@PDOCAL01SL /root # rpcinfo -s<br>   program version(s) netid(s)                         service     owner<br>    100000  2,3,4     local,udp,tcp,udp6,tcp6          portmapper  superuser<br>    100011  2,1       tcp,udp                          rquotad     superuser<br>    100005  3,2,1     tcp6,udp6,tcp,udp                mountd      superuser<br>    100003  4,3,2     udp6,tcp6,udp,tcp                nfs         superuser<br>    100227  3,2       udp6,tcp6,udp,tcp                nfs_acl     superuser<br>    100021  4,3,1     tcp6,udp6,tcp,udp                nlockmgr    superuser<br>root@PDOCAL01SL /root # rpcinfo<br>   program version netid     address                service    owner<br>    100000    4    tcp6      ::.0.111               portmapper superuser<br>    100000    3    tcp6      ::.0.111               portmapper superuser<br>    100000    4    udp6      ::.0.111               portmapper superuser<br>    100000    3    udp6      ::.0.111               portmapper superuser<br>    100000    4    tcp       0.0.0.0.0.111          portmapper superuser<br>    100000    3    tcp       0.0.0.0.0.111          portmapper superuser<br>    100000    2    tcp       0.0.0.0.0.111          portmapper superuser<br>    100000    4    udp       0.0.0.0.0.111          portmapper superuser<br>    100000    3    udp       0.0.0.0.0.111          portmapper superuser<br>    100000    2    udp       0.0.0.0.0.111          portmapper superuser<br>    100000    4    local     /var/run/rpcbind.sock  portmapper superuser<br>    100000    3    local     /var/run/rpcbind.sock  portmapper superuser<br>    100011    1    udp       0.0.0.0.3.107          rquotad    superuser<br>    100011    2    udp       0.0.0.0.3.107          rquotad    superuser<br>    100011    1    tcp       0.0.0.0.3.107          rquotad    superuser<br>    100011    2    tcp       0.0.0.0.3.107          rquotad    superuser<br>    100005    1    udp       0.0.0.0.128.90         mountd     superuser<br>    100005    1    tcp       0.0.0.0.184.85         mountd     superuser<br>    100005    1    udp6      ::.193.220             mountd     superuser<br>    100005    1    tcp6      ::.154.215             mountd     superuser<br>    100005    2    udp       0.0.0.0.226.90         mountd     superuser<br>    100005    2    tcp       0.0.0.0.147.131        mountd     superuser<br>    100005    2    udp6      ::.132.240             mountd     superuser<br>    100005    2    tcp6      ::.215.5               mountd     superuser<br>    100005    3    udp       0.0.0.0.143.173        mountd     superuser<br>    100005    3    tcp       0.0.0.0.205.123        mountd     superuser<br>    100005    3    udp6      ::.190.158             mountd     superuser<br>    100005    3    tcp6      ::.199.250             mountd     superuser<br>    100003    2    tcp       0.0.0.0.8.1            nfs        superuser<br>    100003    3    tcp       0.0.0.0.8.1            nfs        superuser<br>    100003    4    tcp       0.0.0.0.8.1            nfs        superuser<br>    100227    2    tcp       0.0.0.0.8.1            nfs_acl    superuser<br>    100227    3    tcp       0.0.0.0.8.1            nfs_acl    superuser<br>    100003    2    udp       0.0.0.0.8.1            nfs        superuser<br>    100003    3    udp       0.0.0.0.8.1            nfs        superuser<br>    100003    4    udp       0.0.0.0.8.1            nfs        superuser<br>    100227    2    udp       0.0.0.0.8.1            nfs_acl    superuser<br>    100227    3    udp       0.0.0.0.8.1            nfs_acl    superuser<br>    100003    2    tcp6      ::.8.1                 nfs        superuser<br>    100003    3    tcp6      ::.8.1                 nfs        superuser<br>    100003    4    tcp6      ::.8.1                 nfs        superuser<br>    100227    2    tcp6      ::.8.1                 nfs_acl    superuser<br>    100227    3    tcp6      ::.8.1                 nfs_acl    superuser<br>    100003    2    udp6      ::.8.1                 nfs        superuser<br>    100003    3    udp6      ::.8.1                 nfs        superuser<br>    100003    4    udp6      ::.8.1                 nfs        superuser<br>    100227    2    udp6      ::.8.1                 nfs_acl    superuser<br>    100227    3    udp6      ::.8.1                 nfs_acl    superuser<br>    100021    1    udp       0.0.0.0.147.120        nlockmgr   superuser<br>    100021    3    udp       0.0.0.0.147.120        nlockmgr   superuser<br>    100021    4    udp       0.0.0.0.147.120        nlockmgr   superuser<br>    100021    1    tcp       0.0.0.0.175.186        nlockmgr   superuser<br>    100021    3    tcp       0.0.0.0.175.186        nlockmgr   superuser<br>    100021    4    tcp       0.0.0.0.175.186        nlockmgr   superuser<br>    100021    1    udp6      ::.236.150             nlockmgr   superuser<br>    100021    3    udp6      ::.236.150             nlockmgr   superuser<br>    100021    4    udp6      ::.236.150             nlockmgr   superuser<br>    100021    1    tcp6      ::.166.100             nlockmgr   superuser<br>    100021    3    tcp6      ::.166.100             nlockmgr   superuser<br>    100021    4    tcp6      ::.166.100             nlockmgr   superuser<br><br><br>두번째 발생하여 원인 분석이 필요한 상황입니다<br>필요하신 부분 있으시면 연락 부탁드립니다<br><br><publishedDate>2017-12-27T07:34:49Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KyOrBIAV"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-11-15T02:23:56Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-11-15T02:23:56Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>1.  rpcbind를 시작해주고 nfs를 재시작 해줘야 하는 이유를 자세하게 내용을 받을수 있을까요? 연관성에 대해서 설명해주시면 감사하겠습니다<br>sosreport 내에 해당 내용이 포함되어 있는건가요?<br><br>rpcbind는 NFSv2/NFSv3 버전 사용시 관련 프로세스에 포트를 지정해 주는 작용을 합니다.<br>따라서 rpcbind 서비스를 재기동하시면 nfs 서비스를 재기동하여 관련 프로세스 포트가 rpcbind에<br>의하여 지정되게 됩니다.<br><br>예제,<br># /etc/init.d/rpcbind restart<br># rpcinfo -s<br>   program version(s) netid(s)                         service     owner<br>    100000  2,3,4     local,udp,tcp,udp6,tcp6          portmapper  superuser<br># /etc/init.d/nfs restart<br># rpcinfo -s<br>   program version(s) netid(s)                         service     owner<br>    100000  2,3,4     local,udp,tcp,udp6,tcp6          portmapper  superuser<br>    100011  2,1       tcp,udp                          rquotad     superuser<br>    100005  3,2,1     tcp6,udp6,tcp,udp                mountd      superuser<br>    100003  4,3,2     udp6,tcp6,udp,tcp                nfs         superuser<br>    100227  3,2       udp6,tcp6,udp,tcp                nfs_acl     superuser<br>    100021  4,3,1     tcp6,udp6,tcp,udp                nlockmgr    superuser<br><br>현재 이미 재기동으로 서비스가 정상 사용중이기 때문에 이슈 발생시 관련 로그가 있는지를 확인해 보실수 있으십니다. sosreport를 올려주시면 관련 에러가 있는지 확인해 보도록 하겠습니다.<br><br>2. 동일한 이슈 발생시 분석에 필요한 내용이나 명령어들이 있으면 알려주세요 동일한 증상 발생시 자세한 분석에 필요한 내용이 있으면 부탁드립니다<br>rpcbind의 상태, nfs 서비스의 상태 등...<br><br># service rpcbind status<br># service nfs status<br># rpcinfo -s<br><br>또한 디버깅을 활성화하여 더 많은 정보를 수집하실수 있으십니다.<br><br>rpcdebug -m nfs -s all<br>rpcdebug -m nfsd -s all<br>rpcdebug -m rpc -s all<br>rpcdebug -m nlm -s all<br><br>감사합니다.<br><br><publishedDate>2017-11-15T02:23:56Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Ky8daIAB"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-11-14T07:36:21Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-11-14T07:36:21Z</b><br><br>우선 답변 감사합니다<br><br>실제로 rpcbind를 start해준 후에 nfs서비스를 재시작해줌으로써 이슈 해소가 되었는데요<br><br>고객이 몇가지 궁금한 사항이 있어서 추가 문의 드립니다<br><br>1.  rpcbind를 시작해주고 nfs를 재시작 해줘야 하는 이유를 자세하게 내용을 받을수 있을까요? 연관성에 대해서 설명해주시면 감사하겠습니다<br>sosreport 내에 해당 내용이 포함되어 있는건가요?<br><br><br>2. 동일한 이슈 발생시 분석에 필요한 내용이나 명령어들이 있으면 알려주세요 동일한 증상 발생시 자세한 분석에 필요한 내용이 있으면 부탁드립니다<br>rpcbind의 상태, nfs 서비스의 상태 등...<br><br>감사합니다<br><br><publishedDate>2017-11-14T07:36:21Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000Ky66GIAR"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-11-14T02:41:03Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-11-14T02:41:03Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>일반적으로 rpcbind를 서비스를 재기동하면 관련 설정이 NFS에 반영되기 위하여<br>NFS 서비스도 함께 재시작해주셔야 합니다.<br><br>현상에 의하면 이번 이슈는 NFS와 rpcbind 서비스 사이에 알수 없는 원인으로 통신이 되지 <br>않아 발생된 것으로 보이며 순서적으로 rpcbind, nfs-server 서비스 재기동으로<br>이슈가 해소된 것으로 보입니다.<br><br>감사합니다.<br><br><publishedDate>2017-11-14T02:41:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Kxop1IAB"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-11-13T06:34:35Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-11-13T06:34:35Z</b><br><br>rpcbind를 재시작하고 nfslock 데몬을 재시작 하여도 <br><br>root@PDOCAL01SL /root # showmount -e localhost<br>clnt_create: RPC: Unable to send<br><br>위와 같은 증상이 있었습니다<br>service nfs restart 를 실행하였고 증상이 없어졌습니다<br><br>root@PDOCAL01SL /root # showmount -e localhost<br>Export list for localhost:<br>/views_raw localhost<br>/views_hs  localhost<br><br>어떤 원인이 있을까요?  확인 부탁드립니다<br><br><publishedDate>2017-11-13T06:34:35Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KxoerIAB"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-11-13T06:15:04Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-11-13T06:15:03Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>NFSv4 버전으로 마운트 작업시에는 rpcbind 서비스를 사용하지 않으십니다.<br>즉 rpcbind 서비스가 기동되지 않아도 NFSv4 버전으로 마운트 작업을<br>하시면 정상적으로 마운트 됩니다.<br><br>showmount 명령어는 rpcbind와 통신을 하기 때문에 rpcbind가<br>기동되어 있지 않으시다면 사용하실수 없으십니다.<br><br>감사합니다.<br><br><publishedDate>2017-11-13T06:15:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>