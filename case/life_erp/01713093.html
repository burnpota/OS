======================<br><b>생성계정 : 우택 심</b><br><b>생성날짜 : 2016-09-30T04:16:38Z</b><br><b>마지막 답변자 : Jake Jaewook Shin</b><br><b>마지막 수정 일자 : 2016-10-06T04:32:07Z</b><br><b>id : 500A000000VDqZuIAL</b><br>======================<br><br><b><font size=15>
제목  : Network Packet Drop 현상 문의 드립니다.
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요.<br><br>이번에 보험ERP 전 시스템 점검을 진행 중 아래와 같이 Packet Drop 현상을 발견하였습니다.<br><br>VMWare 가상화 환경에서는 대부분 ifconfig 또는 netstat -ni 로 볼 때 Drop Count 가 증가되어 있음을 확인하였습니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>우선 ifconfig 와 netstat -ni 로 확인 했을 때의 결과 입니다.<br><br>root@PBISAL08SL /root # ifconfig<br>eno16780032: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 100.254.143.68  netmask 255.255.255.0  broadcast 100.254.143.255<br>        ether 00:50:56:bc:65:ae  txqueuelen 1000  (Ethernet)<br>        RX packets 2932530666  bytes 567920047139 (528.9 GiB)<br>        RX errors 0  dropped 46793  overruns 0  frame 0<br>        TX packets 2836856994  bytes 410290705859 (382.1 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>eno33559296: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.18.13.68  netmask 255.255.255.0  broadcast 172.18.13.255<br>        ether 00:50:56:bc:0d:8b  txqueuelen 1000  (Ethernet)<br>        RX packets 7753605  bytes 571374405 (544.9 MiB)<br>        RX errors 0  dropped 781  overruns 0  frame 0<br>        TX packets 12646012  bytes 649101545948 (604.5 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536<br>        inet 127.0.0.1  netmask 255.0.0.0<br>        loop  txqueuelen 0  (Local Loopback)<br>        RX packets 311679621  bytes 32687828978 (30.4 GiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 311679621  bytes 32687828978 (30.4 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>root@PBISAL08SL /root # netstat -ni<br>Kernel Interface table<br>Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg<br>eno16780  1500 2932533311      0  46793 0      2836859601      0      0      0 BMRU<br>eno33559  1500  7753607      0    781 0      12646015      0      0      0 BMRU<br>lo       65536 311679817      0      0 0      311679817      0      0      0 LRU<br><br>RX Buffer 와 Drop 확인을 하기 위해 아래와 같이 ethtool 로도 확인 하였습니다.<br><br>root@PBISAL08SL /root # ethtool -g eno16780032<br>Ring parameters for eno16780032:<br>Pre-set maximums:<br>RX:             4096<br>RX Mini:        0<br>RX Jumbo:       2048<br>TX:             4096<br>Current hardware settings:<br>RX:             4096<br>RX Mini:        0<br>RX Jumbo:       128<br>TX:             4096<br><br>root@PBISAL08SL /root # ethtool -S eno16780032 | grep -i drop<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 45979<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br><br>마지막으로 /proc/net 에서 dev 와 softnet_stat 확인 결과 입니다.<br><br>root@PBISAL08SL /root # cat /proc/net/dev<br>Inter-|   Receive                                                |  Transmit<br> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed<br>eno33559296: 571374645 7753608    0  781    0     0          0         0 649101546654 12646017    0    0    0     0       0          0<br>    lo: 32688451416 311687179    0    0    0     0          0         0 32688451416 311687179    0    0    0     0       0          0<br>eno16780032: 567927663983 2932592693    0 46793    0     0          0         0 410298450006 2836918979    0    0    0     0       0          0<br><br>root@PBISAL08SL /root # cat /proc/net/softnet_stat<br>b63e7917 00000000 000143b2 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>024323be 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>016f4e2f 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>013f91d2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>010dfe30 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00ed8906 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00d05d0a 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c5624d 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c20b22 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c123c5 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c4885a 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00cc9f26 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br><br>언제 문제가 발생했습니까? 이러한 문제가 자주 발생합니까? 반복적으로 발생합니까? 특정 시간에 발생합니까?<br><br>전체 시스템에서 전반적으로 적게는 50 많게는 20,000 개 이상의 Drop Count 가 발견되었으며,<br><br>시스템에 rx, tx ring buffer 설정과 sysctl.conf 의 net.core.somaxconn, net.ipv4.tcp_max_syn_backlog 외에는 별도로 추가 튜닝을 하지는 않았습니다.<br><br>이와 관련하여 net.core.netdev_max_backlog 등의 추가 튜닝이 필요한지 여부 확인 부탁 드립니다.<br><br>추가로 ifconfig, netstat -ni 로 drop 이 증가되었지만, 실제 ethtool -S 로 봤을 때 drop 이 0 인 시스템들이 있는데요,<br><br>ifconfig, netstat 와 ethtool -S 결과 값이 서로 틀린 이유에 대해 확인 부탁 드립니다.<br><br>문제 해결 기간 및  긴급도와 관련된 정보를 제공해 주시겠습니까?<br><br>dropbox.redhat.com/incoming 에 sosreport-PBISAL08SL-20160930111633.tar.xz 파일명으로 sosreport 를 업로드 하였습니다.<br><br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><br><comment id="a0aA000000HwfpvIAB"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2016-10-06T04:32:07Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2016-10-06T04:32:07Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>해당 케이스를 직접 close 처리해주셔서 감사합니다!<br><br>케이스가 처리 완료되면 고객 설문조사 메일이 발송됩니다.<br>고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해,<br>소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-10-06T04:32:07Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Hwf01IAB"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-10-06T02:12:42Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-10-06T02:12:42Z</b><br><br>감사합니다.<br><br>향후 성능 튜닝 시 문제점이 있으면 다시 케이스를 오픈하도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-10-06T02:12:42Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000Hw9BbIAJ"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2016-10-04T02:30:30Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2016-10-04T02:40:57Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service를 이용해주셔서 감사합니다.<br><br>문의하신 내용에 대하여 안내를 드리면 다음과 같습니다.<br><br>sosreport 를 토대로 네트워크 관련 부분을 정리해보면 다음과 같습니다.<br>~~~<br>ETHTOOL<br>  Interface Status:<br>    eno16780032  0000:0b:00.0  link=up 10000Mb/s full (autoneg=N)  rx ring 4096/4096  drv vmxnet3 v1.3.5.0-k-NAPI / fw UNKNOWN<br>    eno33559296  0000:13:00.0  link=up 10000Mb/s full (autoneg=N)  rx ring 4096/4096  drv vmxnet3 v1.3.5.0-k-NAPI / fw UNKNOWN<br>  Interface Errors:<br>    eno16780032  Rx Queue#: 4<br>                   pkts rx OOB: 45979<br>                   drv dropped rx total: 45979<br>                      err: 45979<br><br>SOFTIRQ<br>  Backlog max is sufficient (Current value: net.core.netdev_max_backlog = 1000)<br>  Budget is not sufficient, needs to be increased! (Current value: net.core.netdev_budget = 300)<br><br>NETDEV<br>  Interface    RxMiBytes  RxPackets  RxErrs  RxDrop      RxFifo  RxComp  RxFrame  RxMultCast<br>  =========    =========  =========  ======  ======      ======  ======  =======  ==========<br>  eno16780032  541605     2932497 k  0       46793 (0%)  0       0       0        0 <br>  eno33559296  545        7754 k     0       781 (0%)    0       0       0        0 <br>  - - - - - - - - - - - - - - - - -<br>  Interface    TxMiBytes  TxPackets  TxErrs  TxDrop      TxFifo  TxComp  TxColls  TxCarrier <br>  =========    =========  =========  ======  ======      ======  ======  =======  ==========<br>  eno16780032  391251     2836825 k  0       0           0       0       0        0 <br>  eno33559296  619031     12646 k    0       0           0       0       0        0 <br>~~~<br>현재, eno16780032 에서 패킷드랍(pkts rx 00B)이 발생하고 있는 것이 확인됩니다.<br><br>이와 관련하여 다음 KBase 가 보고되어 있으며 버퍼를 넘어서는 패킷으로 인한 패킷 드랍 이슈로 확인됩니다.<br><br>Receiving errors on vmware interface 'pkts rx OOB' <br>- https://access.redhat.com/solutions/1751293<br><br>일반적으로 링버퍼가 부족할 경우 패킷 드랍이 발생할 수 있습니다만,<br>현재 이미 최대값으로 조정하신 것으로 보이므로 기타 다른 튜닝요소를 고려해보실 수 있습니다.<br><br>이와 관련하여 다음 KBase 들을 참고하셔서 관련 값들을 조정해보시기 바랍니다.<br><br>How to tune `net.core.netdev_max_backlog` and `net.core.netdev_budget` sysctl kernel tunables? <br>- https://access.redhat.com/solutions/1241943<br><br>What are the recommendations for 10 Gigabit network adapter tuning parameters? <br>- https://access.redhat.com/solutions/127143<br><br>참고로 eno33559296 의 경우 ethtool 상 패킷드랍이 확인되지 않습니다. 상위에서 표시되는 781 의 경우 현재 사용하고 계신 네트워크 서비스의<br>이전 값이 남아 있는 것으로 보이며 현재는 패킷드랍이 발생하지 않는 것으로 보입니다.<br><br>이와 관련하여 ethtool과 ifcofing 및 netstat 의 값에 차이가 나는 이유에 대해서는 다음 KBase 를 참조하시기 바랍니다.<br><br>Why are packet drops reported in ifconfig not reflected in ethtool? <br>- https://access.redhat.com/solutions/504293<br><br>즉, 구현의 차이로 인해 ethtool 값과 ifconfig 의 값에 차이가 있을 수 있으며, ethtool 이 보다 정확한 값을 나타냅니다.<br><br>마지막으로 네트워크 이용시의 패킷 드랍은 정상적인 동작입니다.<br>그러므로, 신뢰성을 중시하는 TCP 구현상 패킷드랍이 발생하여도 패킷 재전송을 통해<br>데이터 정합성을 보장하며, 트래픽 대비 0 % 에 수렴하는 패킷 드랍율을 나타낸다면 이것이 문제다라고 단정짓기는 어렵습니다.<br><br>관련하여 다음 KBase 를 참고하시기 바랍니다.<br><br>Should I be concerned about a 0.05% packet drop rate? <br>- https://access.redhat.com/solutions/742043<br><br>만약, 여러 튜닝을 통해서도 만족하시는 패킷 드랍율을 확보하지 못하셨을 경우에는 이는 결국<br>트래픽에 대한 시스템의 성능한계로 볼 수도 있으며 본딩 등을 통해 트래픽을 분산하는 방법을 생각하시어<br>패킷 드랍율을 낮추는 방법도 생각해보실 필요가 있습니다.<br><br>RHEL network interface dropping packets <br>- https://access.redhat.com/solutions/21301<br><br>감사합니다.<br><br><publishedDate>2016-10-04T02:30:30Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HesMNIAZ"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2016-09-30T07:21:18Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2016-09-30T07:21:18Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service를 이용해주셔서 감사합니다.<br><br>저는 신재욱 과장이라고 하며 앞으로 해당 케이스를 담당하게 되었습니다.<br>현재 케이스의 내용을 살펴보는 중이며, 관련하여 업데이트 드리도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-09-30T07:21:18Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br><br>이번에 보험ERP 전 시스템 점검을 진행 중 아래와 같이 Packet Drop 현상을 발견하였습니다.<br><br>VMWare 가상화 환경에서는 대부분 ifconfig 또는 netstat -ni 로 볼 때 Drop Count 가 증가되어 있음을 확인하였습니다.</issue><environment>우선 ifconfig 와 netstat -ni 로 확인 했을 때의 결과 입니다.<br><br>root@PBISAL08SL /root # ifconfig<br>eno16780032: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 100.254.143.68  netmask 255.255.255.0  broadcast 100.254.143.255<br>        ether 00:50:56:bc:65:ae  txqueuelen 1000  (Ethernet)<br>        RX packets 2932530666  bytes 567920047139 (528.9 GiB)<br>        RX errors 0  dropped 46793  overruns 0  frame 0<br>        TX packets 2836856994  bytes 410290705859 (382.1 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>eno33559296: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.18.13.68  netmask 255.255.255.0  broadcast 172.18.13.255<br>        ether 00:50:56:bc:0d:8b  txqueuelen 1000  (Ethernet)<br>        RX packets 7753605  bytes 571374405 (544.9 MiB)<br>        RX errors 0  dropped 781  overruns 0  frame 0<br>        TX packets 12646012  bytes 649101545948 (604.5 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536<br>        inet 127.0.0.1  netmask 255.0.0.0<br>        loop  txqueuelen 0  (Local Loopback)<br>        RX packets 311679621  bytes 32687828978 (30.4 GiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 311679621  bytes 32687828978 (30.4 GiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>root@PBISAL08SL /root # netstat -ni<br>Kernel Interface table<br>Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg<br>eno16780  1500 2932533311      0  46793 0      2836859601      0      0      0 BMRU<br>eno33559  1500  7753607      0    781 0      12646015      0      0      0 BMRU<br>lo       65536 311679817      0      0 0      311679817      0      0      0 LRU<br><br>RX Buffer 와 Drop 확인을 하기 위해 아래와 같이 ethtool 로도 확인 하였습니다.<br><br>root@PBISAL08SL /root # ethtool -g eno16780032<br>Ring parameters for eno16780032:<br>Pre-set maximums:<br>RX:             4096<br>RX Mini:        0<br>RX Jumbo:       2048<br>TX:             4096<br>Current hardware settings:<br>RX:             4096<br>RX Mini:        0<br>RX Jumbo:       128<br>TX:             4096<br><br>root@PBISAL08SL /root # ethtool -S eno16780032 | grep -i drop<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped tx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 45979<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br>       drv dropped rx total: 0<br><br>마지막으로 /proc/net 에서 dev 와 softnet_stat 확인 결과 입니다.<br><br>root@PBISAL08SL /root # cat /proc/net/dev<br>Inter-|   Receive                                                |  Transmit<br> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed<br>eno33559296: 571374645 7753608    0  781    0     0          0         0 649101546654 12646017    0    0    0     0       0          0<br>    lo: 32688451416 311687179    0    0    0     0          0         0 32688451416 311687179    0    0    0     0       0          0<br>eno16780032: 567927663983 2932592693    0 46793    0     0          0         0 410298450006 2836918979    0    0    0     0       0          0<br><br>root@PBISAL08SL /root # cat /proc/net/softnet_stat<br>b63e7917 00000000 000143b2 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>024323be 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>016f4e2f 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>013f91d2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>010dfe30 00000000 00000002 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00ed8906 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00d05d0a 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c5624d 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c20b22 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c123c5 00000000 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00c4885a 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000<br>00cc9f26 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</environment><periodicityOfIssue>전체 시스템에서 전반적으로 적게는 50 많게는 20,000 개 이상의 Drop Count 가 발견되었으며,<br><br>시스템에 rx, tx ring buffer 설정과 sysctl.conf 의 net.core.somaxconn, net.ipv4.tcp_max_syn_backlog 외에는 별도로 추가 튜닝을 하지는 않았습니다.<br><br>이와 관련하여 net.core.netdev_max_backlog 등의 추가 튜닝이 필요한지 여부 확인 부탁 드립니다.<br><br>추가로 ifconfig, netstat -ni 로 drop 이 증가되었지만, 실제 ethtool -S 로 봤을 때 drop 이 0 인 시스템들이 있는데요,<br><br>ifconfig, netstat 와 ethtool -S 결과 값이 서로 틀린 이유에 대해 확인 부탁 드립니다.</periodicityOfIssue><timeFramesAndUrgency>dropbox.redhat.com/incoming 에 sosreport-PBISAL08SL-20160930111633.tar.xz 파일명으로 sosreport 를 업로드 하였습니다.<br><br>감사합니다.</timeFramesAndUrgency><cep>false</cep></case>