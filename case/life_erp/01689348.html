======================<br><b>생성계정 : jimin kim</b><br><b>생성날짜 : 2016-08-23T08:41:12Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2016-09-10T10:05:36Z</b><br><b>id : 500A000000V6oSOIAZ</b><br>======================<br><br><b><font size=15>
제목  : lvol과 실제 mount 된 용량의 차이에 대한 문의 입니다.
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요<br><br>SDS 상암센터 김지민 선임입니다.<br><br>보험ERP Mig 서버 관련해서 문의를 드리려고 합니다.<br><br>ext4 파일시스템을 사용하는 redhat 6.5 OS에서<br>lvol과 실제 mount 된 용량의 차이에 대한 문의를 드리려고 합니다.<br><br>아래에서 보면 1023G의 lvol을 만든 상태인데요.<br>[root@PIDSEL01SL:/root] lvs | grep sapdata43<br>  oracle_MZ0_sapdata43 LLIDSDATVG9  -wi-ao---- 1023.00g<br><br>아래와 같이 mount를 하여서 확인해보면 Size가 1007G로 mount가 됩니다.<br>Used(931G)와 Avail(26G)을 계산해보면 1007G와는 다른 값(957G)를 가지게 되는데 파일시스템이 이렇게 마운트되는 원인 관련해서 문의를 드리려고 합니다.<br><br>[root@PIDSEL01SL:/root] df -Ph | grep -i sapdata43<br>/dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43  1007G  931G   26G  98% /oracle/MZ0/sapdata43<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.4</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 4 (Low)</b><br>======================<br><comment id="a0aA000000HkVHXIA3"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-08-25T00:30:24Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-08-25T00:30:24Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>&gt;  조금 자세히 설명을 부탁드리려고 합니다.<br><br>우선 reserved 영역에 대해서 설명을 드리면, 해당 영역은 root가 사용할 수 있는 영역으로 일반 유저는 사용이 불가능 한 영역입니다.<br>이러한 영역을 두는 이유는 root 유저가 혹시라도 I/O처리를 하지 못함에 따라 시스템이 hang이 되고 문제가 되는 것을 방지하기 위해서 주어지는 영역으로써 일반적으로 rsyslog 같은 부분들이 향후 일반유저들이 파일시스템을 다 쓰더라도 사용할 수 있는 여유 공간을 제공하게 되는 역할을 하게 됩니다.<br><br>또한 해당 영역을 통해서 data의 fragmentation이 발생하는 부분도 조절할때 이용이 되기도 합니다. 그러다 보니 해당 영역이 결과적으로 성능 향상에도 도움이 되는 것으로 일반적으로 알려져있습니다.<br>======================<br>&gt; 파일시스템 생성 후 mount 시 기본적으로 5%에 해당하는 사이즈를 reserve하게 됩니다. 그래서 약 50G(13408665*4MB)의 reserve block이 생기고 이유는 아래와 같습니다. 5% 값이 조정은 가능한 것 같은데 redhat의 기본 권고 사이즈여서 확인은 필요해 보입니다.<br><br>이미 아시는 것 처럼, 현재 5%를 기본적으로 잡는 것이 시스템의 기본설정이고 Red Hat의 권고는 기본 설정값이라고 보시면 됩니다. 물론 확인하신 것 처럼 사이즈 조절이 가능은 하나, 해당 영역이 root 유저에 의해서 실제로는 사용되고 있는 영역이기 때문에 시스템 성능 그리고.. 향후 파일시스템이 full이 되는 상황에서 로그 등의 IO를 보장하기 위해서 적절한 값을 유지할 수 있도록 관리해주시는 것을 권장 드립니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-08-25T00:30:24Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HkIFBIA3"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2016-08-24T05:16:02Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2016-08-24T05:16:02Z</b><br><br>답변 감사드립니다. 위의 내용과 같이 계산해서 보니 1007G의 내용은 확인이 가능한 것 같습니다.<br>추가로 1007G지만 931G+26G해서 957G로 보이는 것에 대해서 확인해보니 <br>reserved 부분이 관련이 있어 보이는데 reserved 부분을 tune2fs의 man page에서 확인 했을 때 특정 process에 대해서 할당하기 위해서 남겨놓는 부분으로 설명이 되어있는 것 같은데요. 여기에 대해서 조금 자세히 설명을 부탁드리려고 합니다. 그리고 해당 %를 조정이 가능한지도 문의드리려고 합니다.<br><br>[root@PIDSEL01SL:/] df -Ph | grep sapdata43<br>/dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43  1007G  931G   26G  98% /oracle/MZ0/sapdata43<br>[root@PIDSEL01SL:/] tune2fs -l /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;Block count:&quot;<br>Block count:              268173312<br>[root@PIDSEL01SL:/] tune2fs -l /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;First block:&quot;<br>First block:              0<br>[root@PIDSEL01SL:/] dumpe<br>dumpe2fs  dumpet<br>[root@PIDSEL01SL:/] dumpe<br>dumpe2fs  dumpet<br>[root@PIDSEL01SL:/] dumpe2fs /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep superblock | wc -l<br>dumpe2fs 1.41.12 (17-May-2010)<br>19<br>[root@PIDSEL01SL:/] dumpe2fs /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;Group descriptors&quot; | wc -l<br>dumpe2fs 1.41.12 (17-May-2010)<br>19<br>[root@PIDSEL01SL:/] dumpe2fs /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep ^Group | wc -l<br>dumpe2fs 1.41.12 (17-May-2010)<br>8184<br>[root@PIDSEL01SL:/] tune2fs -l /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;Inode blocks per group:&quot;<br>Inode blocks per group:   512<br>[root@PIDSEL01SL:/] dumpe2fs /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;Journal length&quot;<br>dumpe2fs 1.41.12 (17-May-2010)<br>Journal length:           32768<br><br>0 + 19 + 19 + (2 + 512)*8184 + 32768 = 4239382<br>block 1당 4MB이기 때문에 4239382*4/1024/1024 하면 16G 정도 차이가 나는 것을 확인이 가능합니다.<br>1023G - 16G 하면 1007G 정도 됩니다.<br><br>[root@PIDSEL01SL:/root] tune2fs -l /dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43 | grep &quot;Reserved block count:&quot;<br>tune2fs 1.41.12 (17-May-2010)<br>Reserved block count:     13408665<br><br>파일시스템 생성 후 mount 시 기본적으로 5%에 해당하는 사이즈를 reserve하게 됩니다. 그래서 약 50G(13408665*4MB)의 reserve block이 생기고 이유는 아래와 같습니다. 5% 값이 조정은 가능한 것 같은데 redhat의 기본 권고 사이즈여서 확인은 필요해 보입니다.<br><br>-m reserved-blocks-percentage<br>     Set the percentage of the filesystem which may only be allocated by privileged processes.   Reserving some number of filesystem blocks for use  by  privileged processes  is  done to avoid filesystem fragmentation, and to allow system daemons, such as syslogd(8), to continue to function correctly after non-privileged processes are prevented from writing to the filesystem.  Normally, the default percentage of reserved blocks is 5%.<br><br><publishedDate>2016-08-24T05:16:02Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HkHzaIAF"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-08-24T04:39:52Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-08-24T04:39:52Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>우선 lvs와 df에서 보이는 수치는 다를 수 있습니다.  df는 파일시스템에 대한 부분이 고려가 되기 때문에 파일시스템에서 사용하는 inode 라든지 기타 사용을 위해 차지하는 영역들이 존재합니다.<br><br><br>이와 관련되어 계산되는 방식을 설명한 문서가 있어서 아래 첨부해드립니다.<br>문서를 보시면 조금 더 이해가 쉬울 것 같아서 문서를 첨부드립니다.<br><br><br>Red Hat Document(KCS), https://access.redhat.com/solutions/109623<br>======================<br>감사합니다.<br><br><publishedDate>2016-08-24T04:39:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HkGRVIA3"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-08-24T00:22:03Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-08-24T00:22:03Z</b><br><br>안녕하세요,<br>Red Hat Technical Account Manager 한진구 입니다.<br><br><br>문의주신 내용에 대해서 검토 후, 업데이트 드리도록 하겠습니다.<br><br><br>감사합니다.<br><br><publishedDate>2016-08-24T00:22:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br><br>SDS 상암센터 김지민 선임입니다.<br><br>보험ERP Mig 서버 관련해서 문의를 드리려고 합니다.<br><br>ext4 파일시스템을 사용하는 redhat 6.5 OS에서<br>lvol과 실제 mount 된 용량의 차이에 대한 문의를 드리려고 합니다.<br><br>아래에서 보면 1023G의 lvol을 만든 상태인데요.<br>[root@PIDSEL01SL:/root] lvs | grep sapdata43<br>  oracle_MZ0_sapdata43 LLIDSDATVG9  -wi-ao---- 1023.00g<br><br>아래와 같이 mount를 하여서 확인해보면 Size가 1007G로 mount가 됩니다.<br>Used(931G)와 Avail(26G)을 계산해보면 1007G와는 다른 값(957G)를 가지게 되는데 파일시스템이 이렇게 마운트되는 원인 관련해서 문의를 드리려고 합니다.<br><br>[root@PIDSEL01SL:/root] df -Ph | grep -i sapdata43<br>/dev/mapper/LLIDSDATVG9-oracle_MZ0_sapdata43  1007G  931G   26G  98% /oracle/MZ0/sapdata43</issue><cep>false</cep></case>