======================<br><b>생성계정 : 우택 심</b><br><b>생성날짜 : 2016-02-04T14:43:08Z</b><br><b>마지막 답변자 : JINKOO HAN</b><br><b>마지막 수정 일자 : 2016-03-07T02:03:54Z</b><br><b>id : 500A000000TSvWvIAL</b><br>======================<br><br><b><font size=15>
제목  : Multipath fail 된 path 에 대한 자동 복구 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요.<br><br>SAN Storage 테스트 중 한가지 문제점이 발견되어 문의 드립니다.<br><br>Storage Controller 또는 SAN Switch 중 한개를 임의 Power Down 시 Multipath 에서는 정상적으로 fail faulty 상태로 장애를 잘 감지 합니다.<br><br>하지만 시간이 좀 지나 multipath -ll 에서 path 장치가 사라지고 (dev_loss) 이 상태에서 Storage Controller 또는 SAN Switch 를 다시  Power on 하게 되면,<br><br>정상적으로 Multipath 에 SCSI LUN 에 대한 Path 가 자동으로 추가가 안되는 현상이 발견 되었습니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>예를 들어,  정상적인 Multipath 상태는 아래와 같습니다.<br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  |- 2:0:1:1  sdae 65:224 active ready running<br>  |- 3:0:0:1  sdaj 66:48  active ready running<br>  `- 3:0:1:1  sdag 66:0   active ready running<br><br>이 상태에서 1개 path 에 문제가 생기면 <br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  |- 2:0:1:1  sdae 65:224 failed faulty running<br>  |- 3:0:0:1  sdaj 66:48  active ready running<br>  `- 3:0:1:1  sdag 66:0   failed faulty running<br><br>상태가 되고, dev_loss 가 된 후 장치가 아주 사라지게 되면 <br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  `- 3:0:0:1  sdaj 66:48  active ready running<br><br>이와 같은 상태가 됩니다.<br><br>하지만 단절 된 1 path 가 다시 Active 가 되면 즉시 multipath 복구가 이루어 지지 않고 scsi-rescan 명령 또는 /sys/class/scsi_host/host1/scan 같은 수동 스캔을 통해서만<br><br>SCSI 볼륨을 인식하여 multipath 가 복구 되는 문제가 있습니다.<br><br>언제 문제가 발생했습니까? 이러한 문제가 자주 발생합니까? 반복적으로 발생합니까? 특정 시간에 발생합니까?<br><br>이와 같은 문제는 오늘 발생 한 I/O Error 문제점 역추적 과정에서 발견하게 되었으며,<br><br>실제로 multipath 가 제대로 복구 안된 상태에서 추가 가용성 테스트가 진행되어 모든 볼륨이 off 되는 심각한 장애가 발생되었습니다.<br><br>CEE 에 문의드리는 질문을 정리하자면 아래와 같습니다.<br><br>1. multipath 또는 Qlogic (qla2xxx) 드라이버를 통해 추가 변경되는 LUN 에 대한 이벤트를 자동으로 감지할 수 있는 방법.<br><br>2. 장애를 유발한 후 multipath 의 path 상태가 failed faulty running 되고 얼마의 시간이 흐른뒤에 dev_loss 가 되어 완전히 multipath 에서 사라지는지. (설정 파라미터)<br><br>2가지 사항에 대해 문의 드립니다.<br><br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><br><comment id="a0aA000000GdBoaIAF"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-03-07T02:03:52Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-03-07T02:03:52Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>약 4주간 본 케이스에 추가 업데이트가 없어서 본 케이스를 Close하도록 하겠습니다.<br><br>본 케이스가 Close되더라도 추가문의사항이 있으시면 해당 케이스를 &quot;Re-open&quot;하셔도 되고, 새로운 문의사항은 새로운 케이스를 오픈하시면 해당 케이스를 통해서 지원하도록 하겠습니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-03-07T02:03:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GYIzEIAX"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-17T01:59:58Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-17T01:59:57Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>우선 내용을 아래와 같이 정리하여 드립니다.<br><br>1. Storage와 서버간의 path에 이상이 있을경우(Storage 이슈, HBA, cable 등등), 일정 시간(30초)에 해당 path는 loss하게 되며, 이후에는 path를 수동적으로 rescan하셔서 복구 하셔야 합니다.<br><br># echo &quot;c t l&quot; &gt;  /sys/class/scsi_host/hostH/scan<br><br>참고문서: https://access.redhat.com/solutions/3941<br><br><br>2. 추가적으로 Storage가 GAD(Role Change)되었다고는 하나, 로그상으로 보면 SCSI Layer에서 일부 path가 연결이 되지 않았던 것으로 보입니다. 그리고 그로인하여 장애가 발생했던 것으로 유추됩니다.<br><br>42645:Feb  4 09:48:56 PECCAL01SL kernel: sd 2:0:0:0: [sdaj] tag#0 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE<br>42646:Feb  4 09:48:56 PECCAL01SL kernel: sd 2:0:0:0: [sdaj] tag#0 Sense Key : Illegal Request [current] <br>42647:Feb  4 09:48:56 PECCAL01SL kernel: sd 2:0:0:0: [sdaj] tag#0 Add. Sense: Logical unit not supported<br>42648:Feb  4 09:48:56 PECCAL01SL kernel: sd 2:0:0:0: [sdaj] tag#0 CDB: Write(10) 2a 00 01 80 ec 60 00 04 00 00<br><br>이 부분은 Storage 벤더하고의 확인을 해볼 필요가 있다고 생각되며, 만약 GAD를 한 후에 Storage Controller가 Ready하기까지 필요한 시간이 얼마나 필요한지에 대한 부분을 확인하여 현재 multipath가 장애시 path가 인식되어야 할 설정에 대해서는 Stroage Vendoer쪽에 확인하시여 설정하시는 것을 권장 드립니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-02-17T01:59:57Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GX7sFIAT"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-02-11T06:03:42Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-02-11T06:03:42Z</b><br><br>안녕하세요.<br><br>이번 이슈와 관련해서 한 가지 추가 의문점을 드리자면, <br><br>물리적으로 SAN 스위치 또는 Storage Controller 절체 후 다시 원복을 하게 되면, <br><br>일부 LUN 은 자동으로 Recovery 되지만, 일부 LUN 은 처음 문의 드린것 처럼 scsi-rescan 후 복원이 된다는 것 입니다.<br><br>항상 동일한 증상이 아닌 매번 테스트 할 때마다 LUN Recovery 시점이 장비마다 틀립니다.<br><br>이번 문제에 대해 스토리지 가용성 테스트를 다시 진행 할 예정입니다.<br><br>감사합니다.<br><br><publishedDate>2016-02-11T06:03:42Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GX6X7IAL"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-11T01:11:08Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-11T01:39:19Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>앞서 저희 엔지니어가 update하였듯이, dev_loss가 발생하는 것은 정상적인 상황이며 이로 인하여 lun이 삭제될 경우, 수동으로 다시 Rescan을 하셔야 합니다.<br><br>만약 이러한 부분을 막기 위해서는 앞서 안내한 것처럼 multipath.conf에 아래 설정을 추가해주시기 바랍니다.<br><br><br>/etc/multipath.conf<br><br>defaults {<br>        polling_interval        5<br>        user_friendly_names     yes<br>        fast_io_fail_tmo 5               #&lt;&lt;= Add this line<br>        dev_loss_tmo infinity            #&lt;&lt;= Add this line<br>}<br>======================<br># systemctl restart multipathd<br>======================<br>참고적으로, Red Hat은 Default로 storage와의 path가 죽을 경우에 삭제되는 것이 기본설정입니다. 왜냐하면, path가 지속적으로 장애가 발생하는 상황에서 사람의 개입이 불가피하며 또한 어떤 하드웨어 장치가 변경될 경우 해당 패스가 스스로 살아나지는 않을 것이기 때문입니다.  앞서서 infinity로 설명은 드렸으나, 해당 값을 기본 30초의 기본값에서 늘리는 방안을 생각하는 것도 좋은 선택사항이라 생각됩니다.<br><br><br>감사합니다.<br><br><publishedDate>2016-02-11T01:11:07Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GWzlWIAT"><br>======================<br><b>생성계정 : Magrini, Jon</b><br><b>생성날짜 : 2016-02-10T15:12:50Z</b><br><b>마지막 답변자 : Magrini, Jon</b><br><b>마지막 수정 일자 : 2016-02-10T15:12:50Z</b><br><br>This is normal behavior, starting with RHE6 offline devices are removed from the system once dev_loss_tmo expires.  The default dev_loss_tmo for fc target port is 30s which can be observed here:   <br>---<br>Feb  2 14:04:05 PECCAL01SL kernel: qla2xxx [0000:44:00.1]-500b:2: LOOP DOWN detected (2 7 0 0).<br>[..]<br>Feb  2 14:04:35 PECCAL01SL kernel: rport-2:0-1: blocked FC remote port time out: removing target and saving binding<br>Feb  2 14:04:35 PECCAL01SL kernel: rport-2:0-0: blocked FC remote port time out: removing target and saving binding<br>[..]<br>Feb  2 14:04:35 PECCAL01SL multipathd: sdo: remove path (uevent)<br>Feb  2 14:04:35 PECCAL01SL multipathd: DMP100G001: load table [0 209715200 multipath 0 0 1 1 service-time 0 1 1 8:240 1]<br>Feb  2 14:04:35 PECCAL01SL multipathd: sdo [8:224]: path removed from map DMP100G001<br><br>Once the devices are deleted, they will need to be manually rediscovered, ie: scsi bus rescan.  To alter this behavior and not remove devices on failure, an infinite amount of time for dev_loss_tmo and enabling fast_io_fail_tmo would need be set.  This can be done by adding the following to /etc/multipath.conf: <br><br>defaults {<br>        polling_interval        5<br>        user_friendly_names     yes<br>        fast_io_fail_tmo 5               #&lt;&lt;= Add this line<br>        dev_loss_tmo infinity            #&lt;&lt;= Add this line<br>}<br><br>To apply, reload multipathd: <br><br># systemctl restart multipathd<br><br><publishedDate>2016-02-10T15:12:50Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GAUZ2IAP"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-05T14:05:38Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-05T14:05:37Z</b><br><br>안녕하세요,<br>Red Hat Technical Account Manager 한진구 입니다.<br><br>보내주신 sosreport를 다운로드 받고 있으며, 확인 후, 답변드리도록 하겠습니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-02-05T14:05:37Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GATf8IAH"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-02-05T12:20:11Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-02-05T12:20:10Z</b><br><br>안녕하세요.<br><br>현재 설정 된 사항을 01578254_sosreport-PECCAL01SL-20160204103403.tar.xz 이름으로 dropbox 에 업로드 하였습니다.<br><br>참고로 오늘 테스트 해 본 결과<br><br>물리적으로 라인 또는 SAN Switch Power off 가 아닌 스토리지에서 LUN Mask 를 하게 되면, <br><br>HBA 의 port_state 및 fc_report_ports 상태가 모두 Online 상태이며, multipath 상태에서 device removed 가 일어나지 않아<br><br>path 가 다시 살아나면 정상적으로  복구가 되는 것을 확인 하였습니다.<br><br>현재까지 파악 된 시스템 내 주요 파라미터를 보면<br><br># systool -m qla2xxx -v<br>...<br>qlport_down_retry = 0<br><br>* multipath 의 dev_loss_tmo 값 : 30<br>* multipath 의 polling_interval 값 : 5<br>* multipath 의 no_path_retry 값 : fail<br><br>로 설정되어 있습니다.<br><br><publishedDate>2016-02-05T12:20:10Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br><br>SAN Storage 테스트 중 한가지 문제점이 발견되어 문의 드립니다.<br><br>Storage Controller 또는 SAN Switch 중 한개를 임의 Power Down 시 Multipath 에서는 정상적으로 fail faulty 상태로 장애를 잘 감지 합니다.<br><br>하지만 시간이 좀 지나 multipath -ll 에서 path 장치가 사라지고 (dev_loss) 이 상태에서 Storage Controller 또는 SAN Switch 를 다시  Power on 하게 되면,<br><br>정상적으로 Multipath 에 SCSI LUN 에 대한 Path 가 자동으로 추가가 안되는 현상이 발견 되었습니다.</issue><environment>예를 들어,  정상적인 Multipath 상태는 아래와 같습니다.<br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  |- 2:0:1:1  sdae 65:224 active ready running<br>  |- 3:0:0:1  sdaj 66:48  active ready running<br>  `- 3:0:1:1  sdag 66:0   active ready running<br><br>이 상태에서 1개 path 에 문제가 생기면 <br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  |- 2:0:1:1  sdae 65:224 failed faulty running<br>  |- 3:0:0:1  sdaj 66:48  active ready running<br>  `- 3:0:1:1  sdag 66:0   failed faulty running<br><br>상태가 되고, dev_loss 가 된 후 장치가 아주 사라지게 되면 <br><br>SAP100G002 (360060e8007c7fb000030c7fb00001004) dm-0 HITACHI ,OPEN-V<br>size=100G features='0' hwhandler='0' wp=rw<br>`-+- policy='service-time 0' prio=1 status=active<br>  |- 2:0:0:1  sdak 66:64  active ready running<br>  `- 3:0:0:1  sdaj 66:48  active ready running<br><br>이와 같은 상태가 됩니다.<br><br>하지만 단절 된 1 path 가 다시 Active 가 되면 즉시 multipath 복구가 이루어 지지 않고 scsi-rescan 명령 또는 /sys/class/scsi_host/host1/scan 같은 수동 스캔을 통해서만<br><br>SCSI 볼륨을 인식하여 multipath 가 복구 되는 문제가 있습니다.</environment><periodicityOfIssue>이와 같은 문제는 오늘 발생 한 I/O Error 문제점 역추적 과정에서 발견하게 되었으며,<br><br>실제로 multipath 가 제대로 복구 안된 상태에서 추가 가용성 테스트가 진행되어 모든 볼륨이 off 되는 심각한 장애가 발생되었습니다.<br><br>CEE 에 문의드리는 질문을 정리하자면 아래와 같습니다.<br><br>1. multipath 또는 Qlogic (qla2xxx) 드라이버를 통해 추가 변경되는 LUN 에 대한 이벤트를 자동으로 감지할 수 있는 방법.<br><br>2. 장애를 유발한 후 multipath 의 path 상태가 failed faulty running 되고 얼마의 시간이 흐른뒤에 dev_loss 가 되어 완전히 multipath 에서 사라지는지. (설정 파라미터)<br><br>2가지 사항에 대해 문의 드립니다.<br><br>감사합니다.</periodicityOfIssue><cep>false</cep></case>