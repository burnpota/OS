======================<br><b>생성계정 : 우택 심</b><br><b>생성날짜 : 2016-10-25T04:45:34Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2016-11-15T11:05:29Z</b><br><b>id : 500A000000VXaFiIAL</b><br>======================<br><br><b><font size=15>
제목  : Pacemaker 클러스터 관련 데몬 비정상 테스트 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요. <br><br>이번 프로젝트 가용성 테스트의 시나리오 중 비정상 종료에 관한 절차가 있어 아래와 같이 테스트를 진행하였습니다.<br><br>Case 1 : 한쪽노드에서 kill -9 corosync[PID]<br>첫번 째 결과 : 문제가 생긴 노드의 Resource 가 Stop -&gt; 다시 Start (Recovery) 됨.<br>두번 째 결과 : 문제가 생긴 노드가 Power Fencing 되어 Resource Failover 됨<br><br>Case 2 : kill -9 pacemakerd[PID]<br>결과 : 자동적으로 데몬이 Pacemakerd 다시 복구되어 서비스에 영향 없음<br><br>Case 3 : killall pacemakerd<br>결과 : 문제가 생긴 노드의 모든 클러스터 데몬이 종료 되고 Resource Failover 됨.<br><br>Case 4 : 양쪽 노드에서 killall pacemakerd<br>결과 : 전체 클러스터 서비스 Stop (pcs cluster stop --all 과 같은 결과)<br><br>Case 5 : 양쪽 노드에서 killall pacemakerd corosync<br>결과 : 각 클러스터 노드의 Resource 가 Stop -&gt; 다시 Start (Recovery) 됨.<br><br>위와 같은 결과를 얻었습니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>가용성 테스트 결과 kill -9 로 할 때와 killall (SIGTERM 15) 의 결과는 pacemakerd 의 경우 확실히 차이점을 알았는데요,<br><br>corosync 의 경우 두 가지 테스트 한 결과가 틀리게 나와 문의를 드리려고 합니다.<br><br>첫번 째 Power Fence 실패한 테스트의 경우 <br><br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]: warning: Processing failed op monitor for kdump_stonith on PBWSAL01SL-HB: not running (7)<br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]: warning: Processing failed op monitor for vmware_stonith2 on PBWSAL01SL-HB: not running (7)<br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]:  notice: Calculated Transition 4: /var/lib/pacemaker/pengine/pe-input-123.bz2<br>Oct 24 14:39:45 PBWSAL02SL crmd[5181]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on PBWSAL01SL-HB<br><br>이와 같이 Stonith 에 뭔가 문제가 있다고 보여지는데요, 두번 째 테스트에서는 문제 없이 정상적으로 Power Fencing 이 되었습니다.<br><br>corosync 가 문제가 생겼을 경우 fence 가 작동해야 하는 것으로 생각되어 문의를 드립니다.<br><br>관련 로그를 발췌하여 파일로 첨부하였습니다.<br><br>확인 부탁 드립니다.<br><br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 4 (Low)</b><br>======================<br><comment id="a0aA000000I9ieCIAR"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-11-01T04:03:12Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-11-01T04:03:12Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>먼저 양 노드의 sosreport를 보내 주셔서 대단히 감사드립니다. <br><br>관련 로그를 확인 할 결과를 아래와 같이 정리 하여 보내 드립니다. <br><br>1. systemd에 corosync/pacemaker 프로세스가 killed 될시 자동적으로 재시작 하게 <br>   설정 하신 것으로 보입니다. 이와 같이 설정 하지 않으면 corosync killed 될시 <br>   무조건 fencing 되는 것으로 추정 됩니다. <br><br><br>2. corosync/pacemaker killed 될시 자동적으로 재시작 하게 설정 하는 목적은 만약<br>   이와 같이 이슈가 발생 할시 fencing 없이 복구 됨을 원하시는 것으로 추측 하며 <br>   만약 그렇다면 heartbeat 을 약 5초 정도로 늘리면 corosync killed 될시 fencing<br>   발생 하지 않을 것입니다. <br><br><br>3. 두번 테스트 로그를 비교 한 결과 두번 째 테스트, 즉 fencing이 발생 되는 테스트에서<br>   pengine이 작동 하여 장애노드가 fencing 해야 됨을 판단을 하였습니다. <br><br>   하지만 첫 번째 로그에서는 pengine이 작동 하지 않고 crmd 에서 새로운 노드가 조인 <br>   되는 것으로 판단 하여 fencing이 발생 하지 않았습니다. 이와 같은 현상은 corosync가<br>   복귀 시간이 너무 짧아 pengine에서 fencing을 scheduling 하기전에 장애 노드가 이미<br>   rejoin 되어서 우연히 발생 된 것으로 생각 됩니다. 그리고 이와 같은 현상은 저희 쪽 <br>   횐경에서는 재현 시킬수 없었습니다.  <br><br>   아래는 두번의 테스트 로그를 정리 한 내용입니다. <br><br><br>첫번 째 테스트 수행 시간: Oct 24 14:38:45 <br>kill -9 corosync[PID] 수행 한 노드: PBWSAL01SL<br>테스트 하기전 DC 노드: PBWSAL01SL<br><br><br>** corosync를 kill 한 시점 <br>Oct 24 14:38:45 PBWSAL01SL systemd: corosync.service: main process exited, code=killed, status=9/KILL<br><br>** corosync가 재기동 된 시점<br>[27466] PBWSAL01SL corosyncnotice  [MAIN  ] Corosync Cluster Engine ('2.3.4'): started and ready to provide service.<br>Oct 24 14:38:47 [27501] PBWSAL01SL pacemakerd:   notice: mcp_read_config:       Configured corosync to accept connections from group 189: OK (1)<br><br>** 장애 노드가 disconnected 된 것으로 인식 한 시점 <br>[5155] PBWSAL02SL corosyncnotice  [TOTEM ] A new membership (172.18.21.51:180) was formed. Members joined: 1 left: 1<br>Oct 24 14:38:47 [5179] PBWSAL02SL      attrd:     info: pcmk_cpg_membership:    Node 1 left group attrd (peer=PBWSAL01SL-HB, counter=1.0)<br><br>** 장애 노드가 DC 노드이며 DC 노드가 죽음을 인식 한 시점 <br>Oct 24 14:38:47 [5181] PBWSAL02SL       crmd:   notice: peer_update_callback:   Our peer on the DC (PBWSAL01SL-HB) is dead<br><br>** 장애 노드가 rejoin 됨을 인식 한 시점 <br>[5155] PBWSAL02SL corosyncnotice  [QUORUM] Members[2]: 1 2<br>[5155] PBWSAL02SL corosyncnotice  [MAIN  ] Completed service synchronization, ready to provide service.<br>Oct 24 14:38:47 [5181] PBWSAL02SL       crmd:     info: do_dc_takeover: Taking over DC status for this partition<br>Oct 24 14:38:47 [5181] PBWSAL02SL       crmd:     info: update_dc:      Set DC to PBWSAL02SL-HB (3.0.10)<br>======================<br><br>두번 째 테스트 수행 시간: Oct 24 15:30:19<br>kill -9 corosync[PID] 수행 한 노드: PBWSAL02SL<br>테스트 하기전 DC 노드: PBWSAL02SL<br><br><br>** 장애 노드가 disconnected 됨을 인식 한 시점 <br>Oct 24 15:30:19 PBWSAL01SL corosync[27467]: [TOTEM ] A new membership (172.18.21.51:188) was formed. Members joined: 2 left: 2<br>Oct 24 15:30:19 PBWSAL01SL corosync[27467]: [TOTEM ] Failed to receive the leave message. failed: 2<br><br>** 장애 노드가 DC 노드이며 DC 노드 죽음을 인식 한 시점 <br>Oct 24 15:30:19 PBWSAL01SL crmd[84663]:  notice: Our peer on the DC (PBWSAL02SL-HB) is dead<br><br><br>** pengine이 작동 하여 장애 노드를 fencing 해야함을 판단 한 시점<br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Node PBWSAL02SL-HB will be fenced because our peer process is no longer available<br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Node PBWSAL02SL-HB is unclean<br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Scheduling Node PBWSAL02SL-HB for STONITH<br><br><br>** 장애 노드에서 corosync를 kill  한 시점 <br>Oct 24 15:30:19 PBWSAL02SL systemd: corosync.service: main process exited, code=killed, status=9/KILL<br>[65049] PBWSAL02SL corosyncnotice  [MAIN  ] Corosync Cluster Engine ('2.3.4'): started and ready to provide service.<br>======================<br>참고로, 이번 이슈는 fencing 장비 장애로 인하여 fencing이 발생 하지 않는 이슈가 아닙니다. 말씀 하신 <br>fencing device 에러는 fencing device를 start/monitor 중에 발생 한 것으로 보입니다. <br><br><br>만약 관련 하여 추가적인 문의 사항이 있으시면 다시 코멘트 남겨 주시거나 영업 시간(월 ~ 금 9:00 ~ 17:00)에 <br>080 081 0880 혹은 02 3490 5252로 연락 주시기 바랍니다. <br>======================<br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-11-01T04:03:12Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000I91BeIAJ"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-10-28T04:29:34Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-10-28T04:29:34Z</b><br><br>안녕하세요.<br><br>dropbox.redhat.com 에 case_01727279.zip 으로 파일을 업로드 하였습니다.<br><br>압축파일 내에는 sosreport 와 로그파일이 들어 있습니다.<br><br>그리고 이번 가용성 테스트는 클러스터 데몬이 비정상 종료 되었을 시 어떤 결과가 나오는지 검증하는 단계라 동일 문제가 발생하거나 하지는 않습니다.<br><br>감사합니다.<br><br><publishedDate>2016-10-28T04:29:34Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000I8hUqIAJ"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-10-27T07:30:20Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-10-27T07:30:19Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br>======================<br>먼저 corosync를 kill 시킨 노드가 재부팅 되어야 정상입니다. <br><br>2번 째 테스트 결과가 예상적인 결과입니다. <br><br>노드 PBWSAL02SL에서 corosync 프로세스를 kill 하신 후 corosync 재기동으로 <br>다시 클러스터에 조인 된 것으로 보이지만 반대방 노드에서 장애 노드가 unclean <br>되어 있는 것으로 인식 하여 fencing 시켰습니다. <br><br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Node PBWSAL02SL-HB will be fenced because our peer process is no longer available<br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Node PBWSAL02SL-HB is unclean<br>Oct 24 15:30:20 PBWSAL01SL pengine[84662]: warning: Scheduling Node PBWSAL02SL-HB for STONITH<br>======================<br>첫번 째 테스트 결과로는 장애노드가 fencing 되지 않았던 것으로 보입니다. 노드가 <br>fencing 될 여부는 pengine에서 결정 하여 pengine 로그를 확인 해 보았는데 관련 <br>정보가 없었습니다. 보내 주신 로그가 완전하지 않아 혹시 pengine 에서 관련 로그가 <br>있었는지 확인 부탁드립니다. 가능 하시면 장애 발생 당시의 /var/log/pacemaker.log<br>를 보내 주실수 있다면 더 좋을 것 같습니다. <br><br>추가적으로 해당 현상이 자주 발생 하고 있는지오? 만약 재현이 가능 하다면 어떤 조건에 <br>발생 할수 있을까요? <br><br>확인 부탁드립니다. <br>======================<br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-10-27T07:30:19Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000I01GTIAZ"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-10-26T02:50:36Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-10-26T02:50:36Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>저는 정미연이라고 하며 앞으로 이 케이스를 담당하게 되었습니다. 올려 주신 문의 <br>사항에 대해 자세히 살펴 보고 업데이트 드리도록 하겠습니다. <br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-10-26T02:50:36Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br><br>이번 프로젝트 가용성 테스트의 시나리오 중 비정상 종료에 관한 절차가 있어 아래와 같이 테스트를 진행하였습니다.<br><br>Case 1 : 한쪽노드에서 kill -9 corosync[PID]<br>첫번 째 결과 : 문제가 생긴 노드의 Resource 가 Stop -&gt; 다시 Start (Recovery) 됨.<br>두번 째 결과 : 문제가 생긴 노드가 Power Fencing 되어 Resource Failover 됨<br><br>Case 2 : kill -9 pacemakerd[PID]<br>결과 : 자동적으로 데몬이 Pacemakerd 다시 복구되어 서비스에 영향 없음<br><br>Case 3 : killall pacemakerd<br>결과 : 문제가 생긴 노드의 모든 클러스터 데몬이 종료 되고 Resource Failover 됨.<br><br>Case 4 : 양쪽 노드에서 killall pacemakerd<br>결과 : 전체 클러스터 서비스 Stop (pcs cluster stop --all 과 같은 결과)<br><br>Case 5 : 양쪽 노드에서 killall pacemakerd corosync<br>결과 : 각 클러스터 노드의 Resource 가 Stop -&gt; 다시 Start (Recovery) 됨.<br><br>위와 같은 결과를 얻었습니다.</issue><environment>가용성 테스트 결과 kill -9 로 할 때와 killall (SIGTERM 15) 의 결과는 pacemakerd 의 경우 확실히 차이점을 알았는데요,<br><br>corosync 의 경우 두 가지 테스트 한 결과가 틀리게 나와 문의를 드리려고 합니다.<br><br>첫번 째 Power Fence 실패한 테스트의 경우 <br><br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]: warning: Processing failed op monitor for kdump_stonith on PBWSAL01SL-HB: not running (7)<br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]: warning: Processing failed op monitor for vmware_stonith2 on PBWSAL01SL-HB: not running (7)<br>Oct 24 14:39:45 PBWSAL02SL pengine[5180]:  notice: Calculated Transition 4: /var/lib/pacemaker/pengine/pe-input-123.bz2<br>Oct 24 14:39:45 PBWSAL02SL crmd[5181]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on PBWSAL01SL-HB<br><br>이와 같이 Stonith 에 뭔가 문제가 있다고 보여지는데요, 두번 째 테스트에서는 문제 없이 정상적으로 Power Fencing 이 되었습니다.<br><br>corosync 가 문제가 생겼을 경우 fence 가 작동해야 하는 것으로 생각되어 문의를 드립니다.<br><br>관련 로그를 발췌하여 파일로 첨부하였습니다.<br><br>확인 부탁 드립니다.<br><br>감사합니다.</environment><cep>false</cep></case>