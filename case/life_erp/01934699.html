======================<br><b>생성계정 : jimin kim</b><br><b>생성날짜 : 2017-09-19T12:30:10Z</b><br><b>마지막 답변자 : Workflow Integration</b><br><b>마지막 수정 일자 : 2017-10-26T06:20:31Z</b><br><b>id : 500A000000YQMgZIAX</b><br>======================<br><br><b><font size=15>
제목  : 서버에서 umount 수행 후 다시 mount를 수행했을 때 umount 했던 마운트 포인트 부분이 busy가 발생하면서 umount가 되지 않았던 상황입니다.
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>서버에서 umount 후 mount 수행 시 문제 상황에 대해서 문의드리려고 합니다.<br><br>아래는 현재 서버의 상황입니다.<br><br> - 현재 mount 상태<br>root@ISLTAL02SL / # df -Ph<br>=== 생략 ===<br>/dev/mapper/LLISLTSAPVG-oracle       2.0G  215M  1.8G  11% /oracle<br>/dev/mapper/LLISLTSAPVG-usr_sap       10G  2.3G  7.8G  23% /usr/sap<br>/dev/mapper/LLISLTSAPVG-usr_sap_PLT  100G   21G   80G  21% /usr/sap/PLT<br>/dev/mapper/LLISLTSAPVG-APP           10G  558M  9.5G   6% /APP<br>100.254.144.208:/SLT_SAPP             10G     0   10G   0% /SAPP<br>100.254.144.208:/SLT_sapmnt_PLT       50G  3.1G   47G   7% /sapmnt/PLT<br>100.254.144.208:/SLT_MIG             100G     0  100G   0% /MIG<br>100.254.144.208:/SLT_bat_SAM_work    200G     0  200G   0% /batch_sam/SAM/work<br>100.254.144.208:/SLT_usr_sap_trans    50G   40G   11G  80% /usr/sap/trans<br>100.254.144.208:/SLT_bat_SAM_bak     200G     0  200G   0% /batch_sam/SAM/backup<br>=== 생략 ===<br><br> - 작업 전 mount 상태<br>작업 전에는 LLPQUOSAPVG VG의 lvol과 /etc/fstab에서 PQU NAS 부분이 mount 되어있었습니다.<br>상황 확인을 위하여 /etc/fstab 첨부하였습니다.<br><br> - 작업 내용<br>기존 LLPQUOSAPVG의 VG와 PQU NAS 부분을 umount하고 LLPQUOSAPVG의 경우는 /usr/sap 부분이 umount가 바로 안되어서<br>fuser -kcu /usr/sap 후 umount 하였습니다.<br><br>LLPQUOSAPVG를 vgchange -an LLPQUOSAPVG 후 LLISLTSAPVG의 lvol를 mount 하려고 했을 때 messages 파일에서는 device busy로 표현이 되어서<br>/usr/sap 부분의 fuser -kcu /usr/sap를 다시 한번 수행 후 mount를 하였습니다.<br><br>그리고 얼마 후 서버가 리부팅이 된 상태입니다.<br><br>DUMP/sosreport는 redhat dropbox에 올려놓겠습니다.<br>파일명은 아래와 같습니다.<br>vmcore-dmesg_ISLTAL02SL_170919<br>vmcore_ISLTAL02SL_170919<br><br><br>root@ISLTAL02SL / # vgs<br>  VG          #PV #LV #SN Attr   VSize   VFree<br>  LLISLTSAPVG   1   4   0 wz--n- 150.00g 28.00g<br>  LLPQUOSAPVG   5   5   0 wz--n- 499.98g  7.98g<br>  vg0           1   3   0 wz--n- 129.57g     0<br>root@ISLTAL02SL / # lvs<br>  LV          VG          Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert<br>  APP         LLISLTSAPVG -wi-ao----  10.00g<br>  oracle      LLISLTSAPVG -wi-ao----   2.00g<br>  usr_sap     LLISLTSAPVG -wi-ao----  10.00g<br>  usr_sap_PLT LLISLTSAPVG -wi-ao---- 100.00g<br>  APP         LLPQUOSAPVG -wi-a-----  80.00g<br>  LOG         LLPQUOSAPVG -wi-a----- 200.00g<br>  oracle      LLPQUOSAPVG -wi-a-----   2.00g<br>  usr_sap     LLPQUOSAPVG -wi-a-----  10.00g<br>  usr_sap_PQU LLPQUOSAPVG -wi-a----- 200.00g<br>  crash       vg0         -wi-ao----  99.57g<br>  home        vg0         -wi-ao----  10.00g<br>  sysadmin    vg0         -wi-ao----  20.00g<br><br>root@ISLTAL02SL / # cat /etc/fstab<br><br>#<br># /etc/fstab<br># Created by anaconda on Wed Jun 29 07:15:53 2016<br>#<br># Accessible filesystems, by reference, are maintained under '/dev/disk'<br># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info<br>#<br>UUID=321278fa-f1b9-448f-a601-1dc98e1214ec /                       xfs     defaults        0 1<br>UUID=aba90277-32a0-4e36-a77e-08a5f1f6665d /boot                   xfs     defaults        0 2<br>UUID=dc441f2e-85e2-4c65-9610-470fb925510a /var                    xfs     defaults        0 2<br>UUID=1f7a89ad-39d9-43ea-abfd-dd1439040888 swap                    swap    defaults        0 0<br>/dev/vg0/home                             /home                   xfs     defaults        0 2<br>/dev/vg0/sysadmin                         /sysadmin               xfs     defaults        0 2<br>/dev/vg0/crash                            /var/crash              xfs     defaults        0 2<br><br>### ISLT LOCAL DATA ###<br>/dev/LLISLTSAPVG/usr_sap                  /usr/sap                xfs     defaults        0 2<br>/dev/LLISLTSAPVG/usr_sap_PLT              /usr/sap/PLT            xfs     defaults        0 2<br>/dev/LLISLTSAPVG/oracle                   /oracle                 xfs     defaults        0 2<br>/dev/LLISLTSAPVG/APP                      /APP                    xfs     defaults        0 2<br><br>###  LOCAL DATA ###<br>#/dev/LLPQUOSAPVG/usr_sap                  /usr/sap                xfs     defaults        0 2<br>#/dev/LLPQUOSAPVG/usr_sap_PQU              /usr/sap/PQU            xfs     defaults        0 2<br>#/dev/LLPQUOSAPVG/oracle                   /oracle                 xfs     defaults        0 2<br>#/dev/LLPQUOSAPVG/APP                      /APP                    xfs     defaults        0 2<br>#/dev/LLPQUOSAPVG/LOG                      /LOG                    xfs     defaults        0 2<br><br>### ISLT NAS ###<br>100.254.144.208:/SLT_usr_sap_trans        /usr/sap/trans        nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>100.254.144.208:/SLT_sapmnt_PLT           /sapmnt/PLT           nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>100.254.144.208:/SLT_SAPP                 /SAPP                 nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>100.254.144.208:/SLT_bat_SAM_work         /batch_sam/SAM/work   nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>100.254.144.208:/SLT_bat_SAM_bak          /batch_sam/SAM/backup nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>100.254.144.208:/SLT_MIG                  /MIG                  nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br><br>### PQU NAS ###<br>#100.254.144.209:/QUO_usr_sap_trans      /usr/sap/trans        nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.209:/QUO_sapmnt_PQU         /sapmnt/PQU           nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.209:/QUO_MIG                /MIG                  nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.209:/QUO_FSQUO              /FSQUO                nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.209:/QUO_SAPP               /SAPP                 nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.202:/BATCH_bat_sam_SAM_work /batch_sam/SAM/work   nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.202:/BATCH_bat_sam_SAM_bak  /batch_sam/SAM/backup nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>#100.254.144.209:/QUO_FSQUO2             /FSQUO2               nfs vers=3,proto=tcp,intr,hard,timeo=300,retrans=2,wsize=65536,rsize=65536,bg,_netdev 0 0<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>타입  : Other</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><hostname>ISLTAL02SL</hostname><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><br><comment id="a0aA000000JHbS9IAL"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-10-26T06:20:30Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-10-26T06:20:30Z</b><br><br>네, 케이스 종료합니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-10-26T06:20:30Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JH0TdIAL"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2017-10-24T00:39:32Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2017-10-24T00:39:32Z</b><br><br>본 케이스는 Closed 하도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-10-24T00:39:32Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000Kg5VrIAJ"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-10-11T06:19:44Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-10-11T06:19:44Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>hpwdt 관련 내용 공유해주셔서 감사합니다.<br><br><publishedDate>2017-10-11T06:19:44Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Kfok3IAB"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-10-10T05:59:27Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-10-10T05:59:27Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>hpwdt timeout 값은 확인 결과 아래와 같습니다.<br><br>600초 입니다.<br><br>3.     hpwdt 데몬이 어느정도 시간의 응답이 없을 경우 시그널을 발생 시키는지 확인 요청<br>A.     담당자가 보낸 메일 내용에 있는 hpwdt_pretimeout 발생으로 인한 문의.<br>[답변] 600초(10분) 입니다<br><br><publishedDate>2017-10-10T05:59:27Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KeS9qIAF"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-10-03T01:52:52Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-10-03T01:52:52Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>의견 주신데로 모든 프로세스가 kill 되면서 hpwdt 모듈도 영향을 받은 것으로 보입니다.<br>여러개의 프로세스가 한꺼번에 종료되어서 구체적으로 어떤 프로세스가 직접 영향을 주었는지는 확인이 어렵지만, <br>이 결과로 hpwdt가 dump를 수행한 것은 맞습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br>(In reply to kim, jimin)<br>&gt; 안녕하세요<br>&gt; <br>&gt; SDS 김지민 선임입니다.<br>&gt; <br>&gt; 아래는 vmcore 부분 분석해주신 내용에서 NMI 가 hpwdt의 timeout과 관련이 있어 보이는데요.<br>&gt; <br>&gt; [2022138.102950] Call Trace:<br>&gt; [2022138.102962]  &lt;NMI&gt;  [&lt;ffffffff81636b21&gt;] dump_stack+0x19/0x1b<br>&gt; [2022138.103003]  [&lt;ffffffff816303b0&gt;] panic+0xd8/0x1e7<br>&gt; [2022138.103025]  [&lt;ffffffffa04008ed&gt;] hpwdt_pretimeout+0xdd/0xe0 [hpwdt]<br>&gt; [2022138.103049]  [&lt;ffffffff8163fc99&gt;] nmi_handle.isra.0+0x69/0xb0<br>&gt; <br>&gt; 아래와 같이 문제가 생겼던 서버에는 hpwdt module이 kernel에 load 되어있는 상태이고 fuser -kcu 수행했을 때<br>&gt; /에 있는 모든 프로세스를 kill 수행을 하기 때문에 hpwdt module에 영향이 있을 수도 있을 것 같다고 생각이됩니다.<br>&gt; <br>&gt; root@ISLTAL01SL /root # modinfo -n hpwdt<br>&gt; /lib/modules/3.10.0-327.53.1.el7.x86_64/kernel/drivers/watchdog/hpwdt.ko<br>&gt; root@ISLTAL01SL /root # lsmod | grep -i hpwdt<br>&gt; hpwdt                  14242  0<br>&gt; root@ISLTAL01SL /root # cat /proc/modules  | grep -i hpwdt<br>&gt; hpwdt 14242 0 - Live 0xffffffffa03aa000<br><br><publishedDate>2017-10-03T01:52:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KeA2rIAF"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-10-01T10:39:45Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-10-01T10:39:45Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>아래는 vmcore 부분 분석해주신 내용에서 NMI 가 hpwdt의 timeout과 관련이 있어 보이는데요.<br><br>[2022138.102950] Call Trace:<br>[2022138.102962]  &lt;NMI&gt;  [&lt;ffffffff81636b21&gt;] dump_stack+0x19/0x1b<br>[2022138.103003]  [&lt;ffffffff816303b0&gt;] panic+0xd8/0x1e7<br>[2022138.103025]  [&lt;ffffffffa04008ed&gt;] hpwdt_pretimeout+0xdd/0xe0 [hpwdt]<br>[2022138.103049]  [&lt;ffffffff8163fc99&gt;] nmi_handle.isra.0+0x69/0xb0<br><br>아래와 같이 문제가 생겼던 서버에는 hpwdt module이 kernel에 load 되어있는 상태이고 fuser -kcu 수행했을 때<br>/에 있는 모든 프로세스를 kill 수행을 하기 때문에 hpwdt module에 영향이 있을 수도 있을 것 같다고 생각이됩니다.<br><br>root@ISLTAL01SL /root # modinfo -n hpwdt<br>/lib/modules/3.10.0-327.53.1.el7.x86_64/kernel/drivers/watchdog/hpwdt.ko<br>root@ISLTAL01SL /root # lsmod | grep -i hpwdt<br>hpwdt                  14242  0<br>root@ISLTAL01SL /root # cat /proc/modules  | grep -i hpwdt<br>hpwdt 14242 0 - Live 0xffffffffa03aa000<br><br><publishedDate>2017-10-01T10:39:45Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KdvuyIAB"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-29T09:07:38Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-29T09:07:37Z</b><br><br>안녕하세요 <br><br>SDS 김지민 선임입니다.<br><br>H/W 업체 통해서 분석 결과가 전달되어 공유드립니다.<br>=======================================================================<br>[분석 및 확인된 내용]<br><br>6대 장비에서 유사한 시간(최초 발생 : SGH623Y6NC, 09/19/2017 11:01:00) 에 순차적으로 NMI 이벤트가 발생되었으며, 모두 동일한 footprint 로 확인되었습니다.<br><br>SGH623Y6NC : Critical,540,80623,0x0014,System Error,,,09/19/2017 11:01:00,687: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br>SGH623Y6NK : Critical,539,89005,0x0014,System Error,,,09/19/2017 11:03:00,38: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br>SGH623Y6NE : Critical,559,90890,0x0014,System Error,,,09/19/2017 11:04:00,35: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br>SGH547X4VH : Critical,774,94559,0x0014,System Error,,,09/19/2017 11:06:00,141: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br>SGH623Y6NM : Critical,560,87051,System Error,IML, ,Customer,09/19/2017 11:07:00: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br>SGH623Y6NA : Critical,541,78181,0x0014,System Error,,,09/19/2017 11:08:00,49: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)<br><br> <br><br>각 서버의 문제가 발생된 시점에 하드웨어 치명적인 오류는 감지되지 않았으나(특정 Bits 값), ILO NMI stat 이 SET 기록된 것으로 확인되었습니다.<br><br>이는 Remote NMI 디버그 트랩 및 Application watchdog 이 감지되었음을 의미하는 것이며, 발생된 NMI 는 하드웨어 오류로 발생된 것이 아닌 Application watchdog timer 의 timeout 발생에 따른 것입니다.<br><br>·         This NMI was caused by a timeout of the application watchdog timer, which is a software event. This failure will need to be examined by the OS team.<br>=======================================================================<br><br><publishedDate>2017-09-29T09:07:37Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KZAhUIAX"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-09-27T08:02:28Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-09-27T08:02:28Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>/usr/sap 디렉토리가 이미 umount 되어 있어서 비어있는 경우라면 fuser -kcu /usr/sap 명령을 수행할 경우 / 파일시스템을 사용하는 프로세스들이 kill signal을 받아서 종료하게 됩니다.<br><br>보내주셨던 vmcore와 이 작업간의 연관 관계를 좀 더 분석한 후 업데이트 드리겠습니다.<br><br>허 경 드림.<br><br><publishedDate>2017-09-27T08:02:28Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KZ0JuIAL"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-26T15:22:25Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-26T15:22:25Z</b><br><br>안녕하세요 <br><br>하드웨어 벤더쪽에서 분석 결과 확인되면 공유 드리겠습니다.<br><br>이번 case 관련해서 한가지 문의드리려고 하는데요.<br><br>OS에서 예를 들면 아래와 같이 마운트 되어있는 상태에서 <br>/usr/sap은 디렉토리만 존재할 경우에 fuser -kcu /usr/sap을 할 경우 어떠한 현상이 발생할 수 있는지 문의드립니다.<br><br>root@DECCAL01SL / # df -Ph<br>Filesystem                                  Size  Used Avail Use% Mounted on<br>/dev/sda2                                    30G  7.3G   23G  25% /<br>devtmpfs                                    504G     0  504G   0% /dev<br>tmpfs                                       900G     0  900G   0% /dev/shm<br>tmpfs                                       504G  435M  504G   1% /run<br>tmpfs                                       504G     0  504G   0% /sys/fs/cgroup<br>/dev/sda1                                  1016M  411M  605M  41% /boot<br>/dev/sda3                                    20G  3.5G   17G  18% /var<br>/dev/mapper/vg0-sysadmin                     20G  6.9G   14G  35% /sysadmin<br>/dev/mapper/vg0-home                         10G  746M  9.3G   8% /home<br><br><publishedDate>2017-09-26T15:22:25Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KYrZ2IAL"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-09-26T04:16:41Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-09-26T04:16:41Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>하드웨어 벤더쪽 분석 결과를 받으시면 저희에게도 공유 부탁드립니다.<br><br>(In reply to kim, jimin)<br>&gt; 안녕하세요 <br>&gt; <br>&gt; 분석 감사합니다.<br>&gt; <br>&gt; 해당문제 관련<br>&gt; <br>&gt; 하드웨어쪽으로도 서버에서 하드웨어 로그와 sosreport 통해서 분석 요청을 하였습니다.<br>&gt; <br>&gt; NMI 발생이 어떤 식으로 발생하게 되었는지 문의한 상태고 답변이 오면 확인해보겠습니다.<br>&gt; <br>&gt; 감사합니다.<br><br><publishedDate>2017-09-26T04:16:41Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KYb9HIAT"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-25T04:43:23Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-25T04:43:23Z</b><br><br>안녕하세요 <br><br>분석 감사합니다.<br><br>해당문제 관련<br><br>하드웨어쪽으로도 서버에서 하드웨어 로그와 sosreport 통해서 분석 요청을 하였습니다.<br><br>NMI 발생이 어떤 식으로 발생하게 되었는지 문의한 상태고 답변이 오면 확인해보겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-09-25T04:43:23Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KYFj8IAH"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-09-22T01:23:38Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-09-22T01:23:38Z</b><br><br>안녕하세요? 아래 내용은 전문 엔지니어의 분석 결과입니다.<br><br>보내주신 vmcore 모두 동일한 내용이고 NMI 발생이 OS쪽에서 발생한 상황이 아니어서 저희쪽에서 추가로 더 설명드릴만한 내용이 없습니다.<br><br>All 5 vmcore are showing the exactly same issue which was triggered by NMI signal. Before this, we see also exactly same log.<br><br><br>[2013832.250175] qla2xxx 0000:84:00.0: vpd r/w failed.  This is likely a firmware bug on this device.  Contact the card vendor for a firmware update<br>[2013832.303137] qla2xxx 0000:84:00.1: vpd r/w failed.  This is likely a firmware bug on this device.  Contact the card vendor for a firmware update<br>[2013833.251410] qla2xxx 0000:84:00.0: vpd r/w failed.  This is likely a firmware bug on this device.  Contact the card vendor for a firmware update<br>[2013833.304363] qla2xxx 0000:84:00.1: vpd r/w failed.  This is likely a firmware bug on this device.  Contact the card vendor for a firmware update<br>[2013836.445409] XFS (dm-3): Mounting V4 Filesystem<br>[2013836.494441] XFS (dm-3): Ending clean mount<br>[2013836.495910] XFS (dm-4): Mounting V4 Filesystem<br>[2013836.532274] XFS (dm-4): Ending clean mount<br>[2013836.533681] XFS (dm-5): Mounting V4 Filesystem<br>[2013836.573728] XFS (dm-5): Ending clean mount<br>[2013836.575153] XFS (dm-6): Mounting V4 Filesystem<br>[2013836.588911] XFS (dm-6): Ending clean mount<br>[2022138.102688] Kernel panic - not syncing: An NMI occurred. Depending on your system the reason for the NMI is logged in any one of the following resources:<br>                 1. Integrated Management Log (IML)<br>                 2. OA Syslog<br>                 3. OA Forward Progress Log<br>                 4. iLO Event Log<br>[2022138.102793] CPU: 0 PID: 0 Comm: swapper/0 Tainted: P           OE  ------------   3.10.0-327.53.1.el7.x86_64 #1<br>[2022138.102828] Hardware name: HP ProLiant DL380 Gen9/ProLiant DL380 Gen9, BIOS P89 02/17/2017<br>[2022138.102858]  ffffffffa04012d8 d0546bda81b2e535 ffff88bd7e805de0 ffffffff81636b21<br>[2022138.102889]  ffff88bd7e805e60 ffffffff816303b0 0000000000000008 ffff88bd7e805e70<br>[2022138.102920]  ffff88bd7e805e10 d0546bda81b2e535 00000000791bd028 ffffc90070dfe072<br>[2022138.102950] Call Trace:<br>[2022138.102962]  &lt;NMI&gt;  [&lt;ffffffff81636b21&gt;] dump_stack+0x19/0x1b<br>[2022138.103003]  [&lt;ffffffff816303b0&gt;] panic+0xd8/0x1e7<br>[2022138.103025]  [&lt;ffffffffa04008ed&gt;] hpwdt_pretimeout+0xdd/0xe0 [hpwdt]<br>[2022138.103049]  [&lt;ffffffff8163fc99&gt;] nmi_handle.isra.0+0x69/0xb0<br>[2022138.103072]  [&lt;ffffffff8163fe06&gt;] do_nmi+0x126/0x340<br>[2022138.103091]  [&lt;ffffffff8163f0d3&gt;] end_repeat_nmi+0x1e/0x2e<br>[2022138.103116]  [&lt;ffffffff81058e96&gt;] ? native_safe_halt+0x6/0x10<br>[2022138.103139]  [&lt;ffffffff81058e96&gt;] ? native_safe_halt+0x6/0x10<br>[2022138.103162]  [&lt;ffffffff81058e96&gt;] ? native_safe_halt+0x6/0x10<br>[2022138.103184]  &lt;&lt;EOE&gt;&gt;  [&lt;ffffffff8101dbff&gt;] default_idle+0x1f/0xc0<br>[2022138.103211]  [&lt;ffffffff8101e506&gt;] arch_cpu_idle+0x26/0x30<br>[2022138.103234]  [&lt;ffffffff810d6605&gt;] cpu_startup_entry+0x245/0x290<br>[2022138.103257]  [&lt;ffffffff81626787&gt;] rest_init+0x77/0x80<br>[2022138.103278]  [&lt;ffffffff81a90057&gt;] start_kernel+0x429/0x44a<br>[2022138.103299]  [&lt;ffffffff81a8fa37&gt;] ? repair_env_string+0x5c/0x5c<br>[2022138.103323]  [&lt;ffffffff81a8f120&gt;] ? early_idt_handlers+0x120/0x120<br>[2022138.103346]  [&lt;ffffffff81a8f5ee&gt;] x86_64_start_reservations+0x2a/0x2c<br>[2022138.103370]  [&lt;ffffffff81a8f742&gt;] x86_64_start_kernel+0x152/0x175<br><br><br>Please check the log from HP side for further help. HP may can help to find the reason in one of the below log.<br><br>                 1. Integrated Management Log (IML)<br>                 2. OA Syslog<br>                 3. OA Forward Progress Log<br>                 4. iLO Event Log<br>                 <br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-09-22T01:23:38Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KYDKcIAP"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-21T20:45:30Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-21T20:45:30Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>추가로 ISLTAL03SL ~ ISLTAL07SL의 sosreport , dump를 upload 하였습니다.<br><br>파일명은 아래와 같습니다.<br><br>sosreport-ISLTAL03SL-20170922031304.tar.xz<br>sosreport-ISLTAL04SL-20170922031355.tar.xz<br>sosreport-ISLTAL05SL-20170922031505.tar.xz<br>sosreport-ISLTAL06SL-20170922031549.tar.xz<br>sosreport-ISLTAL07SL-20170922031631.tar.xz<br>vmcore-dmesg_ISLTAL03SL_170919<br>vmcore-dmesg_ISLTAL04SL_170919<br>vmcore-dmesg_ISLTAL05SL_170919<br>vmcore-dmesg_ISLTAL06SL_170919<br>vmcore-dmesg_ISLTAL07SL_170919<br>vmcore_ISLTAL03SL_170919<br>vmcore_ISLTAL04SL_170919<br>vmcore_ISLTAL05SL_170919<br>vmcore_ISLTAL06SL_170919<br>vmcore_ISLTAL07SL_170919<br><br>확인 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2017-09-21T20:45:30Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KYBA4IAP"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-21T18:11:34Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-21T18:11:34Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>ISLTAL02SL 호스트만 기술 문의를 했지만<br>ISLTAL03SL ~ ISLTAL07SL까지 모두 동일한 현상이 발생했었습니다.<br><br>혹시 vmcore에서 하드웨어쪽 문제로 인하여 NMI 신호를 받아서 crash가 발생하였다는 부분을 자세히 확인부탁드려도될까요?<br><br>문제가 발생한 일에 ISLTAL02SL ~ ISLTAL07SL 6대 서버에 대해서 umount 후 mount 하는 작업이 있었는데<br>mount 시에 매끄럽게 진행이 되지 않고 /usr/sap 부분이 target busy가 발생하여서 mount되어있지 않은 /usr/sap에 대하여 <br>fuser -kcu /usr/sap을 수행했었습니다.<br> <br>그 후 mount는 잘 되었었는데 작업을 수행한 해당 서버들만 dump 발생 후 rebooting이 되어서 혹시 작업 내용과 연관지어서 문제가 발생할 수도 있는지 문의드립니다.<br><br>나머지 서버들의 sosreport 및 dump도 upload 하겠습니다.<br><br>HW 벤더 통해서도 분석 요청을 동시에 진행하겠습니다.<br><br>SLT 업무의 서버들이 MIG 시 SAP의 마이그 solution SLT를 수행하는 서버여서  확인 부탁드립니다.<br><br><publishedDate>2017-09-21T18:11:34Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KY0IdIAL"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-09-21T05:53:22Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-09-21T05:53:22Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>보내주신 vmcore를 분석한 결과 하드웨어쪽에서 NMI 신호를 받아서 crash가 발생한 것으로 확인되었습니다.<br>스토리지 문제로인해 NMI가 발생했을 가능성이 있기 때문에 HBA card 벤더에도 문의하셔서 확인하실 것을 권고드립니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-09-21T05:53:22Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KXjGlIAL"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2017-09-20T05:13:37Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2017-09-20T05:13:37Z</b><br><br>안녕하세요.<br><br>하드웨어 로그를 확인 해 보니 실제 Crash 시점 아래와 같은 로그가 감지 되었습니다.<br><br>root@ISLTAL02SL /root # cat hplog.txt<br><br>0685 Information    06:28  07/23/2017 06:35  07/23/2017 0002<br>LOG: Option ROM POST Information: 1787-Slot 0 Drive Array Operating in Interim Recovery (Degraded) Mode.<br><br>0686 Repaired       16:20  07/26/2017 16:20  07/26/2017 0001<br>LOG: Internal Storage Enclosure Device SMART Wear Error (Bay 2, Box 3, Port 1I, Slot 0)<br><br>0687 Critical       11:01  09/19/2017 11:01  09/19/2017 0001 &lt;-- 로그 기록 됨<br>LOG: An Unrecoverable System Error (NMI) has occurred (Service Information: 0x00CC47F0, 0x00CC4AF0)  &lt;-- 로그 기록 됨<br><br><publishedDate>2017-09-20T05:13:37Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KXi84IAD"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-09-20T01:54:03Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-09-20T01:54:03Z</b><br><br>안녕하세요?<br><br>Senior Technical Account Manager 허경입니다.<br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>문의하신 내용에 대해서 올려주신 파일을 받아서 확인 중입니다.<br>분석하는데로 추후 업데이트 드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-09-20T01:54:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KXXvPIAX"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-09-19T12:36:42Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-09-19T12:36:42Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>sosreport : sosreport-ISLTAL02SL-20170919205833.tar.xz 파일도 올렸습니다.<br><br>감사합니다.<br><br><publishedDate>2017-09-19T12:36:42Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>