======================<br><b>생성계정 : jimin kim</b><br><b>생성날짜 : 2017-07-12T01:53:27Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2017-08-12T10:04:40Z</b><br><b>id : 500A000000XV3RIIA1</b><br>======================<br><br><b><font size=15>
제목  : 서버 hang 상태에서 NMI 통한 dump 생성 후 분석 요청 드리려고 합니다.
</font></b><br><br>======================<br><b>사전문의<br></b><br>서버 hang 상태에서 NMI 통한 dump 생성 후 분석 요청 드리려고 합니다.<br><br>dropbox.redhat.com으로 dump는 전송후 분석 요청드리려고 합니다.<br><br>파일명은 DECCAL03SL_vmcore_170712 입니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 3 (Normal)</b><br><hostname>DECCAL03SL</hostname><br><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2017-07-22T10:16:50Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2017-07-24T07:19:29Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다. <br><br>vmcore에 대한 분석결과를 정리하여 보내드립니다.<br><br>CASE #01888419<br><br>* Customer Information<br>- Customer: SLI (Samsung Life Insurance)<br>- Account#: 5251314<br><br><br>* Case History<br>- 7/12 Wed 10:53 - (customer) 케이스 오픈<br>- 7/12 Wed 13:25 - (redhat) vmcore 업로드 확인 및 분석 진행<br>- 7/13 Thu 10:30 - (customer) sosreport 업로드 <br>- 7/13 Thu 14:48 - (redhat) vmcore 1차 분석 전달내용 전달 (hang 원인 전달), 추가적인 원인분석을 위한 filesystem Engineer 내부 분석요청<br>- 7/17 Mon 14:56 - (redhat) 분석 중, 확인필요 crontab 정보 요청<br>- 7/17 Mon 15:18 - (customer) crontab 정보 업로드<br>- 7/19 Wed 15:10 - (redht) filesystem engineer 분석내용 전달<br><br><br>* Crash, System information<br>      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-327.44.2.el7.x86_64/vmlinux<br>    DUMPFILE: vmcore  [PARTIAL DUMP]<br>        CPUS: 80<br>        DATE: Tue Jul 11 21:31:31 2017<br>      UPTIME: 156 days, 18:48:04<br>LOAD AVERAGE: 1330.23, 1322.31, 1301.18<br>       TASKS: 3764<br>    NODENAME: DECCAL03SL<br>     RELEASE: 3.10.0-327.44.2.el7.x86_64<br>     VERSION: #1 SMP Thu Nov 24 05:49:35 EST 2016<br>     MACHINE: x86_64  (2793 Mhz)<br>      MEMORY: 1023.9 GB<br>       PANIC: &quot;Kernel panic - not syncing: An NMI occurred. Depending on your system the reason for the NMI is logged in any one of the following resources:&quot;<br>         PID: 61776<br>     COMMAND: &quot;xfs_db&quot;<br>        TASK: ffff88bf26cd5080  [THREAD_INFO: ffff88bd351a0000]<br>         CPU: 0<br>       STATE: TASK_RUNNING (PANIC)<br><br><br>* Crash 원인 분석<br>- 시스템 hang 으로 인하여 사용자가 NMI 버튼으로 강제적으로 system crash 발생하고 vmcore 생성됨.<br><br><br>* 당시 시스템 Hang 원인분석 결과<br>프로세스 61776(&quot;xfs_db&quot;)는  XFS filesystem을 sync하는 것으로, inodes를 flushing하는 과정에서 global spinlock을 얻으려고 대기하는 과정에 빠지게 되었으며,<br>이러므로써, 해당 프로세스가 잡고 있는 lock에 의해서 다른 프로세스들은 mutex lock을 얻지 못함으로써 결국 시스템이 Hang이 되는 현상이 발생됨.<br><br><br>&lt; 시스템 Hang 원인분석 내용 &gt; <br><br>1. 해당 시스템은 약 1214개의 'D' State를 가진 프로세스들이 발견됨. 'D' State라고 함은, 프로세스가 멈춘 상태로 보면 됨.<br>[  0 08:56:11.527] [UN]  PID: 119337  TASK: ffff887e1c351700  CPU: 28  COMMAND: &quot;REC_10_UPD_W82&quot;<br>[  0 08:57:04.288] [UN]  PID: 62001  TASK: ffff88fd4f6cb980  CPU: 69  COMMAND: &quot;df&quot;<br>[  0 08:57:51.061] [UN]  PID: 116739  TASK: ffff88bf1aa7e780  CPU: 45  COMMAND: &quot;REC_00_BTC_W175&quot;<br>[  0 08:58:05.366] [UN]  PID: 61962  TASK: ffff88e6bee95080  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 08:58:19.052] [UN]  PID: 2226   TASK: ffff883f160b4500  CPU: 3   COMMAND: &quot;kworker/3:1H&quot;<br>[  0 08:58:19.052] [UN]  PID: 76266  TASK: ffff883ed7a80b80  CPU: 3   COMMAND: &quot;kworker/3:0&quot;<br>[  0 08:59:06.448] [UN]  PID: 61933  TASK: ffff88e6bee94500  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 09:00:07.519] [UN]  PID: 61905  TASK: ffff88e6bee96780  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 09:00:39.102] [UN]  PID: 4955   TASK: ffff88ff23c95080  CPU: 31  COMMAND: &quot;ntpd&quot;<br>[  0 09:01:56.493] [UN]  PID: 100701  TASK: ffff8828a57c5080  CPU: 13  COMMAND: &quot;ontuned&quot;<br><br>crash&gt; ps -m | grep UN | wc -l<br>1214<br><br><br>2. 그래서 해당 프로세스들이 왜 'D' State인지를 확인 한 결과,  해당 프포레스들은 mutex lock을 잡는 과정에서 멈춘 것으로 확인됨.<br>PID: 100701  TASK: ffff8828a57c5080  CPU: 13  COMMAND: &quot;ontuned&quot;<br> #0 [ffff88210f3eb9c0] __schedule at ffffffff8163ba3d<br> #1 [ffff88210f3eba28] schedule_preempt_disabled at ffffffff8163d1b9<br> #2 [ffff88210f3eba38] __mutex_lock_slowpath at ffffffff8163aeb5<br> #3 [ffff88210f3eba98] mutex_lock at ffffffff8163a31f<br> #4 [ffff88210f3ebab0] lookup_slow at ffffffff81633cf9<br> #5 [ffff88210f3ebae8] link_path_walk at ffffffff811ee00f<br> #6 [ffff88210f3ebb98] path_lookupat at ffffffff811ee20b<br> #7 [ffff88210f3ebc30] filename_lookup at ffffffff811ee96b<br> #8 [ffff88210f3ebc68] kern_path at ffffffff811f04f5<br> #9 [ffff88210f3ebd38] wr_lookupname at ffffffffa1627846 [RedCastle]<br>#10 [ffff88210f3ebd90] ips_open_pre at ffffffffa1634558 [RedCastle]<br>#11 [ffff88210f3ebf00] prst_open at ffffffffa1634a47 [RedCastle]<br>#12 [ffff88210f3ebf58] rg_open at ffffffffa1634bf0 [RedCastle]<br>#13 [ffff88210f3ebf80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007ff91cf9186d  RSP: 00007ff919d634c0  RFLAGS: 00010202<br>    RAX: 0000000000000002  RBX: ffffffff816470c9  RCX: 00007ff8f8026760<br>    RDX: 00000000000001b6  RSI: 0000000000000000  RDI: 000000000041b552<br>    RBP: 00007ff919d62a10   R8: 000000000041b4a6   R9: 0000000000000000<br>    R10: 0000000000000024  R11: 0000000000000293  R12: 0000000000000008<br>    R13: 00007ff8f80133a0  R14: 000000000041b4a0  R15: ffffffffa1634bf0<br>    ORIG_RAX: 0000000000000002  CS: 0033  SS: 002b<br>    <br>crash&gt; dis -lr ffffffff81633cf9<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1521<br>0xffffffff81633cc6 &lt;lookup_slow&gt;:       nopl   0x0(%rax,%rax,1) [FTRACE NOP]<br>0xffffffff81633ccb &lt;lookup_slow+0x5&gt;:   push   %rbp<br>0xffffffff81633ccc &lt;lookup_slow+0x6&gt;:   mov    %rsp,%rbp<br>0xffffffff81633ccf &lt;lookup_slow+0x9&gt;:   push   %r14<br>0xffffffff81633cd1 &lt;lookup_slow+0xb&gt;:   push   %r13<br>0xffffffff81633cd3 &lt;lookup_slow+0xd&gt;:   push   %r12<br>0xffffffff81633cd5 &lt;lookup_slow+0xf&gt;:   push   %rbx<br>0xffffffff81633cd6 &lt;lookup_slow+0x10&gt;:  mov    %rdi,%rbx<br>0xffffffff81633cd9 &lt;lookup_slow+0x13&gt;:  push   %rcx<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1525<br>0xffffffff81633cda &lt;lookup_slow+0x14&gt;:  mov    0x8(%rdi),%r13<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1526<br>0xffffffff81633cde &lt;lookup_slow+0x18&gt;:  mov    0x30(%rdi),%rdi<br>0xffffffff81633ce2 &lt;lookup_slow+0x1c&gt;:  cmp    0x30(%r13),%rdi<br>0xffffffff81633ce6 &lt;lookup_slow+0x20&gt;:  je     0xffffffff81633cea &lt;lookup_slow+0x24&gt;<br>0xffffffff81633ce8 &lt;lookup_slow+0x22&gt;:  ud2    <br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1528<br>0xffffffff81633cea &lt;lookup_slow+0x24&gt;:  add    $0xa8,%rdi<br>0xffffffff81633cf1 &lt;lookup_slow+0x2b&gt;:  mov    %rsi,%r12<br>0xffffffff81633cf4 &lt;lookup_slow+0x2e&gt;:  callq  0xffffffff8163a300 &lt;mutex_lock&gt;<br><br>1519 /* Fast lookup failed, do it the slow way */<br>1520 static int lookup_slow(struct nameidata *nd, struct path *path)<br>1521 {<br>1522   struct dentry *dentry, *parent;<br>1523   int err;<br>1524 <br>1525   parent = nd-&gt;path.dentry;<br>1526   BUG_ON(nd-&gt;inode != parent-&gt;d_inode);<br>1527 <br>1528   mutex_lock(&amp;parent-&gt;d_inode-&gt;i_mutex);<br><br><br>2-1. 해당 mutex의 위치는 '/proc/'임을 확인할 수 있었으며, 해당 mutex를 기다리는 프로세스의 수가 약 736임을 확인할 수 있었음.<br><br>crash&gt; nameidata.path ffff88210f3ebc70<br>  path = {<br>    mnt = 0xffff887f2763a520, <br>    dentry = 0xffff88bf27008240<br>  }<br>crash&gt; dentry.d_iname,d_parent 0xffff88bf27008240<br>  d_iname = &quot;/\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000&quot;<br>  d_parent = 0xffff88bf27008240<br>crash&gt; vfsmount 0xffff887f2763a520<br>struct vfsmount {<br>  mnt_root = 0xffff88bf27008240, <br>  mnt_sb = 0xffff88bf2f524000, <br>  mnt_flags = 0x1027<br>}<br>crash&gt; mount | grep ffff88bf2f524000<br>ffff887f2763a500 ffff88bf2f524000 proc   proc      /proc  <br><br>crash&gt; dentry.d_inode 0xffff88bf27008240<br>  d_inode = 0xffff88bf27018040<br>crash&gt; inode.i_mutex 0xffff88bf27018040 -ox<br>struct inode {<br>  [ffff88bf270180e8] struct mutex i_mutex;<br>}<br><br>crash&gt; crashinfo --mutex=0xffff88bf270180e8 | wc -l<br>736<br><br><br>2-2. 해당 mutex를 가진 프로세스를 확인할 결과, 'saposcol' 프로세스이며, 이 프로세스 또한 오랜시간동안 CPU에서 looping을 돌고 있는 상태로 확인됨.<br>crash&gt; bt 26332<br>PID: 26332  TASK: ffff88e722581700  CPU: 71  COMMAND: &quot;saposcol&quot;<br> #0 [ffff88ff7f2c5e70] crash_nmi_callback at ffffffff81045982<br> #1 [ffff88ff7f2c5e80] nmi_handle at ffffffff8163fb59<br> #2 [ffff88ff7f2c5ec8] do_nmi at ffffffff8163fc70<br> #3 [ffff88ff7f2c5ef0] end_repeat_nmi at ffffffff8163ef93<br>    [exception RIP: _raw_spin_lock+0x37]<br>    RIP: ffffffff8163e387  RSP: ffff88f7e62ebd50  RFLAGS: 00000206<br>    RAX: 0000000000002d09  RBX: ffff88ff21b92190  RCX: 00000000000077d4<br>    RDX: 00000000000077d6  RSI: 00000000000077d6  RDI: ffffffff81943400<br>    RBP: ffff88f7e62ebd50   R8: 000000000001a060   R9: 0000000000000001<br>    R10: 0000000000000000  R11: 0000000000000000  R12: ffff88bf2614e780<br>    R13: 0000000000000000  R14: ffff88bf27008240  R15: ffffffff811f2d50<br>    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018<br>--- &lt;NMI exception stack&gt; ---<br> #4 [ffff88f7e62ebd50] _raw_spin_lock at ffffffff8163e387<br> #5 [ffff88f7e62ebd58] inode_sb_list_add at ffffffff811f9769<br> #6 [ffff88f7e62ebd70] new_inode at ffffffff811fbd09<br> #7 [ffff88f7e62ebd88] proc_pid_make_inode at ffffffff8124db7b<br> #8 [ffff88f7e62ebdb8] proc_pid_instantiate at ffffffff8124dc6b<br> #9 [ffff88f7e62ebdd8] proc_fill_cache at ffffffff8124e4c9<br>#10 [ffff88f7e62ebe48] proc_pid_readdir at ffffffff8124f1fe<br>#11 [ffff88f7e62ebec8] proc_root_readdir at ffffffff8124a1af<br>#12 [ffff88f7e62ebef0] vfs_readdir at ffffffff811f2c40<br>#13 [ffff88f7e62ebf30] sys_getdents at ffffffff811f3065<br>#14 [ffff88f7e62ebf80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007f0098d349d5  RSP: 00007ffec8145320  RFLAGS: 00010202<br>    RAX: 000000000000004e  RBX: ffffffff816470c9  RCX: 000000000000bd31<br>    RDX: 0000000000008000  RSI: 0000000001936750  RDI: 0000000000000006<br>    RBP: 0000000001936750   R8: 0000000000020000   R9: 0000000000008030<br>    R10: 0000000000000076  R11: 0000000000000246  R12: 00000000004d1640<br>    R13: 0000000000000000  R14: ffffffffffffff60  R15: 0000000001936750<br>    ORIG_RAX: 000000000000004e  CS: 0033  SS: 002b<br><br><br>3. 해당 프로세스는 다른 프로세스들이 필요로 하는 mutex lock을 점유한 상태에서, global spinlock을 잡기 위해서 기다리는 중임을 확인.<br>crash&gt; dis -lr ffffffff811f9769<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/inode.c: 447<br>0xffffffff811f9750 &lt;inode_sb_list_add&gt;: nopl   0x0(%rax,%rax,1) [FTRACE NOP]<br>0xffffffff811f9755 &lt;inode_sb_list_add+0x5&gt;:     push   %rbp<br>0xffffffff811f9756 &lt;inode_sb_list_add+0x6&gt;:     mov    %rsp,%rbp<br>0xffffffff811f9759 &lt;inode_sb_list_add+0x9&gt;:     push   %rbx<br>0xffffffff811f975a &lt;inode_sb_list_add+0xa&gt;:     mov    %rdi,%rbx<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/include/linux/spinlock.h: 293<br>0xffffffff811f975d &lt;inode_sb_list_add+0xd&gt;:     mov    $0xffffffff81943400,%rdi<br>0xffffffff811f9764 &lt;inode_sb_list_add+0x14&gt;:    callq  0xffffffff8163e350 &lt;_raw_spin_lock&gt;<br><br> 442 /**<br> 443  * inode_sb_list_add - add inode to the superblock list of inodes<br> 444  * @inode: inode to add<br> 445  */<br> 446 void inode_sb_list_add(struct inode *inode)<br> 447 {<br> 448   spin_lock(&amp;inode_sb_list_lock);<br><br><br>&lt;--- looping here<br> 449   list_add(&amp;inode-&gt;i_sb_list, &amp;inode-&gt;i_sb-&gt;s_inodes);<br> 450   spin_unlock(&amp;inode_sb_list_lock);<br> 451 }<br><br><br>4. 'inode_sb_list_lock'은 현재 PID 61776에 의해 잡혀 있는 상태이며, 또한 CPU를 오랫동안(약 9시간) 점유한 상태에서 멈춰있는 것으로 확인됨.<br><br> CPU 0: 30107532d14f8d  32520.75s behind<br> <br>crash&gt; pd 32520/60<br>$3 = 542<br>crash&gt; pd 32520/60/60<br>$4 = 9<br><br><br>5. 해당  프로세스는 spin lock을 잡은 상태에서 sync를 진행하는 과정이었음을 확인하며, 여기서 다른 lock을 얻기 위해서 대기하고 있는 상태임을 확인할 수 있었음.<br>PID: 61776  TASK: ffff88bf26cd5080  CPU: 0   COMMAND: &quot;xfs_db&quot;<br> #0 [ffff883f7f805cb8] machine_kexec at ffffffff81051e9b<br> #1 [ffff883f7f805d18] crash_kexec at ffffffff810f2a42<br> #2 [ffff883f7f805de8] panic at ffffffff81630267<br> #3 [ffff883f7f805e68] hpwdt_pretimeout at ffffffffa02c98ed [hpwdt]<br> #4 [ffff883f7f805e80] nmi_handle at ffffffff8163fb59<br> #5 [ffff883f7f805ec8] do_nmi at ffffffff8163fcc6<br> #6 [ffff883f7f805ef0] end_repeat_nmi at ffffffff8163ef93<br>    [exception RIP: _raw_spin_lock+0x12]<br>    RIP: ffffffff8163e362  RSP: ffff88bd351a3d70  RFLAGS: 00000286<br>    RAX: 00000000d242d242  RBX: ffff88f9583df8b8  RCX: ffff88bd351a3fd8<br>    RDX: 000000000000d240  RSI: 0000000000000000  RDI: ffff88f9583df940<br>    RBP: ffff88bd351a3d70   R8: ffff88bd351a3cc0   R9: 0000000000000000<br>    R10: 0000000000000100  R11: 0000000000000001  R12: ffff88f9583df940<br>    R13: ffff88ff156b18a0  R14: ffff88e68bacebf8  R15: ffff88f9583dfa08<br>    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018<br>--- &lt;NMI exception stack&gt; ---<br> #7 [ffff88bd351a3d70] _raw_spin_lock at ffffffff8163e362<br> #8 [ffff88bd351a3d78] sync_inodes_sb at ffffffff8120938e<br> #9 [ffff88bd351a3e18] sync_filesystem at ffffffff8121050b<br>#10 [ffff88bd351a3e30] fsync_bdev at ffffffff8121ab94<br>#11 [ffff88bd351a3e50] blkdev_ioctl at ffffffff812d945b<br>#12 [ffff88bd351a3ea8] block_ioctl at ffffffff8121a1f1<br>#13 [ffff88bd351a3eb8] do_vfs_ioctl at ffffffff811f28f5<br>#14 [ffff88bd351a3f30] sys_ioctl at ffffffff811f2b71<br>#15 [ffff88bd351a3f80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007fa861615317  RSP: 00007ffd0f3de120  RFLAGS: 00010246<br>    RAX: 0000000000000010  RBX: ffffffff816470c9  RCX: 0000000000001000<br>    RDX: 0000000000000000  RSI: 0000000000001261  RDI: 0000000000000004<br>    RBP: 0000000000000004   R8: 00000000023f5f50   R9: 0000000000000001<br>    R10: 00007ffd0f3de660  R11: 0000000000000246  R12: 0000000000000000<br>    R13: 0000000000000000  R14: 0000000000000000  R15: 0000000000000000<br>    ORIG_RAX: 0000000000000010  CS: 0033  SS: 002b<br>    <br>1215 static void wait_sb_inodes(struct super_block *sb)<br>1216 {<br>1217   struct inode *inode, *old_inode = NULL;<br>1218 <br>1219   /*<br>1220    * We need to be protected against the filesystem going from<br>1221    * r/o to r/w or vice versa.<br>1222    */<br>1223   WARN_ON(!rwsem_is_locked(&amp;sb-&gt;s_umount));<br>1224 <br>1225   spin_lock(&amp;inode_sb_list_lock);<br><br><br>&lt;--- It's holding the lock<br>1226 <br>1227   /*<br>1228    * Data integrity sync. Must wait for all pages under writeback,<br>1229    * because there may have been pages dirtied before our sync<br>1230    * call, but which had writeout started before we write it out.<br>1231    * In which case, the inode may not be on the dirty list, but<br>1232    * we still have to wait for that writeout.<br>1233    */<br>1234   list_for_each_entry(inode, &amp;sb-&gt;s_inodes, i_sb_list) {<br>1235     struct address_space *mapping = inode-&gt;i_mapping;<br>1236 <br>1237     spin_lock(&amp;inode-&gt;i_lock);<br>======================<br>&lt;--- Tried to get this lock while holding 'inode_sb_list_lock'<br><br><br>5-1. 얻고자 하는 lock에 대한 정보를 확인한 결과 '/APP'에 마운트된 디렉토리임을 확인<br><br>crash&gt; inode.i_lock ffff88f9583df8b8<br>  i_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0xd244d242, <br>            tickets = {<br>              head = 0xd242, <br>              tail = 0xd244<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }<br>crash&gt; inode.i_lock ffff88f9583df8b8 -ox<br>struct inode {<br>  [ffff88f9583df940] spinlock_t i_lock;<br>}<br><br>crash&gt; inode.i_op,i_sb ffff88f9583df8b8<br>  i_op = 0xffffffffa080b800 &lt;xfs_dir_inode_operations&gt;<br>  i_sb = 0xffff88ff156b1800<br>crash&gt; mount | grep ffff88ff156b1800<br>ffff883f24f49300 ffff88ff156b1800 xfs    /dev/mapper/LLDECCSAPVG-APP /APP   <br><br>crash&gt; inode.i_mapping ffff88f9583df8b8<br>  i_mapping = 0xffff88f9583dfa08<br>crash&gt; address_space 0xffff88f9583dfa08<br>struct address_space {<br>  host = 0xffff88f9583df8b8, <br>  page_tree = {<br>    height = 0x0, <br>    gfp_mask = 0x20, <br>    rnode = 0x0<br>  }, <br>  tree_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0x0, <br>            tickets = {<br>              head = 0x0, <br>              tail = 0x0<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }, <br>  i_mmap_writable = 0x0, <br>  i_mmap = {<br>    rb_node = 0x0<br>  }, <br>  i_mmap_nonlinear = {<br>    next = 0xffff88f9583dfa30, <br>    prev = 0xffff88f9583dfa30<br>  }, <br>  i_mmap_mutex = {<br>    count = {<br>      counter = 0x1<br>    }, <br>    wait_lock = {<br>      {<br>        rlock = {<br>          raw_lock = {<br>            {<br>              head_tail = 0x0, <br>              tickets = {<br>                head = 0x0, <br>                tail = 0x0<br>              }<br>            }<br>          }<br>        }<br>      }<br>    }, <br>    wait_list = {<br>      next = 0xffff88f9583dfa48, <br>      prev = 0xffff88f9583dfa48<br>    }, <br>    owner = 0x0, <br>    {<br>      osq = 0x0, <br>      __UNIQUE_ID_rh_kabi_hide0 = {<br>        spin_mlock = 0x0<br>      }, <br>      {&lt;No data fields&gt;}<br>    }<br>  }, <br>  nrpages = 0x0, <br>  nrshadows = 0x0, <br>  writeback_index = 0x0, <br>  a_ops = 0xffffffffa080b0a0 &lt;xfs_address_space_operations&gt;, <br>  flags = 0x2005a, <br>  backing_dev_info = 0xffff883f21a86b98, <br>  private_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0x0, <br>            tickets = {<br>              head = 0x0, <br>              tail = 0x0<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }, <br>  private_list = {<br>    next = 0xffff88f9583dfaa0, <br>    prev = 0xffff88f9583dfaa0<br>  }, <br>  private_data = 0x0<br>}<br><br>#define hlist_for_each(pos, head) \<br>  for (pos = (head)-&gt;first; pos ; pos = pos-&gt;next)<br>  <br>/**<br> * hlist_for_each_entry - iterate over list of given type<br> * @pos:  the type * to use as a loop cursor.<br> * @head: the head for your list.<br> * @member: the name of the hlist_node within the struct.<br> */<br>#define hlist_for_each_entry(pos, head, member)       \<br>  for (pos = hlist_entry_safe((head)-&gt;first, typeof(*(pos)), member);\<br>       pos;             \<br>       pos = hlist_entry_safe((pos)-&gt;member.next, typeof(*(pos)), member))<br>       <br>  <br>  hlist_for_each_entry(dentry, &amp;inode-&gt;i_dentry, d_alias) {<br>    dget(dentry);<br>    spin_unlock(&amp;inode-&gt;i_lock);<br>    <br><br><br>crash&gt; inode.i_dentry ffff88f9583df8b8<br>    i_dentry = {<br>      first = 0xffff88b2b0902fb0<br>    }<br>crash&gt; hlist_node 0xffff88b2b0902fb0<br>struct hlist_node {<br>  next = 0xffff88f8e95c2cb0, <br>  pprev = 0xffff88f9583df9d0<br>}<br><br>crash&gt; dentry.d_alias -ox<br>struct dentry {<br>  [0xb0] struct hlist_node d_alias;<br>}<br><br>crash&gt; px 0xffff88b2b0902fb0-0xb0<br>$2 = 0xffff88b2b0902f00<br>crash&gt; dentry.d_iname,d_parent 0xffff88b2b0902f00<br>  d_iname = &quot;scripts\000ems\000.d\000t\000f\000quires\000\000es\000\000&quot;<br>  d_parent = 0xffff88ff25fe3200<br>crash&gt; dentry.d_iname,d_parent 0xffff88ff25fe3200<br>  d_iname = &quot;/\000atus\000\000memory650\000\000\000\000\000\000\000\000\000\000\000\000\000\000&quot;<br>  d_parent = 0xffff88ff25fe3200<br><br><br>5-2. 해당 lock을 가진 것을 확인하려고 하였으나, 현재 해당 파일을 잡고 있는 것은 어디에서도 확인할 수 없었음<br><br>crash&gt; foreach files | grep scripts<br>crash&gt; foreach bt -f | grep ffff88f9583df8b8<br>    RAX: 00000000d242d242  RBX: ffff88f9583df8b8  RCX: ffff88bd351a3fd8<br>crash&gt; <br><br><br>&lt; 근본 원인을 찾기 위해서 Filesystem 분석 결과 &gt;<br><br>XFS는 기본적으로 inode의 lifetime을 관리하게 되나, 현재 알 수 없는 이유로 lifetime을 제대로 완료못 한 상태로 인하여 lock을 풀지 못하는 상태가 되었고 해당 이슈의 처음 발생원인을 로그로 부터 추정한 결과, double add가 발생하는 상황을 발견<br><br>1. 사용자가 NMI 강제 Crash되기 14시간 전에 double add warning을 발견하였으나, 해당 프로세스가 이미 죽은 상태에서 그 원인을 찾는 것이 어려운 상태임.<br><br>During its execution a double add warning occured. <br>The process completed and its stack context is no longer available to us for analysis in the vmcore. <br><br>Time-to-NMI: (13561415-13510137)/60/60=14 hours<br><br>[13510137.047157] WARNING: at lib/list_debug.c:36 __list_add+0x8a/0xc0()<br>[13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br>[13510137.047286] CPU: 33 PID: 12160 Comm: mkdir Tainted: P           OE  ------------   3.10.0-327.44.2.el7.x86_64 #1<br>[13510137.047288] Hardware name: HP ProLiant DL580 Gen9/ProLiant DL580 Gen9, BIOS U17 09/12/2016<br>[13510137.047289]  ffff887e2de9bb70 000000002c1beaf2 ffff887e2de9bb28 ffffffff816369d1<br>[13510137.047291]  ffff887e2de9bb60 ffffffff8107b260 ffff88f9583df9c0 ffff88f9583df9c0<br>[13510137.047293]  ffff88ff156b18a0 0000000000000000 ffff883f7f31e600 ffff887e2de9bbc8<br>[13510137.047295] Call Trace:<br>[13510137.047311]  [&lt;ffffffff816369d1&gt;] dump_stack+0x19/0x1b<br>[13510137.047318]  [&lt;ffffffff8107b260&gt;] warn_slowpath_common+0x70/0xb0<br>[13510137.047320]  [&lt;ffffffff8107b2fc&gt;] warn_slowpath_fmt+0x5c/0x80<br>[13510137.047322]  [&lt;ffffffff8130c84a&gt;] __list_add+0x8a/0xc0<br>[13510137.047326]  [&lt;ffffffff811f9787&gt;] inode_sb_list_add+0x37/0x50<br>[13510137.047372]  [&lt;ffffffffa07d9d94&gt;] xfs_setup_inode+0x34/0x2f0 [xfs]<br>[13510137.047385]  [&lt;ffffffffa07dc0ed&gt;] xfs_ialloc+0x2cd/0x540 [xfs]<br>[13510137.047395]  [&lt;ffffffffa07dc3d6&gt;] xfs_dir_ialloc+0x76/0x280 [xfs]<br>[13510137.047409]  [&lt;ffffffffa07ec4cb&gt;] ? xfs_log_reserve+0x15b/0x1b0 [xfs]<br>[13510137.047413]  [&lt;ffffffff8163b312&gt;] ? down_write+0x12/0x30<br>[13510137.047423]  [&lt;ffffffffa07dc8b4&gt;] xfs_create+0x284/0x710 [xfs]<br>[13510137.047427]  [&lt;ffffffff811e9a2d&gt;] ? __lookup_hash+0x2d/0x60<br>[13510137.047437]  [&lt;ffffffffa07d8fdb&gt;] xfs_vn_mknod+0xbb/0x250 [xfs]<br>[13510137.047438]  [&lt;ffffffff811ef60f&gt;] ? getname_flags+0x4f/0x1a0<br>[13510137.047448]  [&lt;ffffffffa07d9186&gt;] xfs_vn_mkdir+0x16/0x20 [xfs]<br>[13510137.047449]  [&lt;ffffffff811ead67&gt;] vfs_mkdir+0xb7/0x160<br>[13510137.047450]  [&lt;ffffffff811f0c6f&gt;] SyS_mkdirat+0x6f/0xe0<br>[13510137.047452]  [&lt;ffffffff811f0cf9&gt;] SyS_mkdir+0x19/0x20<br>[13510137.047461]  [&lt;ffffffffa16343ca&gt;] prst_mkdir+0x13a/0x190 [RedCastle]<br>[13510137.047465]  [&lt;ffffffffa1634458&gt;] rg_mkdir+0x38/0x50 [RedCastle]<br>[13510137.047470]  [&lt;ffffffff816470c9&gt;] system_call_fastpath+0x16/0x1b <br><br><br>2. 해당 로그의 데이터를 기반으로 메모리 정보를 통해서 남아있는 흔적을 분석 한 결과, <br>- /APP/scripts directory를 만들던 프로세스 처리중에 double add가 발생하는 이슈가 발생하였고,<br>- 이것이 전체적으로 영향을 미친 것으로 분석됨.<br><br>The double add occurred 14 hours before the NMI was hit, the longest task was 9 hours ago. If the double add caused this there was a window of time where things kept running. <br>The inode involved in the double add could still be around, at least its still allocated.<br>It could have been reused, but then I'd expect to see list_del warnings similar to those from list_add.<br><br>crash&gt; kmem ffff88f9583df9c0<br>CACHE            NAME                 OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE<br>ffff88ff229fc400 xfs_inode               1024     322842    326820  10894    32k<br>  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE<br>  ffffea03e560f600  ffff88f9583d8000     3     30         30     0<br>  FREE / [ALLOCATED]<br>  [ffff88f9583df700]<br><br>crash&gt; xfs_inode.i_ino,i_vnode.i_sb ffff88f9583df700<br>  i_ino = 132<br>  i_vnode.i_sb = 0xffff88ff156b1800,<br><br>crash&gt; mount | grep ffff88ff156b1800<br>ffff883f24f49300 ffff88ff156b1800 xfs    /dev/mapper/LLDECCSAPVG-APP /APP <br><br>crash&gt; struct -o xfs_inode ffff88f9583df700<br>struct xfs_inode {<br>  [ffff88f9583df700] struct xfs_mount *i_mount;<br>  [ffff88f9583df708] struct xfs_dquot *i_udquot;<br>  [ffff88f9583df710] struct xfs_dquot *i_gdquot;<br>  [ffff88f9583df718] struct xfs_dquot *i_pdquot;<br>  [ffff88f9583df720] xfs_ino_t i_ino;<br>  [ffff88f9583df728] struct xfs_imap i_imap;<br>  [ffff88f9583df738] xfs_ifork_t *i_afp;<br>  [ffff88f9583df740] xfs_ifork_t i_df;<br>  [ffff88f9583df780] const struct xfs_dir_ops *d_ops;<br>  [ffff88f9583df788] struct xfs_inode_log_item *i_itemp;<br>  [ffff88f9583df790] mrlock_t i_lock;<br>  [ffff88f9583df7b0] mrlock_t i_iolock;<br>  [ffff88f9583df7d0] mrlock_t i_mmaplock;<br>  [ffff88f9583df7f0] atomic_t i_pincount;<br>  [ffff88f9583df7f4] spinlock_t i_flags_lock;<br>  [ffff88f9583df7f8] unsigned long i_flags;<br>  [ffff88f9583df800] unsigned int i_delayed_blks;<br>  [ffff88f9583df804] spinlock_t i_size_lock;<br>  [ffff88f9583df808] xfs_icdinode_t i_d;<br>  [ffff88f9583df8b8] struct inode i_vnode;<br>}<br>SIZE: 1024<br><br>Digging in the struct inode (xfs_inode.i_vnode)<br>crash&gt; struct inode ffff88f9583df8b8<br>...<br>      i_dentry = {<br>        first = 0xffff88b2b0902fb0<br>      }, <br><br>crash&gt; kmem 0xffff88b2b0902fb0<br>CACHE            NAME                 OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE<br>ffff88bf7f428b00 dentry                   192    3285159   3296790  78495     8k<br>  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE<br>  ffffea02cac24080  ffff88b2b0902000     2     42         25    17<br>  FREE / [ALLOCATED]<br>  [ffff88b2b0902f00]<br><br>crash&gt; files -d ffff88b2b0902f00<br>     DENTRY           INODE           SUPERBLK     TYPE PATH<br><br><br>3. 해당 프로세스가 이미 죽은 상태에서 더이상의 추적이 어려움에 따라, 아래와 같은 몇 가지 원인 추정 및 향후 Action Plan을 제시함<br><br><br>(추정 원인)<br><br>1.커널 내부의 문제  - 하지만 현재 시스템이  156일간 동작되던 시스템으로, XFS 파일시스템의 inode를 관리하는 문제가 156일 후에 발생하기는 상당히 어려움으로 현 이슈의 원인과 버그로 연결하기는 어려운 상태임<br><br><br>2. 외부 모듈(RedCastle)에서 해당 프로세스중에 강제적인 intercept로 인하여 double add가 발생되었을 가능성<br><br>현재 mkdir 프로세스 call stack에는 RedCastle에서 부터 시작됨에 따라, 해당 모듈이 어떠한 문제를 야기했을 가능성이 존재<br><br>  [13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br>  [13510137.047165] Modules linked in: rpcsec_gss_krb5 nfsv4 dns_resolver nls_utf8 isofs loop fuse btrfs zlib_deflate raid6_pq xor vfat msdos fat ext4 mbcache jbd2 bridge stp llc ipmi_watchdog ipmi_devintf RedCastle(POE) nfsv3 nfs fscache bonding iptable_filter iTCO_wdt iTCO_vendor_support intel_powerclamp coretemp intel_rapl kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd pcspkr dm_service_time ipmi_ssif sb_edac edac_core sg lpc_ich mfd_core hpilo hpwdt ioatdma shpchp wmi ipmi_si ipmi_msghandler acpi_power_meter dm_multipath dm_mod nfsd auth_rpcgss nfs_acl lockd grace binfmt_misc sunrpc ip_tables xfs libcrc32c crc32c_intel serio_raw ixgbe(OE) vxlan ip6_udp_tunnel udp_tunnel dca mgag200 syscopyarea sysfillrect sysimgblt i2c_algo_bit drm_kms_helper ttm drm i2c_core<br>  [13510137.047272]  tg3(OE) ptp pps_core sd_mod crc_t10dif crct10dif_generic crct10dif_pclmul crct10dif_common qla2xxx(OE) scsi_transport_fc scsi_tgt hpsa(OE) scsi_transport_sas [last unloaded: RedCastle]<br>  [13510137.047286] CPU: 33 PID: 12160 Comm: mkdir Tainted: P           OE  ------------   3.10.0-327.44.2.el7.x86_64 #1<br>  [13510137.047288] Hardware name: HP ProLiant DL580 Gen9/ProLiant DL580 Gen9, BIOS U17 09/12/2016<br>  [13510137.047289]  ffff887e2de9bb70 000000002c1beaf2 ffff887e2de9bb28 ffffffff816369d1<br>  [13510137.047291]  ffff887e2de9bb60 ffffffff8107b260 ffff88f9583df9c0 ffff88f9583df9c0<br>  [13510137.047293]  ffff88ff156b18a0 0000000000000000 ffff883f7f31e600 ffff887e2de9bbc8<br>  [13510137.047295] Call Trace:<br>  [13510137.047311]  [&lt;ffffffff816369d1&gt;] dump_stack+0x19/0x1b<br>  [13510137.047318]  [&lt;ffffffff8107b260&gt;] warn_slowpath_common+0x70/0xb0<br>  [13510137.047320]  [&lt;ffffffff8107b2fc&gt;] warn_slowpath_fmt+0x5c/0x80<br>  [13510137.047322]  [&lt;ffffffff8130c84a&gt;] __list_add+0x8a/0xc0<br>  [13510137.047326]  [&lt;ffffffff811f9787&gt;] inode_sb_list_add+0x37/0x50<br>  [13510137.047372]  [&lt;ffffffffa07d9d94&gt;] xfs_setup_inode+0x34/0x2f0 [xfs]<br>  [13510137.047385]  [&lt;ffffffffa07dc0ed&gt;] xfs_ialloc+0x2cd/0x540 [xfs]<br>  [13510137.047395]  [&lt;ffffffffa07dc3d6&gt;] xfs_dir_ialloc+0x76/0x280 [xfs]<br>  [13510137.047409]  [&lt;ffffffffa07ec4cb&gt;] ? xfs_log_reserve+0x15b/0x1b0 [xfs]<br>  [13510137.047413]  [&lt;ffffffff8163b312&gt;] ? down_write+0x12/0x30<br>  [13510137.047423]  [&lt;ffffffffa07dc8b4&gt;] xfs_create+0x284/0x710 [xfs]<br>  [13510137.047427]  [&lt;ffffffff811e9a2d&gt;] ? __lookup_hash+0x2d/0x60<br>  [13510137.047437]  [&lt;ffffffffa07d8fdb&gt;] xfs_vn_mknod+0xbb/0x250 [xfs]<br>  [13510137.047438]  [&lt;ffffffff811ef60f&gt;] ? getname_flags+0x4f/0x1a0<br>  [13510137.047448]  [&lt;ffffffffa07d9186&gt;] xfs_vn_mkdir+0x16/0x20 [xfs]<br>  [13510137.047449]  [&lt;ffffffff811ead67&gt;] vfs_mkdir+0xb7/0x160<br>  [13510137.047450]  [&lt;ffffffff811f0c6f&gt;] SyS_mkdirat+0x6f/0xe0<br>  [13510137.047452]  [&lt;ffffffff811f0cf9&gt;] SyS_mkdir+0x19/0x20<br>  [13510137.047461]  [&lt;ffffffffa16343ca&gt;] prst_mkdir+0x13a/0x190 [RedCastle]<br>  [13510137.047465]  [&lt;ffffffffa1634458&gt;] rg_mkdir+0x38/0x50 [RedCastle]<br>  [13510137.047470]  [&lt;ffffffff816470c9&gt;] system_call_fastpath+0x16/0x1b<br>  [13510137.047471] ---[ end trace 9e81945cd74aab88 ]---<br><br>또한  메모리의 남아있는 정보를 확인한 결과, linked list에 부적합한 값의 형태를 발견할 수 있었음<br><br>&gt;   [13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br><br>crash&gt; struct list_head ffff88f9583df9c0 &lt;---|<br>struct list_head {                           |<br>  next = 0xffff88f9583df9c0, &lt;---------------|<br>  prev = 0xffff88e68baced00<br>}<br><br><br>참고로, double add가 발생해서 warning을 출력하는 부분,<br><br>lib/list_debug.c<br> 22 void __list_add(struct list_head *new,<br> 23                              struct list_head *prev,<br> 24                              struct list_head *next)<br> 25 {<br>[...]<br> 34        WARN(new == prev || new == next,<br> 35             &quot;list_add double add: new=%p, prev=%p, next=%p.\n&quot;,<br> 36             new, prev, next);  &lt;------ list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0<br> 37        next-&gt;prev = new;<br> 38        new-&gt;next = next;<br> 39        new-&gt;prev = prev;<br> 40        prev-&gt;next = new;<br> 41 }<br> 42 EXPORT_SYMBOL(__list_add);<br><br><br>(Action Plan)<br><br>결론적으로, 현재의 원인 파악은 어려운 상태입니다. 그래서 향후 이문제를 확인하기 위해서는 double add가 발생하는 시점에 해당 프로세스의 정보를 확인해야 원인파악이 가능할 것으로 보입니다.<br>그래서 아래와 같은 kernel parameter 설정이 필요하나, 실 운영시스템에서 어떠한 다른 warning이 발생할 경우에도 system이 crash될 수 있기 때문에 이부분에 대한 결정이 필요합니다.<br><br>  # sysctl -w kernel.panic_on_warn=1<br>  kernel.panic_on_warn = 1<br><br><br>아래는 간략하게 timeline으로 이슈가 발생한 history를 정리하여보았습니다.<br><br>1. A mkdir process was creating a directory /APP/scripts<br><br>During its execution a double add warning occured. <br>The process completed and its stack context is no longer available to us for analysis in the vmcore. <br><br>Time-to-NMI: (13561415-13510137)/60/60=14 hours<br><br>[13510137.047157] WARNING: at lib/list_debug.c:36 __list_add+0x8a/0xc0()<br>[13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br>[13510137.047286] CPU: 33 PID: 12160 Comm: mkdir Tainted: P           OE  ------------   3.10.0-327.44.2.el7.x86_64 #1<br>[...]<br>[13510137.047295] Call Trace:<br>[13510137.047311]  [&lt;ffffffff816369d1&gt;] dump_stack+0x19/0x1b<br>[13510137.047318]  [&lt;ffffffff8107b260&gt;] warn_slowpath_common+0x70/0xb0<br>[13510137.047320]  [&lt;ffffffff8107b2fc&gt;] warn_slowpath_fmt+0x5c/0x80<br>[13510137.047322]  [&lt;ffffffff8130c84a&gt;] __list_add+0x8a/0xc0<br>[13510137.047326]  [&lt;ffffffff811f9787&gt;] inode_sb_list_add+0x37/0x50<br>[13510137.047372]  [&lt;ffffffffa07d9d94&gt;] xfs_setup_inode+0x34/0x2f0 [xfs]<br>[13510137.047385]  [&lt;ffffffffa07dc0ed&gt;] xfs_ialloc+0x2cd/0x540 [xfs]<br>[13510137.047395]  [&lt;ffffffffa07dc3d6&gt;] xfs_dir_ialloc+0x76/0x280 [xfs]<br>[13510137.047409]  [&lt;ffffffffa07ec4cb&gt;] ? xfs_log_reserve+0x15b/0x1b0 [xfs]<br>[13510137.047413]  [&lt;ffffffff8163b312&gt;] ? down_write+0x12/0x30<br>[13510137.047423]  [&lt;ffffffffa07dc8b4&gt;] xfs_create+0x284/0x710 [xfs]<br>[13510137.047427]  [&lt;ffffffff811e9a2d&gt;] ? __lookup_hash+0x2d/0x60<br>[13510137.047437]  [&lt;ffffffffa07d8fdb&gt;] xfs_vn_mknod+0xbb/0x250 [xfs]<br>[13510137.047438]  [&lt;ffffffff811ef60f&gt;] ? getname_flags+0x4f/0x1a0<br>[13510137.047448]  [&lt;ffffffffa07d9186&gt;] xfs_vn_mkdir+0x16/0x20 [xfs]<br>[13510137.047449]  [&lt;ffffffff811ead67&gt;] vfs_mkdir+0xb7/0x160<br>[13510137.047450]  [&lt;ffffffff811f0c6f&gt;] SyS_mkdirat+0x6f/0xe0<br>[13510137.047452]  [&lt;ffffffff811f0cf9&gt;] SyS_mkdir+0x19/0x20<br>[13510137.047461]  [&lt;ffffffffa16343ca&gt;] prst_mkdir+0x13a/0x190 [RedCastle]<br>[13510137.047465]  [&lt;ffffffffa1634458&gt;] rg_mkdir+0x38/0x50 [RedCastle]<br>[13510137.047470]  [&lt;ffffffff816470c9&gt;] system_call_fastpath+0x16/0x1b <br><br>2. xfs_db is run on the filesystem, it has requested a sync of the filesystem.<br><br>While flushing inodes it has hit the duplicate entry in the inode list and is now stuck in an infinite loop in sync_inodes_sb (wait_sb_inodes is in-lined in this function).<br><br>Note: any process could request a sync, this is just a victim of the double add. <br>From earlier analysis<br>&gt; The spinlock 'inode_sb_list_lock' was held by PID 61776 and actually it's the longest CPU holder also.<br>&gt;  ...<br>&gt;  crash&gt; pd 32520/60/60<br>&gt;  $4 = 9<br>  <br>Time-to-NMI: ~9hours<br><br>crash&gt; bt 61776<br>PID: 61776  TASK: ffff88bf26cd5080  CPU: 0   COMMAND: &quot;xfs_db&quot;<br>[...]<br>--- &lt;NMI exception stack&gt; ---<br> #7 [ffff88bd351a3d70] _raw_spin_lock at ffffffff8163e362<br> #8 [ffff88bd351a3d78] sync_inodes_sb at ffffffff8120938e<br> #9 [ffff88bd351a3e18] sync_filesystem at ffffffff8121050b<br>#10 [ffff88bd351a3e30] fsync_bdev at ffffffff8121ab94<br>#11 [ffff88bd351a3e50] blkdev_ioctl at ffffffff812d945b<br>#12 [ffff88bd351a3ea8] block_ioctl at ffffffff8121a1f1<br><br><br>3. From this moment on we can see processes blocked beacause they cannot access this lock. <br><br>crash&gt; ps -m | grep UN | wc -l<br>1214<br><br>crash&gt; ps -m | grep UN | sort | tail<br>[  0 08:56:11.527] [UN]  PID: 119337  TASK: ffff887e1c351700  CPU: 28  COMMAND: &quot;REC_10_UPD_W82&quot;<br>[  0 08:57:04.288] [UN]  PID: 62001  TASK: ffff88fd4f6cb980  CPU: 69  COMMAND: &quot;df&quot;<br>[  0 08:57:51.061] [UN]  PID: 116739  TASK: ffff88bf1aa7e780  CPU: 45  COMMAND: &quot;REC_00_BTC_W175&quot;<br>[  0 08:58:05.366] [UN]  PID: 61962  TASK: ffff88e6bee95080  CPU: 68  COMMAND: &quot;df&quot;<br>...<br><br>4. The XFS active item list deamon began blocking, it can no longer update metadata on disk.<br><br>Time-to-NMI: (13561415-13529096)/60/60=8hours<br><br>[13529096.948566] INFO: task xfsaild/dm-20:2328 blocked for more than 120 seconds.<br>[13529096.948604] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.<br>[13529096.948639] Call Trace:<br>[13529096.948648]  [&lt;ffffffff8163c0d9&gt;] schedule+0x29/0x70<br>[13529096.948711]  [&lt;ffffffffa07ebaf2&gt;] _xfs_log_force+0x192/0x290 [xfs]<br>[13529096.948717]  [&lt;ffffffff810b8d30&gt;] ? wake_up_state+0x20/0x20<br>[13529096.948729]  [&lt;ffffffffa07ebc16&gt;] xfs_log_force+0x26/0x80 [xfs]<br>[...]<br><br><br>감사합니다.<br><br><publishedDate>2017-07-22T10:16:50Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JvrqeIAB"><br>======================<br><b>생성계정 : Song, Chang-An</b><br><b>생성날짜 : 2017-07-19T06:10:29Z</b><br><b>마지막 답변자 : Song, Chang-An</b><br><b>마지막 수정 일자 : 2017-07-19T06:10:29Z</b><br><br>안녕하세요? Red Hat 송 창 안 입니다.<br>먼저 추가적으로 확인된 내용에 대해서 답변 하도록 하겠습니다.<br><br>현재 저희는 기본적으로 Redcastle 모듈을 제외하고 커널에서 이슈가 생길만한 내용을 확인 하였으며, <br>커널코드내에서 제공 해주신 core 파일을 분석을 토대로 커널 코드를 확인 하였지만, 의심되는 지점은 발견되지 않았습니다.<br><br>따라서, sysctl 정보를 확인하여, warn이 발생 시에 core file이 발생 될수 있도록 설정을 시스템에 적용을 해주신다면,<br>이후 발생한 코어 파일에 대해서 분석할 때 많은 도움이 될 것으로 판단 됩니다.<br><br>기본적으로 설정은 disable 상태로 판단 됩니다.<br><br>  # sysctl kernel.panic_on_warn<br>  kernel.panic_on_warn = 0<br><br>수정해야 할 값은 아래와 같습니다.<br><br>  # sysctl -w kernel.panic_on_warn=1<br>  kernel.panic_on_warn = 1<br><br>추가적으로 아래 내용은 저희 전문 엔지니어와 분석된 내용입니다. 이미 이해하고 계신 내용으로 커널에는 다음과 같은 계층을 구조로 가지고 있습니다.<br>현재의 그림은 Redcastle 커널 모듈은 제외한 내용이 포함되어 있습니다.<br><br>           system call(open,read,write, ...)<br>                    ^                             User level<br>+-------------------|----------------------------------------+<br>                    |                            Kernel level<br>     +--------------v-------------+<br>     |                            |<br>     |     Virtual File System    |<br>     |              ^             |<br>     +--------------|-------------+<br>     |              v             |<br>     |          XFS ^             |<br>     |              |             |<br>     +--------------|-------------+<br>     |              v             |<br>     |        block device        |<br>     |              ^             |<br>     +--------------|-------------+<br>     |              v             |<br>     |        SAS or SATA disk    |<br>     |                            |<br>     +----------------------------+<br><br><br>call trace 에 대한 내용을 확인 하시면,  처음 시작의 call trace는 Redcasle의 모듈을 거쳐 가는것에 대해서 확인 이 되었습니다.<br>  <br> crash&gt; log<br>  [13510137.047133] ------------[ cut here ]------------<br>  [13510137.047157] WARNING: at lib/list_debug.c:36 __list_add+0x8a/0xc0()<br>  [13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br>  [13510137.047165] Modules linked in: rpcsec_gss_krb5 nfsv4 dns_resolver nls_utf8 isofs loop fuse btrfs zlib_deflate raid6_pq xor vfat msdos fat ext4 mbcache jbd2 bridge stp llc ipmi_watchdog ipmi_devintf RedCastle(POE) nfsv3 nfs fscache bonding iptable_filter iTCO_wdt iTCO_vendor_support intel_powerclamp coretemp intel_rapl kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd pcspkr dm_service_time ipmi_ssif sb_edac edac_core sg lpc_ich mfd_core hpilo hpwdt ioatdma shpchp wmi ipmi_si ipmi_msghandler acpi_power_meter dm_multipath dm_mod nfsd auth_rpcgss nfs_acl lockd grace binfmt_misc sunrpc ip_tables xfs libcrc32c crc32c_intel serio_raw ixgbe(OE) vxlan ip6_udp_tunnel udp_tunnel dca mgag200 syscopyarea sysfillrect sysimgblt i2c_algo_bit drm_kms_helper ttm drm i2c_core<br>  [13510137.047272]  tg3(OE) ptp pps_core sd_mod crc_t10dif crct10dif_generic crct10dif_pclmul crct10dif_common qla2xxx(OE) scsi_transport_fc scsi_tgt hpsa(OE) scsi_transport_sas [last unloaded: RedCastle]<br>  [13510137.047286] CPU: 33 PID: 12160 Comm: mkdir Tainted: P           OE  ------------   3.10.0-327.44.2.el7.x86_64 #1<br>  [13510137.047288] Hardware name: HP ProLiant DL580 Gen9/ProLiant DL580 Gen9, BIOS U17 09/12/2016<br>  [13510137.047289]  ffff887e2de9bb70 000000002c1beaf2 ffff887e2de9bb28 ffffffff816369d1<br>  [13510137.047291]  ffff887e2de9bb60 ffffffff8107b260 ffff88f9583df9c0 ffff88f9583df9c0<br>  [13510137.047293]  ffff88ff156b18a0 0000000000000000 ffff883f7f31e600 ffff887e2de9bbc8<br>  [13510137.047295] Call Trace:<br>  [13510137.047311]  [&lt;ffffffff816369d1&gt;] dump_stack+0x19/0x1b<br>  [13510137.047318]  [&lt;ffffffff8107b260&gt;] warn_slowpath_common+0x70/0xb0<br>  [13510137.047320]  [&lt;ffffffff8107b2fc&gt;] warn_slowpath_fmt+0x5c/0x80<br>  [13510137.047322]  [&lt;ffffffff8130c84a&gt;] __list_add+0x8a/0xc0<br>  [13510137.047326]  [&lt;ffffffff811f9787&gt;] inode_sb_list_add+0x37/0x50<br>  [13510137.047372]  [&lt;ffffffffa07d9d94&gt;] xfs_setup_inode+0x34/0x2f0 [xfs]<br>  [13510137.047385]  [&lt;ffffffffa07dc0ed&gt;] xfs_ialloc+0x2cd/0x540 [xfs]<br>  [13510137.047395]  [&lt;ffffffffa07dc3d6&gt;] xfs_dir_ialloc+0x76/0x280 [xfs]<br>  [13510137.047409]  [&lt;ffffffffa07ec4cb&gt;] ? xfs_log_reserve+0x15b/0x1b0 [xfs]<br>  [13510137.047413]  [&lt;ffffffff8163b312&gt;] ? down_write+0x12/0x30<br>  [13510137.047423]  [&lt;ffffffffa07dc8b4&gt;] xfs_create+0x284/0x710 [xfs]<br>  [13510137.047427]  [&lt;ffffffff811e9a2d&gt;] ? __lookup_hash+0x2d/0x60<br>  [13510137.047437]  [&lt;ffffffffa07d8fdb&gt;] xfs_vn_mknod+0xbb/0x250 [xfs]<br>  [13510137.047438]  [&lt;ffffffff811ef60f&gt;] ? getname_flags+0x4f/0x1a0<br>  [13510137.047448]  [&lt;ffffffffa07d9186&gt;] xfs_vn_mkdir+0x16/0x20 [xfs]<br>  [13510137.047449]  [&lt;ffffffff811ead67&gt;] vfs_mkdir+0xb7/0x160<br>  [13510137.047450]  [&lt;ffffffff811f0c6f&gt;] SyS_mkdirat+0x6f/0xe0<br>  [13510137.047452]  [&lt;ffffffff811f0cf9&gt;] SyS_mkdir+0x19/0x20<br>  [13510137.047461]  [&lt;ffffffffa16343ca&gt;] prst_mkdir+0x13a/0x190 [RedCastle]  &lt;------ RedCastle 모듈<br>  [13510137.047465]  [&lt;ffffffffa1634458&gt;] rg_mkdir+0x38/0x50 [RedCastle]      &lt;------ RedCastle 모듈<br>  [13510137.047470]  [&lt;ffffffff816470c9&gt;] system_call_fastpath+0x16/0x1b<br>  [13510137.047471] ---[ end trace 9e81945cd74aab88 ]---<br><br>XFS는 자체적으로 inode 수명을 관리합니다. 이는 리눅스에서 지원하는 다른 파일 시스템에 비해 고유한 특성 입니다. <br>특히, xfs inode는 vfs inode를 포함을 하고 있습니다. 즉, XFS는 inode 할당을 관리하며, inode_sb_list_add를 호출하는 것과 같은 vfs 관리를 수행하게됩니다. <br>RHEL에서 이 함수를 직접 호출하는 유일한 파일 시스템입니다. super block에 inode들의 리스트를 추가하려고 할때 이중으로 추가가 되려고 하였으며, <br>이것은 &quot;list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0&quot; 메세지를 통해서 도 확인 하실 수 있습니다. <br><br>lib/list_debug.c<br> 22 void __list_add(struct list_head *new,<br> 23                              struct list_head *prev,<br> 24                              struct list_head *next)<br> 25 {<br> 26        WARN(next-&gt;prev != prev,<br> 27                &quot;list_add corruption. next-&gt;prev should be &quot;<br> 28                &quot;prev (%p), but was %p. (next=%p).\n&quot;,<br> 29                prev, next-&gt;prev, next);<br> 30        WARN(prev-&gt;next != next,<br> 31                &quot;list_add corruption. prev-&gt;next should be &quot;<br> 32                &quot;next (%p), but was %p. (prev=%p).\n&quot;,<br> 33                next, prev-&gt;next, prev);<br> 34        WARN(new == prev || new == next,<br> 35             &quot;list_add double add: new=%p, prev=%p, next=%p.\n&quot;,<br> 36             new, prev, next);  &lt;------ list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0<br> 37        next-&gt;prev = new;<br> 38        new-&gt;next = next;<br> 39        new-&gt;prev = prev;<br> 40        prev-&gt;next = new;<br> 41 }<br> 42 EXPORT_SYMBOL(__list_add);<br><br>메모리 주소를 확인한 결과는 다음과 같습니다.<br><br>crash&gt; kmem ffff88f9583df9c0<br>CACHE            NAME                 OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE<br>ffff88ff229fc400 xfs_inode               1024     322842    326820  10894    32k<br>  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE<br>  ffffea03e560f600  ffff88f9583d8000     3     30         30     0<br>  FREE / [ALLOCATED]<br>  [ffff88f9583df700]<br><br>crash&gt; xfs_inode.i_ino,i_vnode.i_sb ffff88f9583df700<br>  i_ino = 132<br>  i_vnode.i_sb = 0xffff88ff156b1800,<br><br>mount point 정보는 아래와 같습니다.<br>crash&gt; mount | grep ffff88ff156b1800<br>ffff883f24f49300 ffff88ff156b1800 xfs    /dev/mapper/LLDECCSAPVG-APP /APP <br><br>crash&gt; struct -o xfs_inode ffff88f9583df700<br>struct xfs_inode {<br>  [ffff88f9583df700] struct xfs_mount *i_mount;<br>  [ffff88f9583df708] struct xfs_dquot *i_udquot;<br>  [ffff88f9583df710] struct xfs_dquot *i_gdquot;<br>  [ffff88f9583df718] struct xfs_dquot *i_pdquot;<br>  [ffff88f9583df720] xfs_ino_t i_ino;<br>  [ffff88f9583df728] struct xfs_imap i_imap;<br>  [ffff88f9583df738] xfs_ifork_t *i_afp;<br>  [ffff88f9583df740] xfs_ifork_t i_df;<br>  [ffff88f9583df780] const struct xfs_dir_ops *d_ops;<br>  [ffff88f9583df788] struct xfs_inode_log_item *i_itemp;<br>  [ffff88f9583df790] mrlock_t i_lock;<br>  [ffff88f9583df7b0] mrlock_t i_iolock;<br>  [ffff88f9583df7d0] mrlock_t i_mmaplock;<br>  [ffff88f9583df7f0] atomic_t i_pincount;<br>  [ffff88f9583df7f4] spinlock_t i_flags_lock;<br>  [ffff88f9583df7f8] unsigned long i_flags;<br>  [ffff88f9583df800] unsigned int i_delayed_blks;<br>  [ffff88f9583df804] spinlock_t i_size_lock;<br>  [ffff88f9583df808] xfs_icdinode_t i_d;<br>  [ffff88f9583df8b8] struct inode i_vnode;<br>}<br>SIZE: 1024<br><br>struct inode 내용 확인 (xfs_inode.i_vnode)<br>crash&gt; struct inode ffff88f9583df8b8<br>...<br>      i_dentry = {<br>        first = 0xffff88b2b0902fb0<br>      }, <br><br>crash&gt; kmem 0xffff88b2b0902fb0<br>CACHE            NAME                 OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE<br>ffff88bf7f428b00 dentry                   192    3285159   3296790  78495     8k<br>  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE<br>  ffffea02cac24080  ffff88b2b0902000     2     42         25    17<br>  FREE / [ALLOCATED]<br>  [ffff88b2b0902f00]<br><br>해당하는 파일의 정보는 아래와 같습니다.<br>crash&gt; files -d ffff88b2b0902f00<br>     DENTRY           INODE           SUPERBLK     TYPE PATH<br>ffff88b2b0902f00 ffff88f9583df8b8 ffff88ff156b1800 DIR  /APP/scripts<br><br>inode_sb_list_add 함수로 부터 호출된 경로를 확인 할 수 있습니다.<br><br>fs/inode.c<br> 443 /**<br> 444  * inode_sb_list_add - add inode to the superblock list of inodes<br> 445  * @inode: inode to add<br> 446  */<br> 447 void inode_sb_list_add(struct inode *inode)<br> 448 {<br> 449         spin_lock(&amp;inode_sb_list_lock);<br> 450         list_add(&amp;inode-&gt;i_sb_list, &amp;inode-&gt;i_sb-&gt;s_inodes); &lt;------------<br> 451         spin_unlock(&amp;inode_sb_list_lock);<br> 452 }<br><br>커널 코드에서 inode_sb_list_add의 call에 대해서 확인 한 결과, XFS 파일시스템에서 직접 호출하는 것을 알 수 있습니다.<br>  C symbol: inode_sb_list_add<br><br>    File               Function          Line<br>  0 fs/inode.c         &lt;global&gt;           453 EXPORT_SYMBOL_GPL(inode_sb_list_add);<br>  1 fs/inode.c         &lt;global&gt;          1080 inode_sb_list_add(inode);<br>  2 include/linux/fs.h &lt;global&gt;          2747 extern void inode_sb_list_add(struct inode *inode);<br>  3 fs/inode.c         inode_sb_list_add  447 void inode_sb_list_add(struct inode *inode)<br>  4 fs/inode.c         new_inode          949 inode_sb_list_add(inode);<br>  5 fs/inode.c         iget_locked       1147 inode_sb_list_add(inode);<br>  6 fs/xfs/xfs_iops.c  xfs_setup_inode   1220 inode_sb_list_add(inode); &lt;------------<br><br>  fs/xfs/xfs_iops.c<br>  1202 /*<br>  1203  * Initialize the Linux inode and set up the operation vectors.<br>  1204  *<br>  1205  * When reading existing inodes from disk this is called directly from xfs_iget,<br>  1206  * when creating a new inode it is called from xfs_ialloc after setting up the<br>  1207  * inode. These callers have different criteria for clearing XFS_INEW, so leave<br>  1208  * it up to the caller to deal with unlocking the inode appropriately.<br>  1209  */<br>  1210 void<br>  1211 xfs_setup_inode(<br>  1212         struct xfs_inode        *ip)<br>  1213 {<br>  1214         struct inode            *inode = &amp;ip-&gt;i_vnode;<br>  1215         gfp_t                   gfp_mask;<br>  1216 <br>  1217         inode-&gt;i_ino = ip-&gt;i_ino;<br>  1218         inode-&gt;i_state = I_NEW;<br>  1219 <br>  1220         inode_sb_list_add(inode); &lt;------------<br>  1221         /* make the inode look hashed for the writeback code */<br>  1222         hlist_add_fake(&amp;inode-&gt;i_hash);<br>               ...<br><br>다른 파일 시스템은 다음 함수를 수행합니다.<br>  fs/inode.c<br>   928 <br>   929 /**<br>   930  *      new_inode       - obtain an inode<br>   931  *      @sb: superblock<br>   932  *<br>   933  *      Allocates a new inode for given superblock. The default gfp_mask<br>   934  *      for allocations related to inode-&gt;i_mapping is GFP_HIGHUSER_MOVABLE.<br>   935  *      If HIGHMEM pages are unsuitable or it is known that pages allocated<br>   936  *      for the page cache are not reclaimable or migratable,<br>   937  *      mapping_set_gfp_mask() must be called with suitable flags on the<br>   938  *      newly created inode's mapping<br>   939  *<br>   940  */<br>   941 struct inode *new_inode(struct super_block *sb)<br>   942 {<br>   943         struct inode *inode;<br>   944 <br>   945         spin_lock_prefetch(&amp;inode_sb_list_lock);<br>   946 <br>   947         inode = new_inode_pseudo(sb);<br>   948         if (inode)<br>   949                 inode_sb_list_add(inode);<br>   950         return inode;<br>   951 }<br>   952 EXPORT_SYMBOL(new_inode);<br><br>감사합니다.<br>송 창 안 드림.<br><br><publishedDate>2017-07-19T06:10:29Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JvLkcIAF"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-07-17T06:18:14Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-07-17T06:18:14Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>아래 부분이 스토리지 정보 수집할 때 매일 수행되는 스크립트 인데요.<br>pyc 부분이 정확이 어떻게 동작하는지는 벤더 통해서 조금 더 자세히 확인해봐야할 것 같습니다.<br><br>root@DECCAL03SL /sysadmin/work/hsrm/fletaAgent/log # crontab -l<br>=== 생략 ===<br>############### hsrm diskinfo ################## <br>30 0 * * * /sysadmin/work/hsrm/fletaAgent/fletaDaemon.sh  &gt; /dev/null 2&gt;&amp;1<br>=== 생략 ===<br><br>root@DECCAL03SL /sysadmin/work/hsrm/fletaAgent/log # cat /sysadmin/work/hsrm/fletaAgent/fletaDaemon.sh<br>#!/usr/bin/bash<br><br>############################################<br># Cron Sample<br># 10 0 * * * /sysadmin/work/hsrm/fletaAgent/fletaDaemon.sh<br>############################################<br><br>############################################<br>FLETA_HOME='/sysadmin/work/hsrm/fletaAgent'<br>############################################<br><br>PYTHON_HOME=$FLETA_HOME/python26/bin<br>PATH=$PYTHON_HOME:$FLETA_HOME/bin:/sbin:$PATH<br><br>if [ ! -d &quot;$FLETA_HOME&quot; ];then<br>    echo &quot;ERROR : ${FLETA_HOME} DIR NOT FOUND&quot;<br>    exit<br>fi<br><br>if [ ! -d &quot;$PYTHON_HOME&quot; ];then<br>    echo &quot;ERROR : ${PYTHON_HOME} DIR NOT FOUND&quot;<br>    exit<br>fi<br><br>export PYTHON_HOME<br>export FLETA_HOME<br>export PATH<br><br>argc=$#<br>argv=$1<br><br>if [ $argv == 'check' ];then<br>    if [ -f $FLETA_HOME/bin/diskinfo.pyc ];then<br>        echo &quot;OK&quot;<br>    else<br>        echo &quot;$FLETA_HOME/bin/diskinfo.pyc&quot; File NOT FOUND<br>    fi<br>else<br>    dcnt=`ps -ef | grep ${FLETA_HOME}/bin/diskinfo.pyc | grep -v grep | wc -l`<br><br>    if [ $dcnt -eq 0 ];then<br>        if [ -f $FLETA_HOME/bin/diskinfo.pyc ];then<br>            python $FLETA_HOME/bin/diskinfo.pyc SCH<br>        fi<br>    fi<br>fi<br><br><publishedDate>2017-07-17T06:18:14Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000JvLawIAF"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2017-07-17T05:56:53Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2017-07-17T05:57:35Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>조금전에 유선통화했던.. 그 스토리지 벤더에서 crontab에 걸어놓은 정보를 좀 받아볼 수 있을런지요?<br><br><br>확인 바랍니다.<br>======================<br>감사합니다.<br><br><publishedDate>2017-07-17T05:56:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Jv0uhIAB"><br>======================<br><b>생성계정 : Song, Chang-An</b><br><b>생성날짜 : 2017-07-14T02:08:22Z</b><br><b>마지막 답변자 : Song, Chang-An</b><br><b>마지막 수정 일자 : 2017-07-14T02:08:22Z</b><br><br>안녕하세요? Red Hat 송 창 안 입니다.<br><br>현재의 이슈에 대해서 이전 커맨트에서 알려드린 내용과 같이<br>xfs 관련 내용에 대해서 추가적으로 저희 전문 엔지니어에게 분석된 코어에 대해서 확인 요청을 진행 하였습니다.<br>추가적으로 전문엔지니어로 부터 확인 되는 내용에 대해서, 커맨트를 통해 업데이트 하도록 하겠습니다. <br><br>감사합니다.<br>송 창 안 드림.<br><br><publishedDate>2017-07-14T02:08:22Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JumjPIAR"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-07-13T05:48:46Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-07-13T05:48:46Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>지금까지의 분석 결과를 알려드립니다.<br><br>&lt;분석 내용 요약&gt;<br>1214개에 달하는 엄청난 양의 프로세스들이 D state로 되어 있었고, 가장 길었던 것은 9시간 이상이었습니다. <br>PID 61776는  spinlock 'inode_sb_list_lock' 을 갖고 있었고, 그 상황에서 sync가 진행중이었습니다.<br>이 프로세스는 이미 사라져서 확인할 수 있는 방법은 없지만, backtrace에서 'inode_sb_list_add()'을 진행하고 있었음을 보여주고 있습니다.<br><br>문제의 원인이 linked list의 손상에 의한 것일 가능성이 있는데, 확실히 무엇이 손상을 주었는지는 말씀드리기는 어려운 상태입니다.<br>XFS의 버그일 수도 있고 RedCastle에 의한 문제일 수도 있습니다.<br><br>XFS 관련하여 추가 분석을 하기 위해 파일시스템 전문 엔지니어 그룹에 추가로 분석 요청을 한 상태입니다.<br>한편, RedCastle과 관련하여 중요 커널 기능에 대한 intercepting 과 관련한 에러가 확인되었습니다. 따라서 RedCastle로 인한 문제일 가능성도 상당히 높습니다.<br><br>추가 분석에 대한 업데이트 사항이 확인되는 데로 알려드리겠습니다.<br><br>&lt;커널 전문 엔지니어의 분석 상세 내용&gt;<br>The system had huge number of D state processes and the longest one was there for more than 9 hours.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; ps -m | grep UN | wc -l<br>1214<br><br>crash&gt; ps -m | grep UN | sort | tail<br>[  0 08:56:11.527] [UN]  PID: 119337  TASK: ffff887e1c351700  CPU: 28  COMMAND: &quot;REC_10_UPD_W82&quot;<br>[  0 08:57:04.288] [UN]  PID: 62001  TASK: ffff88fd4f6cb980  CPU: 69  COMMAND: &quot;df&quot;<br>[  0 08:57:51.061] [UN]  PID: 116739  TASK: ffff88bf1aa7e780  CPU: 45  COMMAND: &quot;REC_00_BTC_W175&quot;<br>[  0 08:58:05.366] [UN]  PID: 61962  TASK: ffff88e6bee95080  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 08:58:19.052] [UN]  PID: 2226   TASK: ffff883f160b4500  CPU: 3   COMMAND: &quot;kworker/3:1H&quot;<br>[  0 08:58:19.052] [UN]  PID: 76266  TASK: ffff883ed7a80b80  CPU: 3   COMMAND: &quot;kworker/3:0&quot;<br>[  0 08:59:06.448] [UN]  PID: 61933  TASK: ffff88e6bee94500  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 09:00:07.519] [UN]  PID: 61905  TASK: ffff88e6bee96780  CPU: 68  COMMAND: &quot;df&quot;<br>[  0 09:00:39.102] [UN]  PID: 4955   TASK: ffff88ff23c95080  CPU: 31  COMMAND: &quot;ntpd&quot;<br>[  0 09:01:56.493] [UN]  PID: 100701  TASK: ffff8828a57c5080  CPU: 13  COMMAND: &quot;ontuned&quot;<br>---------------------------------------------------------------------------------------<br><br>This process was about to take a mutex lock.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; bt<br>PID: 100701  TASK: ffff8828a57c5080  CPU: 13  COMMAND: &quot;ontuned&quot;<br> #0 [ffff88210f3eb9c0] __schedule at ffffffff8163ba3d<br> #1 [ffff88210f3eba28] schedule_preempt_disabled at ffffffff8163d1b9<br> #2 [ffff88210f3eba38] __mutex_lock_slowpath at ffffffff8163aeb5<br> #3 [ffff88210f3eba98] mutex_lock at ffffffff8163a31f<br> #4 [ffff88210f3ebab0] lookup_slow at ffffffff81633cf9<br> #5 [ffff88210f3ebae8] link_path_walk at ffffffff811ee00f<br> #6 [ffff88210f3ebb98] path_lookupat at ffffffff811ee20b<br> #7 [ffff88210f3ebc30] filename_lookup at ffffffff811ee96b<br> #8 [ffff88210f3ebc68] kern_path at ffffffff811f04f5<br> #9 [ffff88210f3ebd38] wr_lookupname at ffffffffa1627846 [RedCastle]<br>#10 [ffff88210f3ebd90] ips_open_pre at ffffffffa1634558 [RedCastle]<br>#11 [ffff88210f3ebf00] prst_open at ffffffffa1634a47 [RedCastle]<br>#12 [ffff88210f3ebf58] rg_open at ffffffffa1634bf0 [RedCastle]<br>#13 [ffff88210f3ebf80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007ff91cf9186d  RSP: 00007ff919d634c0  RFLAGS: 00010202<br>    RAX: 0000000000000002  RBX: ffffffff816470c9  RCX: 00007ff8f8026760<br>    RDX: 00000000000001b6  RSI: 0000000000000000  RDI: 000000000041b552<br>    RBP: 00007ff919d62a10   R8: 000000000041b4a6   R9: 0000000000000000<br>    R10: 0000000000000024  R11: 0000000000000293  R12: 0000000000000008<br>    R13: 00007ff8f80133a0  R14: 000000000041b4a0  R15: ffffffffa1634bf0<br>    ORIG_RAX: 0000000000000002  CS: 0033  SS: 002b<br>    <br><br><br>crash&gt; dis -lr ffffffff81633cf9<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1521<br>0xffffffff81633cc6 &lt;lookup_slow&gt;:       nopl   0x0(%rax,%rax,1) [FTRACE NOP]<br>0xffffffff81633ccb &lt;lookup_slow+0x5&gt;:   push   %rbp<br>0xffffffff81633ccc &lt;lookup_slow+0x6&gt;:   mov    %rsp,%rbp<br>0xffffffff81633ccf &lt;lookup_slow+0x9&gt;:   push   %r14<br>0xffffffff81633cd1 &lt;lookup_slow+0xb&gt;:   push   %r13<br>0xffffffff81633cd3 &lt;lookup_slow+0xd&gt;:   push   %r12<br>0xffffffff81633cd5 &lt;lookup_slow+0xf&gt;:   push   %rbx<br>0xffffffff81633cd6 &lt;lookup_slow+0x10&gt;:  mov    %rdi,%rbx<br>0xffffffff81633cd9 &lt;lookup_slow+0x13&gt;:  push   %rcx<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1525<br>0xffffffff81633cda &lt;lookup_slow+0x14&gt;:  mov    0x8(%rdi),%r13<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1526<br>0xffffffff81633cde &lt;lookup_slow+0x18&gt;:  mov    0x30(%rdi),%rdi<br>0xffffffff81633ce2 &lt;lookup_slow+0x1c&gt;:  cmp    0x30(%r13),%rdi<br>0xffffffff81633ce6 &lt;lookup_slow+0x20&gt;:  je     0xffffffff81633cea &lt;lookup_slow+0x24&gt;<br>0xffffffff81633ce8 &lt;lookup_slow+0x22&gt;:  ud2    <br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/namei.c: 1528<br>0xffffffff81633cea &lt;lookup_slow+0x24&gt;:  add    $0xa8,%rdi<br>0xffffffff81633cf1 &lt;lookup_slow+0x2b&gt;:  mov    %rsi,%r12<br>0xffffffff81633cf4 &lt;lookup_slow+0x2e&gt;:  callq  0xffffffff8163a300 &lt;mutex_lock&gt;<br>======================<br>1519 /* Fast lookup failed, do it the slow way */<br>1520 static int lookup_slow(struct nameidata *nd, struct path *path)<br>1521 {<br>1522   struct dentry *dentry, *parent;<br>1523   int err;<br>1524 <br>1525   parent = nd-&gt;path.dentry;<br>1526   BUG_ON(nd-&gt;inode != parent-&gt;d_inode);<br>1527 <br>1528   mutex_lock(&amp;parent-&gt;d_inode-&gt;i_mutex);<br>---------------------------------------------------------------------------------------<br><br>The mutex lock was for '/proc/' entry.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; nameidata.path ffff88210f3ebc70<br>  path = {<br>    mnt = 0xffff887f2763a520, <br>    dentry = 0xffff88bf27008240<br>  }<br>crash&gt; dentry.d_iname,d_parent 0xffff88bf27008240<br>  d_iname = &quot;/\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000&quot;<br>  d_parent = 0xffff88bf27008240<br>crash&gt; vfsmount 0xffff887f2763a520<br>struct vfsmount {<br>  mnt_root = 0xffff88bf27008240, <br>  mnt_sb = 0xffff88bf2f524000, <br>  mnt_flags = 0x1027<br>}<br>crash&gt; mount | grep ffff88bf2f524000<br>ffff887f2763a500 ffff88bf2f524000 proc   proc      /proc  <br>---------------------------------------------------------------------------------------<br><br>There were huge number of waiting processes for this mutex.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; dentry.d_inode 0xffff88bf27008240<br>  d_inode = 0xffff88bf27018040<br>crash&gt; inode.i_mutex 0xffff88bf27018040 -ox<br>struct inode {<br>  [ffff88bf270180e8] struct mutex i_mutex;<br>}<br><br>crash&gt; crashinfo --mutex=0xffff88bf270180e8 | wc -l<br>736<br>---------------------------------------------------------------------------------------<br><br>The owner of this mutex was 'saposcol' and it was looping in the CPU for long.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; bt 26332<br>PID: 26332  TASK: ffff88e722581700  CPU: 71  COMMAND: &quot;saposcol&quot;<br> #0 [ffff88ff7f2c5e70] crash_nmi_callback at ffffffff81045982<br> #1 [ffff88ff7f2c5e80] nmi_handle at ffffffff8163fb59<br> #2 [ffff88ff7f2c5ec8] do_nmi at ffffffff8163fc70<br> #3 [ffff88ff7f2c5ef0] end_repeat_nmi at ffffffff8163ef93<br>    [exception RIP: _raw_spin_lock+0x37]<br>    RIP: ffffffff8163e387  RSP: ffff88f7e62ebd50  RFLAGS: 00000206<br>    RAX: 0000000000002d09  RBX: ffff88ff21b92190  RCX: 00000000000077d4<br>    RDX: 00000000000077d6  RSI: 00000000000077d6  RDI: ffffffff81943400<br>    RBP: ffff88f7e62ebd50   R8: 000000000001a060   R9: 0000000000000001<br>    R10: 0000000000000000  R11: 0000000000000000  R12: ffff88bf2614e780<br>    R13: 0000000000000000  R14: ffff88bf27008240  R15: ffffffff811f2d50<br>    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018<br>--- &lt;NMI exception stack&gt; ---<br> #4 [ffff88f7e62ebd50] _raw_spin_lock at ffffffff8163e387<br> #5 [ffff88f7e62ebd58] inode_sb_list_add at ffffffff811f9769<br> #6 [ffff88f7e62ebd70] new_inode at ffffffff811fbd09<br> #7 [ffff88f7e62ebd88] proc_pid_make_inode at ffffffff8124db7b<br> #8 [ffff88f7e62ebdb8] proc_pid_instantiate at ffffffff8124dc6b<br> #9 [ffff88f7e62ebdd8] proc_fill_cache at ffffffff8124e4c9<br>#10 [ffff88f7e62ebe48] proc_pid_readdir at ffffffff8124f1fe<br>#11 [ffff88f7e62ebec8] proc_root_readdir at ffffffff8124a1af<br>#12 [ffff88f7e62ebef0] vfs_readdir at ffffffff811f2c40<br>#13 [ffff88f7e62ebf30] sys_getdents at ffffffff811f3065<br>#14 [ffff88f7e62ebf80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007f0098d349d5  RSP: 00007ffec8145320  RFLAGS: 00010202<br>    RAX: 000000000000004e  RBX: ffffffff816470c9  RCX: 000000000000bd31<br>    RDX: 0000000000008000  RSI: 0000000001936750  RDI: 0000000000000006<br>    RBP: 0000000001936750   R8: 0000000000020000   R9: 0000000000008030<br>    R10: 0000000000000076  R11: 0000000000000246  R12: 00000000004d1640<br>    R13: 0000000000000000  R14: ffffffffffffff60  R15: 0000000001936750<br>    ORIG_RAX: 000000000000004e  CS: 0033  SS: 002b<br>    <br><br><br> 23 int vfs_readdir(struct file *file, filldir_t filler, void *buf)<br> 24 {<br> 25   struct inode *inode = file_inode(file);<br> 26   int res = -ENOTDIR;<br> 27   if (!file-&gt;f_op || !file-&gt;f_op-&gt;readdir)<br> 28     goto out;<br> 29 <br> 30   res = security_file_permission(file, MAY_READ);<br> 31   if (res)<br> 32     goto out;<br> 33 <br> 34   res = mutex_lock_killable(&amp;inode-&gt;i_mutex);<br><br> <br> <br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/readdir.c: 34<br>0xffffffff811f2bf0 &lt;vfs_readdir+0x60&gt;:  lea    0xa8(%r12),%r15<br>0xffffffff811f2bf8 &lt;vfs_readdir+0x68&gt;:  mov    %r15,%rdi<br>0xffffffff811f2bfb &lt;vfs_readdir+0x6b&gt;:  callq  0xffffffff8163aa50 &lt;mutex_lock_killable&gt;<br><br>crash&gt; dis -lr ffffffff8124a1af | head<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/proc/root.c: 208<br>0xffffffff8124a170 &lt;proc_root_readdir&gt;: nopl   0x0(%rax,%rax,1) [FTRACE NOP]<br>0xffffffff8124a175 &lt;proc_root_readdir+0x5&gt;:     push   %rbp<br>0xffffffff8124a176 &lt;proc_root_readdir+0x6&gt;:     mov    %rsp,%rbp<br>0xffffffff8124a179 &lt;proc_root_readdir+0x9&gt;:     push   %r13<br>0xffffffff8124a17b &lt;proc_root_readdir+0xb&gt;:     mov    %rdx,%r13<br>0xffffffff8124a17e &lt;proc_root_readdir+0xe&gt;:     push   %r12 <br><br>&lt;-- inode<br>0xffffffff8124a180 &lt;proc_root_readdir+0x10&gt;:    mov    %rsi,%r12<br>0xffffffff8124a183 &lt;proc_root_readdir+0x13&gt;:    push   %rbx<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/proc/root.c: 212<br><br><br>#11 [ffff88f7e62ebec8] proc_root_readdir at ffffffff8124a1af<br>    ffff88f7e62ebed0: ffff88d5516c5100 ffff88bf27018040 <br>    <br>======================<br><br>======================<br><br>^<br>    <br>======================<br><br>======================<br><br>+--- inode<br>    ffff88f7e62ebee0: ffff88f7e62ebf38 ffff88f7e62ebf28 <br>    ffff88f7e62ebef0: ffffffff811f2c40 <br>#12 [ffff88f7e62ebef0] vfs_readdir at ffffffff811f2c40<br><br><br>crash&gt; inode.i_mutex 0xffff88bf27018040 -ox<br>struct inode {<br>  [ffff88bf270180e8] struct mutex i_mutex;    &lt;--- same inode and taken here<br>}<br>---------------------------------------------------------------------------------------<br><br>While it had the mutex that was awaiting by many other D state processes, it also wanted to take a spinlock for the global spinlock.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; dis -lr ffffffff811f9769<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/fs/inode.c: 447<br>0xffffffff811f9750 &lt;inode_sb_list_add&gt;: nopl   0x0(%rax,%rax,1) [FTRACE NOP]<br>0xffffffff811f9755 &lt;inode_sb_list_add+0x5&gt;:     push   %rbp<br>0xffffffff811f9756 &lt;inode_sb_list_add+0x6&gt;:     mov    %rsp,%rbp<br>0xffffffff811f9759 &lt;inode_sb_list_add+0x9&gt;:     push   %rbx<br>0xffffffff811f975a &lt;inode_sb_list_add+0xa&gt;:     mov    %rdi,%rbx<br>/usr/src/debug/kernel-3.10.0-327.44.2.el7/linux-3.10.0-327.44.2.el7.x86_64/include/linux/spinlock.h: 293<br>0xffffffff811f975d &lt;inode_sb_list_add+0xd&gt;:     mov    $0xffffffff81943400,%rdi<br>0xffffffff811f9764 &lt;inode_sb_list_add+0x14&gt;:    callq  0xffffffff8163e350 &lt;_raw_spin_lock&gt;<br><br><br> 442 /**<br> 443  * inode_sb_list_add - add inode to the superblock list of inodes<br> 444  * @inode: inode to add<br> 445  */<br> 446 void inode_sb_list_add(struct inode *inode)<br> 447 {<br> 448   spin_lock(&amp;inode_sb_list_lock);<br><br><br>&lt;--- looping here<br> 449   list_add(&amp;inode-&gt;i_sb_list, &amp;inode-&gt;i_sb-&gt;s_inodes);<br> 450   spin_unlock(&amp;inode_sb_list_lock);<br> 451 }<br>---------------------------------------------------------------------------------------<br><br>I'm currently checking who was holding this spinlock and why it's not completed. I'll update once I have further progress.<br><br><br>The spinlock 'inode_sb_list_lock' was held by PID 61776 and actually it's the longest CPU holder also.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; runq -t | grep CPU | sort -k3r | awk 'NR==1{now=strtonum(&quot;0x&quot;$3)}1{printf&quot;%s\t%7.2fs behind\n&quot;,$0,(now-strtonum(&quot;0x&quot;$3))/1000000000}'<br> CPU 7: 302e090665b18a     0.00s behind<br>CPU 66: 302e0906654085     0.00s behind<br>CPU 52: 302e0906654083     0.00s behind<br>CPU 29: 302e0906654073     0.00s behind<br>CPU 67: 302e0906654069     0.00s behind<br>CPU 69: 302e0906654047     0.00s behind<br>CPU 37: 302e0906653ffd     0.00s behind<br>CPU 38: 302e0906653ffd     0.00s behind<br>CPU 28: 302e0906653fed     0.00s behind<br> CPU 5: 302e0906653fec     0.00s behind<br>CPU 36: 302e0906653fe9     0.00s behind<br>CPU 79: 302e0906481358     0.00s behind<br>CPU 76: 302e0906480d2a     0.00s behind<br>CPU 68: 302e090648058f     0.00s behind<br>CPU 60: 302e090647edbb     0.00s behind<br>CPU 59: 302e090647da86     0.00s behind<br>CPU 58: 302e090647c27d     0.00s behind<br>CPU 53: 302e090647bcd6     0.00s behind<br>CPU 49: 302e090647b88d     0.00s behind<br>CPU 48: 302e090647b323     0.00s behind<br>CPU 47: 302e090647ad2e     0.00s behind<br>CPU 46: 302e090647a759     0.00s behind<br>CPU 44: 302e09064798ee     0.00s behind<br>CPU 43: 302e09064793f9     0.00s behind<br>CPU 41: 302e090647855d     0.00s behind<br>CPU 40: 302e090647727a     0.00s behind<br>CPU 31: 302e0906475c68     0.00s behind<br>CPU 16: 302e0906475694     0.00s behind<br>CPU 15: 302e0906475135     0.00s behind<br>CPU 14: 302e0906474bb5     0.00s behind<br>CPU 13: 302e0906474545     0.00s behind<br>CPU 11: 302e0906471ff2     0.00s behind<br> CPU 9: 302e09064719b3     0.00s behind<br> CPU 8: 302e09064714b9     0.00s behind<br> CPU 6: 302e0906470fbe     0.00s behind<br> CPU 3: 302e09064708c3     0.00s behind<br> CPU 2: 302e090647035f     0.00s behind<br>CPU 73: 302e090619d1e0     0.00s behind<br>CPU 78: 302e0905f03926     0.01s behind<br>CPU 77: 302e0905effc21     0.01s behind<br>CPU 74: 302e0905efd71a     0.01s behind<br>CPU 65: 302e0905ef6976     0.01s behind<br>CPU 64: 302e0905ef3dc4     0.01s behind<br>CPU 63: 302e0905eded3c     0.01s behind<br>CPU 56: 302e0905ed7bc0     0.01s behind<br>CPU 55: 302e0905ed7591     0.01s behind<br>CPU 54: 302e0905ed6faa     0.01s behind<br>CPU 50: 302e0905ed5e56     0.01s behind<br>CPU 45: 302e0905ed3b3c     0.01s behind<br>CPU 42: 302e0905ed196e     0.01s behind<br>CPU 21: 302e0905ddd14d     0.01s behind<br>CPU 51: 301e3f5cd9a2d6  17358.81s behind<br>CPU 75: 301b0afee160a4  20882.26s behind<br>CPU 20: 3016697cc5669a  25973.98s behind<br> CPU 4: 301252e38fa97a  30469.08s behind<br>CPU 18: 301252e3834446  30469.08s behind<br>CPU 35: 3012177332f49a  30724.37s behind<br>CPU 70: 3011811b966361  31370.09s behind<br>CPU 30: 30113b2e25136a  31670.42s behind<br>CPU 39: 301134dcb2b2b1  31697.56s behind<br>CPU 19: 3011240099b8d6  31769.97s behind<br>CPU 12: 3011201114d14c  31786.87s behind<br> CPU 1: 3010bd39000db6  32211.41s behind<br>CPU 22: 3010bbec0d46f8  32216.99s behind<br>CPU 23: 3010bb7478b5fb  32219.00s behind<br>CPU 57: 3010bb35852622  32220.05s behind<br>CPU 10: 3010bb1ff8880e  32220.42s behind<br>CPU 24: 3010bafce45857  32221.00s behind<br>CPU 25: 3010bafce455f3  32221.00s behind<br>CPU 62: 3010ba854ff0f7  32223.01s behind<br>CPU 61: 3010ba854ff0bf  32223.01s behind<br>CPU 27: 3010ba854ff08c  32223.01s behind<br>CPU 26: 3010ba854fed83  32223.01s behind<br>CPU 32: 3010ba799d6215  32223.21s behind<br>CPU 72: 3010ba799d5844  32223.21s behind<br>CPU 34: 3010ba799d569e  32223.21s behind<br>CPU 33: 3010ba6010f96e  32223.64s behind<br>CPU 17: 301084213ca2a2  32456.62s behind<br>CPU 71: 30107561c7a811  32519.96s behind<br> CPU 0: 30107532d14f8d  32520.75s behind<br> <br>crash&gt; pd 32520/60<br>$3 = 542<br>crash&gt; pd 32520/60/60<br>$4 = 9<br> ---------------------------------------------------------------------------------------<br><br>This process was doing syncing while holding the issued spin lock.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; bt 61776<br>PID: 61776  TASK: ffff88bf26cd5080  CPU: 0   COMMAND: &quot;xfs_db&quot;<br> #0 [ffff883f7f805cb8] machine_kexec at ffffffff81051e9b<br> #1 [ffff883f7f805d18] crash_kexec at ffffffff810f2a42<br> #2 [ffff883f7f805de8] panic at ffffffff81630267<br> #3 [ffff883f7f805e68] hpwdt_pretimeout at ffffffffa02c98ed [hpwdt]<br> #4 [ffff883f7f805e80] nmi_handle at ffffffff8163fb59<br> #5 [ffff883f7f805ec8] do_nmi at ffffffff8163fcc6<br> #6 [ffff883f7f805ef0] end_repeat_nmi at ffffffff8163ef93<br>    [exception RIP: _raw_spin_lock+0x12]<br>    RIP: ffffffff8163e362  RSP: ffff88bd351a3d70  RFLAGS: 00000286<br>    RAX: 00000000d242d242  RBX: ffff88f9583df8b8  RCX: ffff88bd351a3fd8<br>    RDX: 000000000000d240  RSI: 0000000000000000  RDI: ffff88f9583df940<br>    RBP: ffff88bd351a3d70   R8: ffff88bd351a3cc0   R9: 0000000000000000<br>    R10: 0000000000000100  R11: 0000000000000001  R12: ffff88f9583df940<br>    R13: ffff88ff156b18a0  R14: ffff88e68bacebf8  R15: ffff88f9583dfa08<br>    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018<br>--- &lt;NMI exception stack&gt; ---<br> #7 [ffff88bd351a3d70] _raw_spin_lock at ffffffff8163e362<br> #8 [ffff88bd351a3d78] sync_inodes_sb at ffffffff8120938e<br> #9 [ffff88bd351a3e18] sync_filesystem at ffffffff8121050b<br>#10 [ffff88bd351a3e30] fsync_bdev at ffffffff8121ab94<br>#11 [ffff88bd351a3e50] blkdev_ioctl at ffffffff812d945b<br>#12 [ffff88bd351a3ea8] block_ioctl at ffffffff8121a1f1<br>#13 [ffff88bd351a3eb8] do_vfs_ioctl at ffffffff811f28f5<br>#14 [ffff88bd351a3f30] sys_ioctl at ffffffff811f2b71<br>#15 [ffff88bd351a3f80] system_call_fastpath at ffffffff816470c9<br>    RIP: 00007fa861615317  RSP: 00007ffd0f3de120  RFLAGS: 00010246<br>    RAX: 0000000000000010  RBX: ffffffff816470c9  RCX: 0000000000001000<br>    RDX: 0000000000000000  RSI: 0000000000001261  RDI: 0000000000000004<br>    RBP: 0000000000000004   R8: 00000000023f5f50   R9: 0000000000000001<br>    R10: 00007ffd0f3de660  R11: 0000000000000246  R12: 0000000000000000<br>    R13: 0000000000000000  R14: 0000000000000000  R15: 0000000000000000<br>    ORIG_RAX: 0000000000000010  CS: 0033  SS: 002b<br>    <br>1215 static void wait_sb_inodes(struct super_block *sb)<br>1216 {<br>1217   struct inode *inode, *old_inode = NULL;<br>1218 <br>1219   /*<br>1220    * We need to be protected against the filesystem going from<br>1221    * r/o to r/w or vice versa.<br>1222    */<br>1223   WARN_ON(!rwsem_is_locked(&amp;sb-&gt;s_umount));<br>1224 <br>1225   spin_lock(&amp;inode_sb_list_lock);<br><br><br>&lt;--- It's holding the lock<br>1226 <br>1227   /*<br>1228    * Data integrity sync. Must wait for all pages under writeback,<br>1229    * because there may have been pages dirtied before our sync<br>1230    * call, but which had writeout started before we write it out.<br>1231    * In which case, the inode may not be on the dirty list, but<br>1232    * we still have to wait for that writeout.<br>1233    */<br>1234   list_for_each_entry(inode, &amp;sb-&gt;s_inodes, i_sb_list) {<br>1235     struct address_space *mapping = inode-&gt;i_mapping;<br>1236 <br>1237     spin_lock(&amp;inode-&gt;i_lock);<br>======================<br>&lt;--- Tried to get this lock while holding 'inode_sb_list_lock'<br>---------------------------------------------------------------------------------------<br><br>This process was trying to take the lock for the below inode.<br><br> ---------------------------------------------------------------------------------------<br>crash&gt; inode.i_lock ffff88f9583df8b8<br>  i_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0xd244d242, <br>            tickets = {<br>              head = 0xd242, <br>              tail = 0xd244<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }<br>crash&gt; inode.i_lock ffff88f9583df8b8 -ox<br>struct inode {<br>  [ffff88f9583df940] spinlock_t i_lock;<br>}<br><br>crash&gt; inode.i_op,i_sb ffff88f9583df8b8<br>  i_op = 0xffffffffa080b800 &lt;xfs_dir_inode_operations&gt;<br>  i_sb = 0xffff88ff156b1800<br>crash&gt; mount | grep ffff88ff156b1800<br>ffff883f24f49300 ffff88ff156b1800 xfs    /dev/mapper/LLDECCSAPVG-APP /APP   <br><br><br>crash&gt; inode.i_mapping ffff88f9583df8b8<br>  i_mapping = 0xffff88f9583dfa08<br>crash&gt; address_space 0xffff88f9583dfa08<br>struct address_space {<br>  host = 0xffff88f9583df8b8, <br>  page_tree = {<br>    height = 0x0, <br>    gfp_mask = 0x20, <br>    rnode = 0x0<br>  }, <br>  tree_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0x0, <br>            tickets = {<br>              head = 0x0, <br>              tail = 0x0<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }, <br>  i_mmap_writable = 0x0, <br>  i_mmap = {<br>    rb_node = 0x0<br>  }, <br>  i_mmap_nonlinear = {<br>    next = 0xffff88f9583dfa30, <br>    prev = 0xffff88f9583dfa30<br>  }, <br>  i_mmap_mutex = {<br>    count = {<br>      counter = 0x1<br>    }, <br>    wait_lock = {<br>      {<br>        rlock = {<br>          raw_lock = {<br>            {<br>              head_tail = 0x0, <br>              tickets = {<br>                head = 0x0, <br>                tail = 0x0<br>              }<br>            }<br>          }<br>        }<br>      }<br>    }, <br>    wait_list = {<br>      next = 0xffff88f9583dfa48, <br>      prev = 0xffff88f9583dfa48<br>    }, <br>    owner = 0x0, <br>    {<br>      osq = 0x0, <br>      __UNIQUE_ID_rh_kabi_hide0 = {<br>        spin_mlock = 0x0<br>      }, <br>      {&lt;No data fields&gt;}<br>    }<br>  }, <br>  nrpages = 0x0, <br>  nrshadows = 0x0, <br>  writeback_index = 0x0, <br>  a_ops = 0xffffffffa080b0a0 &lt;xfs_address_space_operations&gt;, <br>  flags = 0x2005a, <br>  backing_dev_info = 0xffff883f21a86b98, <br>  private_lock = {<br>    {<br>      rlock = {<br>        raw_lock = {<br>          {<br>            head_tail = 0x0, <br>            tickets = {<br>              head = 0x0, <br>              tail = 0x0<br>            }<br>          }<br>        }<br>      }<br>    }<br>  }, <br>  private_list = {<br>    next = 0xffff88f9583dfaa0, <br>    prev = 0xffff88f9583dfaa0<br>  }, <br>  private_data = 0x0<br>}<br>======================<br><br>#define hlist_for_each(pos, head) \<br>  for (pos = (head)-&gt;first; pos ; pos = pos-&gt;next)<br>  <br>/**<br> * hlist_for_each_entry - iterate over list of given type<br> * @pos:  the type * to use as a loop cursor.<br> * @head: the head for your list.<br> * @member: the name of the hlist_node within the struct.<br> */<br>#define hlist_for_each_entry(pos, head, member)       \<br>  for (pos = hlist_entry_safe((head)-&gt;first, typeof(*(pos)), member);\<br>       pos;             \<br>       pos = hlist_entry_safe((pos)-&gt;member.next, typeof(*(pos)), member))<br>       <br>  <br>  hlist_for_each_entry(dentry, &amp;inode-&gt;i_dentry, d_alias) {<br>    dget(dentry);<br>    spin_unlock(&amp;inode-&gt;i_lock);<br>    <br><br><br>crash&gt; inode.i_dentry ffff88f9583df8b8<br>    i_dentry = {<br>      first = 0xffff88b2b0902fb0<br>    }<br>crash&gt; hlist_node 0xffff88b2b0902fb0<br>struct hlist_node {<br>  next = 0xffff88f8e95c2cb0, <br>  pprev = 0xffff88f9583df9d0<br>}<br><br>crash&gt; dentry.d_alias -ox<br>struct dentry {<br>  [0xb0] struct hlist_node d_alias;<br>}<br><br>crash&gt; px 0xffff88b2b0902fb0-0xb0<br>$2 = 0xffff88b2b0902f00<br>crash&gt; dentry.d_iname,d_parent 0xffff88b2b0902f00<br>  d_iname = &quot;scripts\000ems\000.d\000t\000f\000quires\000\000es\000\000&quot;<br>  d_parent = 0xffff88ff25fe3200<br>crash&gt; dentry.d_iname,d_parent 0xffff88ff25fe3200<br>  d_iname = &quot;/\000atus\000\000memory650\000\000\000\000\000\000\000\000\000\000\000\000\000\000&quot;<br>  d_parent = 0xffff88ff25fe3200<br>---------------------------------------------------------------------------------------<br><br>This file usage is not showing in any where.<br><br>---------------------------------------------------------------------------------------<br>crash&gt; foreach files | grep scripts<br>crash&gt; foreach bt -f | grep ffff88f9583df8b8<br>    RAX: 00000000d242d242  RBX: ffff88f9583df8b8  RCX: ffff88bd351a3fd8<br>crash&gt; <br>---------------------------------------------------------------------------------------<br><br>From the log, we are seeing there's a list corruption earlier.<br><br>---------------------------------------------------------------------------------------<br>[13438787.851363]  sdw: unknown partition table<br>[13438787.852356]  sdx: unknown partition table<br>[13438787.853381]  sdy: unknown partition table<br>[13438787.854326]  sdz: unknown partition table<br>[13510137.047133] ------------[ cut here ]------------<br>[13510137.047157] WARNING: at lib/list_debug.c:36 __list_add+0x8a/0xc0()<br>[13510137.047162] list_add double add: new=ffff88f9583df9c0, prev=ffff88ff156b18a0, next=ffff88f9583df9c0.<br>[13510137.047165] Modules linked in: rpcsec_gss_krb5 nfsv4 dns_resolver nls_utf8 isofs loop fuse btrfs zlib_deflate raid6_pq xor vfat msdos fat ext4 mbcache jbd2 bridge stp llc ipmi_watchdog ipmi_devintf RedCastle(POE) nfsv3 nfs fscache bonding iptable_filter iTCO_wdt iTCO_vendor_support intel_powerclamp coretemp intel_rapl kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd pcspkr dm_service_time ipmi_ssif sb_edac edac_core sg lpc_ich mfd_core hpilo hpwdt ioatdma shpchp wmi ipmi_si ipmi_msghandler acpi_power_meter dm_multipath dm_mod nfsd auth_rpcgss nfs_acl lockd grace binfmt_misc sunrpc ip_tables xfs libcrc32c crc32c_intel serio_raw ixgbe(OE) vxlan ip6_udp_tunnel udp_tunnel dca mgag200 syscopyarea sysfillrect sysimgblt i2c_algo_bit drm_kms_helper ttm drm i2c_core<br>[13510137.047272]  tg3(OE) ptp pps_core sd_mod crc_t10dif crct10dif_generic crct10dif_pclmul crct10dif_common qla2xxx(OE) scsi_transport_fc scsi_tgt hpsa(OE) scsi_transport_sas [last unloaded: RedCastle]<br>[13510137.047286] CPU: 33 PID: 12160 Comm: mkdir Tainted: P           OE  ------------   3.10.0-327.44.2.el7.x86_64 #1<br>[13510137.047288] Hardware name: HP ProLiant DL580 Gen9/ProLiant DL580 Gen9, BIOS U17 09/12/2016<br>[13510137.047289]  ffff887e2de9bb70 000000002c1beaf2 ffff887e2de9bb28 ffffffff816369d1<br>[13510137.047291]  ffff887e2de9bb60 ffffffff8107b260 ffff88f9583df9c0 ffff88f9583df9c0<br>[13510137.047293]  ffff88ff156b18a0 0000000000000000 ffff883f7f31e600 ffff887e2de9bbc8<br>[13510137.047295] Call Trace:<br>[13510137.047311]  [&lt;ffffffff816369d1&gt;] dump_stack+0x19/0x1b<br>[13510137.047318]  [&lt;ffffffff8107b260&gt;] warn_slowpath_common+0x70/0xb0<br>[13510137.047320]  [&lt;ffffffff8107b2fc&gt;] warn_slowpath_fmt+0x5c/0x80<br>[13510137.047322]  [&lt;ffffffff8130c84a&gt;] __list_add+0x8a/0xc0<br>[13510137.047326]  [&lt;ffffffff811f9787&gt;] inode_sb_list_add+0x37/0x50<br>[13510137.047372]  [&lt;ffffffffa07d9d94&gt;] xfs_setup_inode+0x34/0x2f0 [xfs]<br>[13510137.047385]  [&lt;ffffffffa07dc0ed&gt;] xfs_ialloc+0x2cd/0x540 [xfs]<br>[13510137.047395]  [&lt;ffffffffa07dc3d6&gt;] xfs_dir_ialloc+0x76/0x280 [xfs]<br>[13510137.047409]  [&lt;ffffffffa07ec4cb&gt;] ? xfs_log_reserve+0x15b/0x1b0 [xfs]<br>[13510137.047413]  [&lt;ffffffff8163b312&gt;] ? down_write+0x12/0x30<br>[13510137.047423]  [&lt;ffffffffa07dc8b4&gt;] xfs_create+0x284/0x710 [xfs]<br>[13510137.047427]  [&lt;ffffffff811e9a2d&gt;] ? __lookup_hash+0x2d/0x60<br>[13510137.047437]  [&lt;ffffffffa07d8fdb&gt;] xfs_vn_mknod+0xbb/0x250 [xfs]<br>[13510137.047438]  [&lt;ffffffff811ef60f&gt;] ? getname_flags+0x4f/0x1a0<br>[13510137.047448]  [&lt;ffffffffa07d9186&gt;] xfs_vn_mkdir+0x16/0x20 [xfs]<br>[13510137.047449]  [&lt;ffffffff811ead67&gt;] vfs_mkdir+0xb7/0x160<br>[13510137.047450]  [&lt;ffffffff811f0c6f&gt;] SyS_mkdirat+0x6f/0xe0<br>[13510137.047452]  [&lt;ffffffff811f0cf9&gt;] SyS_mkdir+0x19/0x20<br>[13510137.047461]  [&lt;ffffffffa16343ca&gt;] prst_mkdir+0x13a/0x190 [RedCastle]<br>[13510137.047465]  [&lt;ffffffffa1634458&gt;] rg_mkdir+0x38/0x50 [RedCastle]<br>[13510137.047470]  [&lt;ffffffff816470c9&gt;] system_call_fastpath+0x16/0x1b<br>[13510137.047471] ---[ end trace 9e81945cd74aab88 ]---<br>[13525285.928030]  sdaa: unknown partition table<br>[13525285.932809]  sdab: unknown partition table<br>[13525285.939767]  sdac: unknown partition table<br>[13525285.945864]  sdad: unknown partition table**** Working in progress ****<br>---------------------------------------------------------------------------------------<br><br>This process is already gone and there's no way to check it. But, looking at the backtrace is showing it was doing 'inode_sb_list_add()'.<br><br>It's possible that the issue was happened due to corrupted linked list, but we can't tell what was making this list corruption as it's happened earlier. It can be either XFS bug or RedCastle's modification.<br><br>I leave it as WoCollab and adding sbr-filesystem for any help for a known XFS bug, but RedCastle also has high chance to cause of the issue as we are seeing this is intercepting all those important kernel functions and also seeing some errors earlier.<br><br>---------------------------------------------------------------------------------------<br>[ 4967.213536] RedCastle: module license 'unspecified' taints kernel.<br>[ 4967.213537] Disabling lock debugging due to kernel taint<br>[ 5118.142567] Request for unknown module key 'SGA Solutions Co., Ltd.: RedCastle Kernel signing Key: 46<br>1e91f9f5eef9d678ecc12a7ad047cf32c9a94c' err -11<br>[ 5186.235491] Request for unknown module key 'SGA Solutions Co., Ltd.: RedCastle Kernel signing Key: 46<br>1e91f9f5eef9d678ecc12a7ad047cf32c9a94c' err -11<br>[15520.498197] Request for unknown module key 'SGA Solutions Co., Ltd.: RedCastle Kernel signing Key: 46<br>1e91f9f5eef9d678ecc12a7ad047cf32c9a94c' err -11<br>---------------------------------------------------------------------------------------<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-07-13T05:48:46Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JuklxIAB"><br>======================<br><b>생성계정 : 타임게이트, 삼성생명</b><br><b>생성날짜 : 2017-07-13T01:30:12Z</b><br><b>마지막 답변자 : 타임게이트, 삼성생명</b><br><b>마지막 수정 일자 : 2017-07-13T01:30:12Z</b><br><br>안녕하십니까 타임게이트 오선우입니다<br><br>추가로 sosreport를 dropbox에 업로드하였습니다<br><br>파일명 :  sosreport-DECCAL03SL-201707122155.tar.xz<br><br>감사합니다<br><br><publishedDate>2017-07-13T01:30:12Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000JukOAIAZ"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-07-13T00:38:42Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-07-13T00:38:42Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>현재 커널 전문 엔지니어가 보내주신 자료를 분석 중입니다.<br>분석이 마무리되는데로 답변드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-07-13T00:38:42Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JuVYRIA3"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2017-07-12T04:25:09Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2017-07-12T04:25:08Z</b><br><br>안녕하세요?<br><br>Senior Technical Account Manager 허경입니다.<br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>문의하신 내용에 대해서 확인한 후 업데이트 드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2017-07-12T04:25:08Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JuUxBIAV"><br>======================<br><b>생성계정 : kim, jimin</b><br><b>생성날짜 : 2017-07-12T02:24:17Z</b><br><b>마지막 답변자 : kim, jimin</b><br><b>마지막 수정 일자 : 2017-07-12T02:24:17Z</b><br><br>안녕하세요<br><br>파일업로드 중이고 용량은 5G 정도 입니다.<br><br><publishedDate>2017-07-12T02:24:17Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>