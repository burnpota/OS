======================<br><b>생성계정 : 우택 심</b><br><b>생성날짜 : 2016-05-25T05:03:41Z</b><br><b>마지막 답변자 : Jack, Jong Young Moon</b><br><b>마지막 수정 일자 : 2016-06-08T01:30:16Z</b><br><b>id : 500A000000UDGvSIAX</b><br>======================<br><br><b><font size=15>
제목  : Are there any way to find inodes status on NFS exactly?  [ NFS Mount 볼륨의 inode 관련 문의 ]
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요.<br><br>NFS 마운트 된 디렉토리의 inode 숫자가 모두 동일하게 나오는 현상과 실제 stat 으로 봤을 때 나오는 inode 값의 차이를 문의 드립니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>현재 보험ERP SAP 시스템에서 사용하는 정보는 아래와 같습니다.<br><br>root@PECCAL01SL /tmp # cat 20160525_inode.txt<br>root@PECCAL01SL /root # df -h<br>Filesystem                               Size  Used Avail Use% Mounted on<br>/dev/sda2                                 30G   13G   18G  42% /<br>devtmpfs                                 504G     0  504G   0% /dev<br>tmpfs                                    900G     0  900G   0% /dev/shm<br>tmpfs                                    504G  164M  504G   1% /run<br>tmpfs                                    504G     0  504G   0% /sys/fs/cgroup<br>/dev/sda3                                 20G  2.5G   18G  13% /var<br>/dev/sda1                               1016M  253M  763M  25% /boot<br>/dev/mapper/vg0-home                      10G  193M  9.8G   2% /home<br>/dev/mapper/vg0-sysadmin                  20G  3.7G   17G  19% /sysadmin<br>/dev/mapper/LRECCSAPVG-usr_sap            10G  2.5G  7.6G  25% /usr/sap<br>/dev/mapper/LRECCSAPVG-APP                10G   33M   10G   1% /APP<br>/dev/mapper/LRECCSAPVG-usr_sap_PEC       200G   59G  142G  30% /usr/sap/PEC<br>/dev/mapper/LRECCSAPVG-oracle            2.0G  215M  1.8G  11% /oracle<br>/dev/mapper/vg9-CRASH                    727G  1.5G  726G   1% /CRASH<br>100.254.144.201:/ECC_usr_sap_trans       200G   31G  170G  16% /usr/sap/trans<br>100.254.144.201:/ECC_MIG                1000G  931G   70G  94% /MIG<br>100.254.144.201:/ECC_sapmnt_PEC           50G   38G   13G  76% /sapmnt/PEC<br>100.254.144.201:/ECC_SAPP                 10G  4.7M   10G   1% /SAPP<br>tmpfs                                    101G     0  101G   0% /run/user/20472<br>tmpfs                                    101G     0  101G   0% /run/user/20394<br>tmpfs                                    101G     0  101G   0% /run/user/1231<br>100.254.144.202:/BATCH_bat_sam_SAM_work  5.0T  9.2G  5.0T   1% /batch_sam/SAM/work<br>100.254.144.202:/BATCH_bat_sam_SAM_bak   5.0T     0  5.0T   0% /batch_sam/SAM/backup<br>tmpfs                                    101G     0  101G   0% /run/user/32301<br>tmpfs                                    101G     0  101G   0% /run/user/32247<br>tmpfs                                    101G     0  101G   0% /run/user/0<br>tmpfs                                    101G     0  101G   0% /run/user/32312<br>tmpfs                                    101G     0  101G   0% /run/user/5144<br>tmpfs                                    101G     0  101G   0% /run/user/30307<br><br>NFS Mount 된 볼륨의 사용량이 모두 틀립니다.<br><br>하지만, inode 정보를  df 로 보면 아래와 같이 같은 NFS 서버의 inode 정보가 동일하게 나옵니다.<br><br>root@PECCAL01SL /root # df -i<br>Filesystem                                  Inodes     IUsed      IFree IUse% Mounted  on<br>/dev/sda2                                 31453184    188360   31264824    1% /<br>devtmpfs                                 132024608      1331  132023277    1% /dev<br>tmpfs                                    132028764         1  132028763    1% /dev/shm<br>tmpfs                                    132028764      2012  132026752    1% /run<br>tmpfs                                    132028764        13  132028751    1% /sys/fs/cgroup<br>/dev/sda3                                 20967424     13250   20954174    1% /var<br>/dev/sda1                                  1046528       337    1046191    1% /boot<br>/dev/mapper/vg0-home                      10481664       581   10481083    1% /home<br>/dev/mapper/vg0-sysadmin                  20967424     12947   20954477    1% /sysadmin<br>/dev/mapper/LRECCSAPVG-usr_sap            10485760      3147   10482613    1% /usr/sap<br>/dev/mapper/LRECCSAPVG-APP                10485760         5   10485755    1% /APP<br>/dev/mapper/LRECCSAPVG-usr_sap_PEC       209715200     58966  209656234    1% /usr/sap/PEC<br>/dev/mapper/LRECCSAPVG-oracle              2097152        29    2097123    1% /oracle<br>/dev/mapper/vg9-CRASH                    762257408         4  762257404    1% /CRASH<br>100.254.144.201:/ECC_usr_sap_trans      1441447936 270030271 1171417665   19% /usr/sap/trans<br>100.254.144.201:/ECC_MIG                1441447936 270030271 1171417665   19% /MIG<br>100.254.144.201:/ECC_sapmnt_PEC         1441447936 270030271 1171417665   19% /sapmnt/PEC<br>100.254.144.201:/ECC_SAPP               1441447936 270030271 1171417665   19% /SAPP<br>tmpfs                                    132028764         1  132028763    1% /run/user/20472<br>tmpfs                                    132028764         1  132028763    1% /run/user/20394<br>tmpfs                                    132028764         1  132028763    1% /run/user/1231<br>100.254.144.202:/BATCH_bat_sam_SAM_work 2691923968  11125767 2680798201    1% /batch_sam/SAM/work<br>100.254.144.202:/BATCH_bat_sam_SAM_bak  2691923968  11125767 2680798201    1% /batch_sam/SAM/backup<br>tmpfs                                    132028764         1  132028763    1% /run/user/32301<br>tmpfs                                    132028764         1  132028763    1% /run/user/32247<br>tmpfs                                    132028764         1  132028763    1% /run/user/0<br>tmpfs                                    132028764         1  132028763    1% /run/user/32312<br>tmpfs                                    132028764         1  132028763    1% /run/user/5144<br>tmpfs                                    132028764         1  132028763    1% /run/user/30307<br><br>그리고 NFS 마운트 된 디렉토리의 stat 정보를 조회해 보면 아래와 같습니다.<br><br>root@PECCAL01SL /root # stat /usr/sap/trans<br>  File: '/usr/sap/trans'<br>  Size: 6144            Blocks: 16         IO Block: 65536  directory<br>Device: 27h/39d Inode: 3743274056  Links: 41<br>Access: (0771/drwxrwx--x)  Uid: (20472/  pecadm)   Gid: (20083/  sapsys)<br>Access: 2016-05-24 21:02:26.663559453 +0900<br>Modify: 2016-05-22 16:24:06.427894959 +0900<br>Change: 2016-05-22 16:24:06.427894959 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /MIG<br>  File: '/MIG'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 28h/40d Inode: 3898405231  Links: 11<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 09:02:09.286490967 +0900<br>Modify: 2016-05-25 10:23:45.020135508 +0900<br>Change: 2016-05-25 10:23:45.020135508 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /sapmnt/PEC<br>  File: '/sapmnt/PEC'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 29h/41d Inode: 756907566   Links: 7<br>Access: (0755/drwxr-xr-x)  Uid: (20472/  pecadm)   Gid: (20083/  sapsys)<br>Access: 2016-05-24 21:00:48.265325391 +0900<br>Modify: 2016-04-29 15:10:37.751914591 +0900<br>Change: 2016-04-29 15:10:37.751914591 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /SAPP<br>  File: '/SAPP'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 2ah/42d Inode: 1071488307  Links: 3<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-24 21:02:44.806865038 +0900<br>Modify: 2016-03-04 13:14:23.483845955 +0900<br>Change: 2016-03-29 10:27:13.403417627 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /batch_sam/SAM/work<br>  File: '/batch_sam/SAM/work'<br>  Size: 4096            Blocks: 8          IO Block: 65536  directory<br>Device: 26h/38d Inode: 3582964952  Links: 30<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 10:59:18.637510684 +0900<br>Modify: 2016-05-14 15:00:16.074665152 +0900<br>Change: 2016-05-14 15:00:16.074665152 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /batch_sam/SAM/backup<br>  File: '/batch_sam/SAM/backup'<br>  Size: 4096            Blocks: 8          IO Block: 65536  directory<br>Device: 2bh/43d Inode: 1147019423  Links: 30<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 10:40:42.942779635 +0900<br>Modify: 2016-05-14 15:00:47.137162088 +0900<br>Change: 2016-05-14 15:00:47.137162088 +0900<br> Birth: -<br><br>언제 문제가 발생했습니까? 이러한 문제가 자주 발생합니까? 반복적으로 발생합니까? 특정 시간에 발생합니까?<br><br>이 현상은 NFS 의 최대 inode 를 초과하여 file 생성 등이 더 이상 안되는 증상이 있어 문제를 파악하는 과정에서 발견되었으며,<br><br>정확한 inode 정보를 파악하기 위한 다른 방법이 있다면 첨언 부탁드립니다.<br><br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 4 (Low)</b><br><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><br><comment id="a0aA000000HHgvbIAD"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2016-06-08T01:30:16Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2016-06-08T01:30:16Z</b><br><br>안녕하세요.<br><br>Red Hat Global Support Service를 이용해주셔서 감사합니다.<br><br>먼저, 피드백 감사드리며 문제가 잘 해결 되셨다니 정말 다행입니다. 말씀하신것과 같이 본 케이스는 처리완료하도록 하겠습니다.<br>만약 본 케이스와 관련하여 추가 질문이 있으시다면 언제든지 재 오픈 하실 수 있습니다.<br><br>케이스가 처리 완료되면 고객 설문조사 메일이 발송됩니다. 고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해, 소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-06-08T01:30:16Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HHfa9IAD"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-06-08T01:17:51Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-06-08T01:17:51Z</b><br><br>안녕하세요.<br><br>NFS 제공 서버에서 볼륨 별 inode 수를 늘리는 옵션이 있어 부족문제를 해결했다고 합니다.<br><br>향 후 동일문제 발생 시 다시 케이스를 열도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-06-08T01:17:51Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HAJObIAP"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2016-05-30T00:47:53Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2016-05-30T00:47:53Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>올려주신 문의내용을 기반으로 저희 파일시스템 전문그룹으로 의뢰를 하였는데, 아래와 같은 답변을 받았습니다.<br><br>- 아 래<br><br>'df' 는 statfs() 를 호출해서 이것의 정보를 가져오는데, nfs 의 경우에 이 정보는 nfs 서버에 의해서 제공된 정보이고,<br>서버에 의해서 알맞게 주어진 상태를 보고해야할 것입니다. 게다가, '-f' 플래그로 'stat' 를 호출하는것은 path 자체를<br>위한 상태보다 주어진 path 를 위한 파일시스템 정보를 표시할것입니다. 하지만, 'stat -f /path' 는 또한 statfs()<br>로부터 리턴된 정보를 사용해서 다르지 않을것입니다. 'stat' 는 'block size' 와 관련된 block 정보를 보고하는데<br>mount 된것과는 아래와 같이 다를수도 있습니다.<br><br><br># df /mnt/vm1<br>Filesystem           1K-blocks     Used Available Use% Mounted on<br>vm1:/exports/share1  77276192 63412960   9931168  87% /mnt/vm1<br><br># df -i /mnt/vm1<br>Filesystem            Inodes  IUsed   IFree IUse% Mounted on<br>vm1:/exports/share1 4915200 793321 4121879   17% /mnt/vm1<br><br># stat -f /mnt/vm1<br>  File: &quot;/mnt/vm1&quot;<br>    ID: 0        Namelen: 255     Type: nfs<br>Block size: 32768      Fundamental block size: 32768<br>Blocks: Total: 2414881    Free: 433226     Available: 310349<br>Inodes: Total: 4915200    Free: 4121879<br><br><br>그래서, 지금의 상황에서는 현재의 현상에 대한 설명이 어렵기 때문에 NFS 서버와 클라이언트의 sosreport 를 올려주시길 바랍니다.<br><br><br>감사합니다.<br><br><publishedDate>2016-05-30T00:47:53Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000H9mqPIAR"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2016-05-26T04:32:11Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2016-05-26T04:32:11Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>현재 올려주신 문의내용을 자세히 살펴보고 있는중입니다. 확인이 되는대로 업데이트를 남기도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2016-05-26T04:32:11Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br><br>NFS 마운트 된 디렉토리의 inode 숫자가 모두 동일하게 나오는 현상과 실제 stat 으로 봤을 때 나오는 inode 값의 차이를 문의 드립니다.</issue><environment>현재 보험ERP SAP 시스템에서 사용하는 정보는 아래와 같습니다.<br><br>root@PECCAL01SL /tmp # cat 20160525_inode.txt<br>root@PECCAL01SL /root # df -h<br>Filesystem                               Size  Used Avail Use% Mounted on<br>/dev/sda2                                 30G   13G   18G  42% /<br>devtmpfs                                 504G     0  504G   0% /dev<br>tmpfs                                    900G     0  900G   0% /dev/shm<br>tmpfs                                    504G  164M  504G   1% /run<br>tmpfs                                    504G     0  504G   0% /sys/fs/cgroup<br>/dev/sda3                                 20G  2.5G   18G  13% /var<br>/dev/sda1                               1016M  253M  763M  25% /boot<br>/dev/mapper/vg0-home                      10G  193M  9.8G   2% /home<br>/dev/mapper/vg0-sysadmin                  20G  3.7G   17G  19% /sysadmin<br>/dev/mapper/LRECCSAPVG-usr_sap            10G  2.5G  7.6G  25% /usr/sap<br>/dev/mapper/LRECCSAPVG-APP                10G   33M   10G   1% /APP<br>/dev/mapper/LRECCSAPVG-usr_sap_PEC       200G   59G  142G  30% /usr/sap/PEC<br>/dev/mapper/LRECCSAPVG-oracle            2.0G  215M  1.8G  11% /oracle<br>/dev/mapper/vg9-CRASH                    727G  1.5G  726G   1% /CRASH<br>100.254.144.201:/ECC_usr_sap_trans       200G   31G  170G  16% /usr/sap/trans<br>100.254.144.201:/ECC_MIG                1000G  931G   70G  94% /MIG<br>100.254.144.201:/ECC_sapmnt_PEC           50G   38G   13G  76% /sapmnt/PEC<br>100.254.144.201:/ECC_SAPP                 10G  4.7M   10G   1% /SAPP<br>tmpfs                                    101G     0  101G   0% /run/user/20472<br>tmpfs                                    101G     0  101G   0% /run/user/20394<br>tmpfs                                    101G     0  101G   0% /run/user/1231<br>100.254.144.202:/BATCH_bat_sam_SAM_work  5.0T  9.2G  5.0T   1% /batch_sam/SAM/work<br>100.254.144.202:/BATCH_bat_sam_SAM_bak   5.0T     0  5.0T   0% /batch_sam/SAM/backup<br>tmpfs                                    101G     0  101G   0% /run/user/32301<br>tmpfs                                    101G     0  101G   0% /run/user/32247<br>tmpfs                                    101G     0  101G   0% /run/user/0<br>tmpfs                                    101G     0  101G   0% /run/user/32312<br>tmpfs                                    101G     0  101G   0% /run/user/5144<br>tmpfs                                    101G     0  101G   0% /run/user/30307<br><br>NFS Mount 된 볼륨의 사용량이 모두 틀립니다.<br><br>하지만, inode 정보를  df 로 보면 아래와 같이 같은 NFS 서버의 inode 정보가 동일하게 나옵니다.<br><br>root@PECCAL01SL /root # df -i<br>Filesystem                                  Inodes     IUsed      IFree IUse% Mounted  on<br>/dev/sda2                                 31453184    188360   31264824    1% /<br>devtmpfs                                 132024608      1331  132023277    1% /dev<br>tmpfs                                    132028764         1  132028763    1% /dev/shm<br>tmpfs                                    132028764      2012  132026752    1% /run<br>tmpfs                                    132028764        13  132028751    1% /sys/fs/cgroup<br>/dev/sda3                                 20967424     13250   20954174    1% /var<br>/dev/sda1                                  1046528       337    1046191    1% /boot<br>/dev/mapper/vg0-home                      10481664       581   10481083    1% /home<br>/dev/mapper/vg0-sysadmin                  20967424     12947   20954477    1% /sysadmin<br>/dev/mapper/LRECCSAPVG-usr_sap            10485760      3147   10482613    1% /usr/sap<br>/dev/mapper/LRECCSAPVG-APP                10485760         5   10485755    1% /APP<br>/dev/mapper/LRECCSAPVG-usr_sap_PEC       209715200     58966  209656234    1% /usr/sap/PEC<br>/dev/mapper/LRECCSAPVG-oracle              2097152        29    2097123    1% /oracle<br>/dev/mapper/vg9-CRASH                    762257408         4  762257404    1% /CRASH<br>100.254.144.201:/ECC_usr_sap_trans      1441447936 270030271 1171417665   19% /usr/sap/trans<br>100.254.144.201:/ECC_MIG                1441447936 270030271 1171417665   19% /MIG<br>100.254.144.201:/ECC_sapmnt_PEC         1441447936 270030271 1171417665   19% /sapmnt/PEC<br>100.254.144.201:/ECC_SAPP               1441447936 270030271 1171417665   19% /SAPP<br>tmpfs                                    132028764         1  132028763    1% /run/user/20472<br>tmpfs                                    132028764         1  132028763    1% /run/user/20394<br>tmpfs                                    132028764         1  132028763    1% /run/user/1231<br>100.254.144.202:/BATCH_bat_sam_SAM_work 2691923968  11125767 2680798201    1% /batch_sam/SAM/work<br>100.254.144.202:/BATCH_bat_sam_SAM_bak  2691923968  11125767 2680798201    1% /batch_sam/SAM/backup<br>tmpfs                                    132028764         1  132028763    1% /run/user/32301<br>tmpfs                                    132028764         1  132028763    1% /run/user/32247<br>tmpfs                                    132028764         1  132028763    1% /run/user/0<br>tmpfs                                    132028764         1  132028763    1% /run/user/32312<br>tmpfs                                    132028764         1  132028763    1% /run/user/5144<br>tmpfs                                    132028764         1  132028763    1% /run/user/30307<br><br>그리고 NFS 마운트 된 디렉토리의 stat 정보를 조회해 보면 아래와 같습니다.<br><br>root@PECCAL01SL /root # stat /usr/sap/trans<br>  File: '/usr/sap/trans'<br>  Size: 6144            Blocks: 16         IO Block: 65536  directory<br>Device: 27h/39d Inode: 3743274056  Links: 41<br>Access: (0771/drwxrwx--x)  Uid: (20472/  pecadm)   Gid: (20083/  sapsys)<br>Access: 2016-05-24 21:02:26.663559453 +0900<br>Modify: 2016-05-22 16:24:06.427894959 +0900<br>Change: 2016-05-22 16:24:06.427894959 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /MIG<br>  File: '/MIG'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 28h/40d Inode: 3898405231  Links: 11<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 09:02:09.286490967 +0900<br>Modify: 2016-05-25 10:23:45.020135508 +0900<br>Change: 2016-05-25 10:23:45.020135508 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /sapmnt/PEC<br>  File: '/sapmnt/PEC'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 29h/41d Inode: 756907566   Links: 7<br>Access: (0755/drwxr-xr-x)  Uid: (20472/  pecadm)   Gid: (20083/  sapsys)<br>Access: 2016-05-24 21:00:48.265325391 +0900<br>Modify: 2016-04-29 15:10:37.751914591 +0900<br>Change: 2016-04-29 15:10:37.751914591 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /SAPP<br>  File: '/SAPP'<br>  Size: 2048            Blocks: 8          IO Block: 65536  directory<br>Device: 2ah/42d Inode: 1071488307  Links: 3<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-24 21:02:44.806865038 +0900<br>Modify: 2016-03-04 13:14:23.483845955 +0900<br>Change: 2016-03-29 10:27:13.403417627 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /batch_sam/SAM/work<br>  File: '/batch_sam/SAM/work'<br>  Size: 4096            Blocks: 8          IO Block: 65536  directory<br>Device: 26h/38d Inode: 3582964952  Links: 30<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 10:59:18.637510684 +0900<br>Modify: 2016-05-14 15:00:16.074665152 +0900<br>Change: 2016-05-14 15:00:16.074665152 +0900<br> Birth: -<br><br>root@PECCAL01SL /root # stat /batch_sam/SAM/backup<br>  File: '/batch_sam/SAM/backup'<br>  Size: 4096            Blocks: 8          IO Block: 65536  directory<br>Device: 2bh/43d Inode: 1147019423  Links: 30<br>Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)<br>Access: 2016-05-25 10:40:42.942779635 +0900<br>Modify: 2016-05-14 15:00:47.137162088 +0900<br>Change: 2016-05-14 15:00:47.137162088 +0900<br> Birth: -</environment><periodicityOfIssue>이 현상은 NFS 의 최대 inode 를 초과하여 file 생성 등이 더 이상 안되는 증상이 있어 문제를 파악하는 과정에서 발견되었으며,<br><br>정확한 inode 정보를 파악하기 위한 다른 방법이 있다면 첨언 부탁드립니다.<br><br>감사합니다.</periodicityOfIssue><cep>false</cep></case>