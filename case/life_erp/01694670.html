======================<br><b>생성계정 : 우택 심</b><br><b>생성날짜 : 2016-08-31T10:48:21Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2016-09-18T10:03:27Z</b><br><b>id : 500A000000V7fxmIAB</b><br>======================<br><br><b><font size=15>
제목  : cluster (HA-LVM) Fail-over 시 Resource umount 실패로 인한 문제 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>안녕하세요.<br><br>일단 sosreport 는 반출 과정을 거쳐야 하기에 dropbox.redhat.com 업로드 후 따로 코멘트 남기겠습니다.<br><br>A Node 의 /APP/controlm/ctmag1 와 B Node 의 /APP/controlm/ctmag2 가 주요 HA-LVM 리소스 입니다.<br><br>Panic 이 일어나거나 SAN Cable 절체등의 문제가 있을 때는 정상적으로 kdump_fence, ipmi_fence 가 작동하여 문제는 없습니다.<br><br>단, VIP 가 올라가 있는 Service Network 를 모두 절체 했을 때 pingd 가 인식하여 Resource 를 Fail-over 하는 과정에서 문제가 있습니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>우선 간략히 가용성 테스트 절차를 설명 드리자면 아래와 같습니다.<br><br>1. A Node 의 Service Network (Bonding) 모두 절체<br><br>2. 잠시 후 Cluster Pingd 가 장애 감지<br><br>3. script stop -&gt; vip stop -&gt; LVM-LV (mount) stop -&gt; VG Inactive 단계로 정지하여, B Node 로 Fail-over<br><br>원래는 위와 같은 시나리오로 대략 40초 내외로 문제 없이 전환 됩니다.<br><br>하지만, Service Network 를 절체 할 시 NFS 서비스가 문제가 되어 df 등의 명령어를 입력하더라도 아무런 반응이 없습니다. (Hang 비슷)<br><br>이때 LVM-LV 가 Mount 되어 있는 /APP/controlm/ctmag1 디렉토리에 user 또는 아직 stop 안된 Process 등이 남아 있을 경우 umount 실패가 되는 문제가 발생합니다.<br><br>umount 실패가 되더라도 Filesystem on-fail=fence 로 설정하여 4분 정도 후에 IPMI Fence 작동 후 Fail-over 가 되기는 합니다.<br><br>언제 문제가 발생했습니까? 이러한 문제가 자주 발생합니까? 반복적으로 발생합니까? 특정 시간에 발생합니까?<br><br>질문 드리고 싶은 사항은 아래와 같습니다.<br><br>1. NFS Network 문제등으로 Mount 에 문제가 있을 시 fuser 명령어도 원래 작동에 문제가 있는지 궁금합니다.<br><br>Filesystem resource 에 force_unmount=true 옵션을 넣어봤는데요, 이 옵션이 하는 역할 중 fuser -m 을 실행하던데, NFS Mount 문제시 fuser 실행하고 그대로 멈춰있는 현상을 발견하였습니다.<br><br>이 문제를 재현해 보기위해 수동으로 NFS 에 문제를 일으켜 fuser -cuk 를 실행 해 봤으나 동일하게 문제가 발생하였습니다.<br><br>2. pingd 설정 시 사용하는 dampen, multiplier, monitor interval 의 역할을 알고 싶습니다.<br><br>현재 설정은 아래와 같이 되어 있습니다.<br><br> Clone: ping-clone<br>  Resource: ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: dampen=5s multiplier=10 host_list=100.254.141.1 <br>   Operations: start interval=0s timeout=60 (ping-start-interval-0s)<br>               stop interval=0s timeout=20 (ping-stop-interval-0s)<br>               monitor interval=5s (ping-monitor-interval-5s)<br><br>이렇게 설정되어 있을 경우 Service VIP Network 를 모두 절체 했을 때 얼마만에 장애를 인지하고 Fail-over 시도를 시작하는지 알고 싶습니다.<br><br>3. 위와 같이 NFS 에 문제가 발생 시 강제로 NFS 관련 프로세스 등을 죽이고 fuser 를 활성화 할 수 있는 방법이 있는지 확인 부탁드립니다.<br><br>문제 해결 기간 및  긴급도와 관련된 정보를 제공해 주시겠습니까?<br><br>현재 설정되어 있는 pcs config 내용과 df 명령어 결과를 보내드립니다.<br><br>root@PCTAAL02SL / # pcs resource show --full<br> Group: PCTA1_group<br>  Resource: VG_SRPCTAAP1VG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPCTAAP1VG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPCTAAP1VG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPCTAAP1VG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPCTAAP1VG-start-interval-0s)<br>  Resource: LV_APP_con_ctmag1 (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 directory=/APP/controlm/ctmag1 fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=120 (LV_APP_con_ctmag1-stop-interval-0s)<br>               start interval=0s timeout=120 (LV_APP_con_ctmag1-start-interval-0s)<br>               monitor interval=5s timeout=10 on-fail=fence OCF_CHECK_LEVEL=10 (LV_APP_con_ctmag1-monitor-interval-5s)<br>  Resource: PCTA1_vip (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=100.254.142.53<br>   Operations: stop interval=0s timeout=20s (PCTA1_vip-stop-interval-0s)<br>               start interval=0s start-delay=3s (PCTA1_vip-start-interval-0s)<br>               monitor interval=5s start-delay=5s timeout=10s (PCTA1_vip-monitor-interval-5s)<br>  Resource: PCTA1_script (class=ocf provider=status type=MySAP)<br>   Attributes: monitor=/etc/corosync/ctmag1_monitor start=/etc/corosync/ctmag1_start stop=/etc/corosync/ctmag1_stop<br>   Operations: stop interval=0s timeout=120 (PCTA1_script-stop-interval-0s)<br>               monitor interval=60s enabled=false (PCTA1_script-monitor-interval-60s)<br>               start interval=0s timeout=120 (PCTA1_script-start-interval-0s)<br> Group: PCTA2_group<br>  Resource: VG_SRPCTAAP2VG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPCTAAP2VG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPCTAAP2VG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPCTAAP2VG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPCTAAP2VG-start-interval-0s)<br>  Resource: LV_APP_con_ctmag2 (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2 directory=/APP/controlm/ctmag2 fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=120 (LV_APP_con_ctmag2-stop-interval-0s)<br>               start interval=0s timeout=120 (LV_APP_con_ctmag2-start-interval-0s)<br>               monitor interval=5s timeout=10 on-fail=fence OCF_CHECK_LEVEL=10 (LV_APP_con_ctmag2-monitor-interval-5s)<br>  Resource: PCTA2_vip (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=100.254.142.54<br>   Operations: stop interval=0s timeout=20s (PCTA2_vip-stop-interval-0s)<br>               start interval=0s start-delay=3s (PCTA2_vip-start-interval-0s)<br>               monitor interval=5s start-delay=5s timeout=10s (PCTA2_vip-monitor-interval-5s)<br>  Resource: PCTA2_script (class=ocf provider=status type=MySAP)<br>   Attributes: monitor=/etc/corosync/ctmag2_monitor start=/etc/corosync/ctmag2_start stop=/etc/corosync/ctmag2_stop<br>   Operations: stop interval=0s timeout=120 (PCTA2_script-stop-interval-0s)<br>               monitor interval=60s enabled=false (PCTA2_script-monitor-interval-60s)<br>               start interval=0s timeout=120 (PCTA2_script-start-interval-0s)<br> Clone: ping-clone<br>  Resource: ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: dampen=5s multiplier=10 host_list=100.254.142.1<br>   Operations: start interval=0s timeout=60 (ping-start-interval-0s)<br>               stop interval=0s timeout=20 (ping-stop-interval-0s)<br>               monitor interval=5s (ping-monitor-interval-5s)<br>======================<br>root@PCTAAL02SL / # df -Ph<br>Filesystem                               Size  Used Avail Use% Mounted on<br>/dev/sda2                                 30G  9.3G   21G  31% /<br>devtmpfs                                  63G     0   63G   0% /dev<br>tmpfs                                     63G   54M   63G   1% /dev/shm<br>tmpfs                                     63G   18M   63G   1% /run<br>tmpfs                                     63G     0   63G   0% /sys/fs/cgroup<br>/dev/sda1                               1016M  210M  806M  21% /boot<br>/dev/sda3                                 20G  1.6G   19G   8% /var<br>/dev/mapper/vg9-CRASH                    129G   33M  129G   1% /CRASH<br>/dev/mapper/vg0-home                      10G  151M  9.9G   2% /home<br>/dev/mapper/vg0-sysadmin                  20G   15G  5.2G  75% /sysadmin<br>/dev/mapper/LRPCTAAPPVG-APP               10G   33M   10G   1% /APP<br>100.254.144.202:/BATCH_bat_sam_SAM_bak   5.0T  167G  4.9T   4% /batch_sam/SAM/backup<br>100.254.144.202:/BATCH_bat_sam_SAM_work  5.0T   88G  5.0T   2% /batch_sam/SAM/work<br>tmpfs                                     13G     0   13G   0% /run/user/30307<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   100G  3.7G   97G   4% /APP/controlm/ctmag2<br>tmpfs                                     13G     0   13G   0% /run/user/1322<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 5251314</b><br><b>심각도  : 4 (Low)</b><br>======================<br><comment id="a0aA000000HmAgMIAV"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-09-02T08:43:19Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-09-02T08:43:19Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>보내 주신 sosreport를 잘 받아왔습니다. 그리고 sosreport 메시지 로그를 통해 어떤 이슈가 발생 하였는지를 <br>잘 이해가 되네요. <br><br>먼저 로그 내용을 보시면, <br><br>1. 30일 17시 31분 19초에 서비스 네트워크를 절체 시킨 것으로 보입니다. <br><br>Aug 30 17:31:19 PCTAAL02SL kernel: bond0: link status definitely down for interface ens1f0, disabling it<br>Aug 30 17:31:19 PCTAAL02SL kernel: bond0: making interface ens4f1 the new active one<br>Aug 30 17:31:32 PCTAAL02SL kernel: bond0: link status definitely down for interface ens4f1, disabling it<br>Aug 30 17:31:32 PCTAAL02SL kernel: bond0: now running without any active interface!<br><br><br>2. 그리고 pacemaker가 resource 가 장애 난 것을 인식 하고 17시 32분 01에 filesystem stop 시키는 것으로 보입니다. <br><br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag2)[53484]: INFO: Running stop for /dev/mapper/SRPCTAAP2VG-APP_con_ctmag2 on /APP/controlm/ctmag2<br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag1)[53483]: INFO: Running stop for /dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 on /APP/controlm/ctmag1<br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag2)[53484]: INFO: Trying to unmount /APP/controlm/ctmag2<br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag1)[53483]: INFO: Trying to unmount /APP/controlm/ctmag1<br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag1)[53483]: ERROR: Couldn't unmount /APP/controlm/ctmag1; trying cleanup with TERM<br>Aug 30 17:32:01 PCTAAL02SL Filesystem(LV_APP_con_ctmag2)[53484]: ERROR: Couldn't unmount /APP/controlm/ctmag2; trying cleanup with TERM<br><br><br>3. 다만, filesystem unmount 할시 fuser로 해당 파일 시스템을 사용 하는 프로세스를 확인 하는 도중에 nfs hang <br>이슈로 영향을 받고 fuser 가 멈춘 것으로 생각 됩니다. 그리고 약 60초후 stop 옵션이 timeout 되어 stop 작업이 <br>error 가 return 되었습니다. <br><br><br>Aug 30 17:33:01 PCTAAL01SL crmd[4655]: warning: Action 19 (LV_APP_con_ctmag1_stop_0) on PCTAAL02SL-HB failed (target: 0 vs. rc: 1): Error<br>Aug 30 17:33:01 PCTAAL01SL crmd[4655]: warning: Action 19 (LV_APP_con_ctmag1_stop_0) on PCTAAL02SL-HB failed (target: 0 vs. rc: 1): Error<br>Aug 30 17:33:01 PCTAAL01SL crmd[4655]: warning: Action 34 (LV_APP_con_ctmag2_stop_0) on PCTAAL02SL-HB failed (target: 0 vs. rc: 1): Error<br>Aug 30 17:33:01 PCTAAL01SL crmd[4655]: warning: Action 34 (LV_APP_con_ctmag2_stop_0) on PCTAAL02SL-HB failed (target: 0 vs. rc: 1): Error<br><br><br>4. 그리고 stop on-fail=fence 설정으로 2번 노드가 fencing 되면서 리소스가 failover 되는 것으로 보입니다. <br><br>Aug 30 17:33:01 PCTAAL01SL crmd[4655]:  notice: Executing reboot fencing operation (52) on PCTAAL02SL-HB (timeout=60000)<br>Aug 30 17:33:01 PCTAAL01SL stonith-ng[4651]:  notice: Client crmd.4655.f8a34590 wants to fence (reboot) 'PCTAAL02SL-HB' with device '(any)'<br>Aug 30 17:33:01 PCTAAL01SL stonith-ng[4651]:  notice: Initiating remote operation reboot for PCTAAL02SL-HB: e9710a4e-e9c3-44e1-9193-0dfc5975749c (0)<br>Aug 30 17:33:01 PCTAAL01SL stonith-ng[4651]:  notice: kdump_stonith can fence (reboot) PCTAAL02SL-HB: static-list<br>Aug 30 17:33:01 PCTAAL01SL stonith-ng[4651]:  notice: ipmilan_stonith2 can fence (reboot) PCTAAL02SL-HB: static-list<br>Aug 30 17:33:01 PCTAAL01SL fence_kdump[96862]: waiting for message from '172.18.21.152'<br>Aug 30 17:35:01 PCTAAL01SL stonith-ng[4651]:   error: Operation 'reboot' [96862] (call 3 from crmd.4655) for host 'PCTAAL02SL-HB' with device 'kdump_stonith' returned: -62 (Timer expired)<br><br><br>Aug 30 17:35:02 PCTAAL02SL systemd-logind: Power key pressed.  &lt;=== fence_ipmilan 을 통해 fence 된 시점 <br>Aug 30 17:35:02 PCTAAL02SL systemd-logind: Powering Off...<br><br>즉 Filesystem 리소스를 stop 시작 할때 부터 노드가 실제로 fencing 될때 까지 약 3분 걸렸습니다. <br>======================<br><br>이와 같은 이슈는 pacemaker 에서 fuser 를 이용 하여 상태 체크를 하고 있으니 nfs hang이 걸릴 경우에 fuser 명령어 <br>행으로 인하여 발생 된 것으로 보입니다. 그리고 nfs hang으로 되어 있을 경우에 다른 해결책이 없어 보입니다. <br><br>이와 같은 이슈는 NFS 서버와의 연결은 service newtwork을 사용 하기 때문에 발생 한 것으로 보이며 가능 하시면 <br>NFS 네트워크는 다른 대역으로 이관 하시는게 더 좋을 것 같습니다. <br><br>만약 NFS 네트워크를 이관 하는 것이 어려울 경우에는 service 더 빨리 failover 시키기 위해 timeout 시간을 <br>줄여 보세요.<br><br> stop timeout<br> fence_kdump timeout<br><br>다만, timeout 시간을 줄이시면 리소스 stop 또한 vmcore 수집 하는 것을 인식 하는데에 충분한 시간을 가질수 없어 <br>예상치 않는 이슈가 발생 할수 있다는 점을 참고 하시기 바랍니다. <br><br><br>만약 본 이슈와 관련 하여 추가적인 문의 사항이 있으실 경우 다시 연락 주시기 바랍니다. <br>======================<br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-09-02T08:43:19Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HlpWFIAZ"><br>======================<br><b>생성계정 : 심, 우택</b><br><b>생성날짜 : 2016-09-01T12:15:25Z</b><br><b>마지막 답변자 : 심, 우택</b><br><b>마지막 수정 일자 : 2016-09-01T12:15:25Z</b><br><br>안녕하세요.<br><br>dropbox.redhat.com 에 아래와 같은 파일명으로 sosreport 를 업로드 하였습니다.<br><br>sosreport-PCTAAL01SL-20160831184526.tar.xz<br>sosreport-PCTAAL02SL-20160831184545.tar.xz<br><br>답변 주신 내용 감사합니다.<br><br>일단 현재 구성 상태에서 NFS hard 옵션은 사용해야 만 하는 상황이며,<br><br>궁금한 것은 ocf Filesystem Resource 에 force_unmount=true 를 적용 했을 때,<br><br>umount 시 fuser -m 으로 process kill 하는 루틴이 있는 것으로 확인 하였습니다.<br><br>하지만, 현재 가용성 테스트 시나리오 처럼 Service Network 이 절체 되었을 때, NFS Hang 이 걸리는데,<br><br>이 때, fuser 로 NFS Mount 디렉토리가 아닌, Resource Mount 디렉토리에도 fuser 명령이 hang 이 걸린다는 것 입니다.<br><br>(Cluster 의 Resource 는 NFS 가 아닙니다. NFS 는 단지 항상 마운트 되어 있는 볼륨입니다.)<br><br>결론적으로 이러한 NFS Hang 상태에서 fuser 가 정상적으로 작동될 수 있는 방법이 있는지 궁금합니다.<br><br>감사합니다.<br><br><publishedDate>2016-09-01T12:15:25Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HlllcIAB"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-09-01T06:09:29Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-09-01T06:09:29Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br>======================<br>1. ping에서 사용 되는 옵션 dampen,multiplier 또한 monitor interval 의 역할에 대해 <br>아래와 같이 안내 해 드립니다. <br><br>ping 리소스는 host_list 에 지정 한 machine와의 통신 가능 여부를 확인 하는 역할을 하고 <br>있습니다. 통신 가능 여부는 ping 명령어를 통하여 진행 하고 있습니다. <br><br>monitor interval는 host_list에 지정 한 machine와의 연결 상태를 체크 하는 간갹입니다. <br><br>ping 리소스는 일반적으로 location 제약 조건과 같이 사용 하게 되며 매번 연결 상태를 체크 <br>한후 score를 계산 하고 있습니다. multiplier는 score 를 계산 할시 사용 되는 성수이며<br>디폴트로 하나의 호스트가 성공 되면 1점 가질수 있습니다. 만약 host_list에 3개의 호스트를 <br>지정 하시면 모두 연결 상태 체크 성공이되면 3점을 가질수 있습니다. 말씀 하신 대로 multiplier를 <br>10으로 설정 하시면 매개 호스트와의 연결 상태 체크가 성공이 되면 10점을 얻을수 있습니다. <br><br>그리고 dampen는 매번 score를 업데이트 하기전 대기하는 시간입니다. <br><br>구체적인 내용은 아래와 같은 링크에서 확인 가능 하십니다. <br><br>참고 자료:<br>- https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Configuring_the_Red_Hat_High_Availability_Add-On_with_Pacemaker/s1-moving_resources_due_to_connectivity_changes-HAAR.html<br><br>ping 리소스를 사용 하시면 아래와 같이 location 제약 조건도 같이 설정 하셔야 의미 있을 것 <br>같습니다. <br><br># pcs constraint location &lt;resource-id&gt; rule score=-INFINITY pingd lt 1 or not_defined pingd<br><br><br>2. NFS 관련 문의 <br><br>100.254.144.202:/BATCH_bat_sam_SAM_bak   5.0T  167G  4.9T   4% /batch_sam/SAM/backup<br>100.254.144.202:/BATCH_bat_sam_SAM_work  5.0T   88G  5.0T   2% /batch_sam/SAM/work<br><br>현재 클러스터 노드가 NFS 클라이언트로 사용 하고 있는 것으로 보이지만 클러스터 서비스와 NFS 사용 하는 <br>것과 어떤 연관성이 있는지 확인 부탁드립니다. <br><br>또한, NFS 파일 시스템은 디폴트로 hard 옵션으로 마운트 되어 있으며 만약 서버와 연결이 되지 않으면 <br>행이 발생 하게 되고 언마운트 되지 않습니다. 만약 서버랑 연결이 되지 않을 경우 nfs 언마운트 시키시려면 <br>soft 방식으로 마운트 해면 되는데 soft 방식으로 마운트 하시면 데이타가 손상 될수 있습니다. <br><br># mount -o soft 100.254.144.202:/BATCH_bat_sam_SAM_work /batch_sam/SAM/work<br>======================<br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-09-01T06:09:29Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br><br>일단 sosreport 는 반출 과정을 거쳐야 하기에 dropbox.redhat.com 업로드 후 따로 코멘트 남기겠습니다.<br><br>A Node 의 /APP/controlm/ctmag1 와 B Node 의 /APP/controlm/ctmag2 가 주요 HA-LVM 리소스 입니다.<br><br>Panic 이 일어나거나 SAN Cable 절체등의 문제가 있을 때는 정상적으로 kdump_fence, ipmi_fence 가 작동하여 문제는 없습니다.<br><br>단, VIP 가 올라가 있는 Service Network 를 모두 절체 했을 때 pingd 가 인식하여 Resource 를 Fail-over 하는 과정에서 문제가 있습니다.</issue><environment>우선 간략히 가용성 테스트 절차를 설명 드리자면 아래와 같습니다.<br><br>1. A Node 의 Service Network (Bonding) 모두 절체<br><br>2. 잠시 후 Cluster Pingd 가 장애 감지<br><br>3. script stop -&gt; vip stop -&gt; LVM-LV (mount) stop -&gt; VG Inactive 단계로 정지하여, B Node 로 Fail-over<br><br>원래는 위와 같은 시나리오로 대략 40초 내외로 문제 없이 전환 됩니다.<br><br>하지만, Service Network 를 절체 할 시 NFS 서비스가 문제가 되어 df 등의 명령어를 입력하더라도 아무런 반응이 없습니다. (Hang 비슷)<br><br>이때 LVM-LV 가 Mount 되어 있는 /APP/controlm/ctmag1 디렉토리에 user 또는 아직 stop 안된 Process 등이 남아 있을 경우 umount 실패가 되는 문제가 발생합니다.<br><br>umount 실패가 되더라도 Filesystem on-fail=fence 로 설정하여 4분 정도 후에 IPMI Fence 작동 후 Fail-over 가 되기는 합니다.</environment><periodicityOfIssue>질문 드리고 싶은 사항은 아래와 같습니다.<br><br>1. NFS Network 문제등으로 Mount 에 문제가 있을 시 fuser 명령어도 원래 작동에 문제가 있는지 궁금합니다.<br><br>Filesystem resource 에 force_unmount=true 옵션을 넣어봤는데요, 이 옵션이 하는 역할 중 fuser -m 을 실행하던데, NFS Mount 문제시 fuser 실행하고 그대로 멈춰있는 현상을 발견하였습니다.<br><br>이 문제를 재현해 보기위해 수동으로 NFS 에 문제를 일으켜 fuser -cuk 를 실행 해 봤으나 동일하게 문제가 발생하였습니다.<br><br>2. pingd 설정 시 사용하는 dampen, multiplier, monitor interval 의 역할을 알고 싶습니다.<br><br>현재 설정은 아래와 같이 되어 있습니다.<br><br> Clone: ping-clone<br>  Resource: ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: dampen=5s multiplier=10 host_list=100.254.141.1 <br>   Operations: start interval=0s timeout=60 (ping-start-interval-0s)<br>               stop interval=0s timeout=20 (ping-stop-interval-0s)<br>               monitor interval=5s (ping-monitor-interval-5s)<br><br>이렇게 설정되어 있을 경우 Service VIP Network 를 모두 절체 했을 때 얼마만에 장애를 인지하고 Fail-over 시도를 시작하는지 알고 싶습니다.<br><br>3. 위와 같이 NFS 에 문제가 발생 시 강제로 NFS 관련 프로세스 등을 죽이고 fuser 를 활성화 할 수 있는 방법이 있는지 확인 부탁드립니다.</periodicityOfIssue><timeFramesAndUrgency>현재 설정되어 있는 pcs config 내용과 df 명령어 결과를 보내드립니다.<br><br>root@PCTAAL02SL / # pcs resource show --full<br> Group: PCTA1_group<br>  Resource: VG_SRPCTAAP1VG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPCTAAP1VG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPCTAAP1VG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPCTAAP1VG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPCTAAP1VG-start-interval-0s)<br>  Resource: LV_APP_con_ctmag1 (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 directory=/APP/controlm/ctmag1 fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=120 (LV_APP_con_ctmag1-stop-interval-0s)<br>               start interval=0s timeout=120 (LV_APP_con_ctmag1-start-interval-0s)<br>               monitor interval=5s timeout=10 on-fail=fence OCF_CHECK_LEVEL=10 (LV_APP_con_ctmag1-monitor-interval-5s)<br>  Resource: PCTA1_vip (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=100.254.142.53<br>   Operations: stop interval=0s timeout=20s (PCTA1_vip-stop-interval-0s)<br>               start interval=0s start-delay=3s (PCTA1_vip-start-interval-0s)<br>               monitor interval=5s start-delay=5s timeout=10s (PCTA1_vip-monitor-interval-5s)<br>  Resource: PCTA1_script (class=ocf provider=status type=MySAP)<br>   Attributes: monitor=/etc/corosync/ctmag1_monitor start=/etc/corosync/ctmag1_start stop=/etc/corosync/ctmag1_stop<br>   Operations: stop interval=0s timeout=120 (PCTA1_script-stop-interval-0s)<br>               monitor interval=60s enabled=false (PCTA1_script-monitor-interval-60s)<br>               start interval=0s timeout=120 (PCTA1_script-start-interval-0s)<br> Group: PCTA2_group<br>  Resource: VG_SRPCTAAP2VG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPCTAAP2VG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPCTAAP2VG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPCTAAP2VG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPCTAAP2VG-start-interval-0s)<br>  Resource: LV_APP_con_ctmag2 (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2 directory=/APP/controlm/ctmag2 fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=120 (LV_APP_con_ctmag2-stop-interval-0s)<br>               start interval=0s timeout=120 (LV_APP_con_ctmag2-start-interval-0s)<br>               monitor interval=5s timeout=10 on-fail=fence OCF_CHECK_LEVEL=10 (LV_APP_con_ctmag2-monitor-interval-5s)<br>  Resource: PCTA2_vip (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=100.254.142.54<br>   Operations: stop interval=0s timeout=20s (PCTA2_vip-stop-interval-0s)<br>               start interval=0s start-delay=3s (PCTA2_vip-start-interval-0s)<br>               monitor interval=5s start-delay=5s timeout=10s (PCTA2_vip-monitor-interval-5s)<br>  Resource: PCTA2_script (class=ocf provider=status type=MySAP)<br>   Attributes: monitor=/etc/corosync/ctmag2_monitor start=/etc/corosync/ctmag2_start stop=/etc/corosync/ctmag2_stop<br>   Operations: stop interval=0s timeout=120 (PCTA2_script-stop-interval-0s)<br>               monitor interval=60s enabled=false (PCTA2_script-monitor-interval-60s)<br>               start interval=0s timeout=120 (PCTA2_script-start-interval-0s)<br> Clone: ping-clone<br>  Resource: ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: dampen=5s multiplier=10 host_list=100.254.142.1<br>   Operations: start interval=0s timeout=60 (ping-start-interval-0s)<br>               stop interval=0s timeout=20 (ping-stop-interval-0s)<br>               monitor interval=5s (ping-monitor-interval-5s)<br>======================<br>root@PCTAAL02SL / # df -Ph<br>Filesystem                               Size  Used Avail Use% Mounted on<br>/dev/sda2                                 30G  9.3G   21G  31% /<br>devtmpfs                                  63G     0   63G   0% /dev<br>tmpfs                                     63G   54M   63G   1% /dev/shm<br>tmpfs                                     63G   18M   63G   1% /run<br>tmpfs                                     63G     0   63G   0% /sys/fs/cgroup<br>/dev/sda1                               1016M  210M  806M  21% /boot<br>/dev/sda3                                 20G  1.6G   19G   8% /var<br>/dev/mapper/vg9-CRASH                    129G   33M  129G   1% /CRASH<br>/dev/mapper/vg0-home                      10G  151M  9.9G   2% /home<br>/dev/mapper/vg0-sysadmin                  20G   15G  5.2G  75% /sysadmin<br>/dev/mapper/LRPCTAAPPVG-APP               10G   33M   10G   1% /APP<br>100.254.144.202:/BATCH_bat_sam_SAM_bak   5.0T  167G  4.9T   4% /batch_sam/SAM/backup<br>100.254.144.202:/BATCH_bat_sam_SAM_work  5.0T   88G  5.0T   2% /batch_sam/SAM/work<br>tmpfs                                     13G     0   13G   0% /run/user/30307<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   100G  3.7G   97G   4% /APP/controlm/ctmag2<br>tmpfs                                     13G     0   13G   0% /run/user/1322</timeFramesAndUrgency><cep>false</cep></case>