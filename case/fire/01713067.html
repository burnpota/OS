======================<br><b>생성계정 : Seung-Pil Kim</b><br><b>생성날짜 : 2016-09-30T02:21:31Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2016-10-15T10:07:15Z</b><br><b>id : 500A000000VDqTNIA1</b><br>======================<br><br><b><font size=15>
제목  : [삼성화재][보험ERP][DR계][quocld01][HA] pacemaker구성된 시스템에서 HeartBeat network Fail시 Fail-Over 상황에 연관되는 설정값 및 소요시간 계산 방법 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>pacemaker구성된 시스템에서 HeartBeat network Fail시 Fail-Over 상황에 연관되는 설정값 및 소요시간 계산 방법 문의 입니다.<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>####################################################################<br>SOSREPORT REF CASE ID :  https://access.redhat.com/support/cases/#/case/01676390<br><br><br>Host Name : quocld01, quocld02 ( quocld01-HB, quocld02-HB )<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version :<br>                        corosync-2.3.4-7.el7_2.1.x86_64<br>                        pacemaker-1.1.13-10.el7.x86_64<br>                        pcs-0.9.143-15.el7.x86_64<br>                        fence-agents-all-4.0.11-27.el7.x86_64<br><br>[root@quocld01 ~]# pcs config<br>Cluster Name: quoc_cluster<br>Corosync Nodes:<br> quocld01-HB quocld02-HB<br>Pacemaker Nodes:<br> quocld01-HB quocld02-HB<br><br>Resources:<br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.8.231.1<br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.83<br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_ascs1 (class=ocf provider=sfmi type=MySAP)<br>   Attributes: start=/home/qfpadm/mysap_start stop=/home/qfpadm/mysap_stop monitor=/home/qfpadm/mysap_monitor<br>   Operations: start interval=0s timeout=60 (rsc_ascs1-start-interval-0s)<br>               stop interval=0s timeout=60 (rsc_ascs1-stop-interval-0s)<br>               monitor interval=60s enabled=false (rsc_ascs1-monitor-interval-60s)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.84<br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br><br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;quocld01-HB quocld02-HB&quot; pcmk_off_timeout=90<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld01-HB ipaddr=42.8.240.144 login=redhat passwd=quocld01!1 lanplus=on auth=password delay=15<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld02-HB ipaddr=42.8.240.145 login=redhat passwd=quocld02!1 lanplus=on auth=password<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br><br> Node: quocld01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: quocld02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: quocld01-HB (score:10) (id:location-grp1-quocld01-HB-10)<br>  Resource: grp2<br>    Enabled on: quocld02-HB (score:10) (id:location-grp2-quocld02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: quocld01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: quocld02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_vip1<br>    Enabled on: quocld01-HB (score:10) (id:location-rsc_vip1-quocld01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: quocld02-HB (score:10) (id:location-rsc_vip2-quocld02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: quoc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> last-lrm-refresh: 1455954793<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>###################################################################<br><br><br>########## Fail-Over 관련 Timeout 계산 법 ########## <br><br>서비스가 정상화 되기위해서는<br>fail-over되는 클러스터 노드에서 서비스를 시작하는 시간이 필요하므로<br>각각의 상황별로 Fail-Over 되는 시간에, 별도로 서비스 시작 소요시간을 추가하여야 함을 고려하고 있으며,<br><br>아래  Q6)상황에서 각각의 소요시간 계산이 맞는지 문의 드립니다.<br><br><br>예)<br>삼성화재 ASCS 서비스 시작 소요시간( 2016.08 기준 ) : 약 30~60초<br><br><br>Q6) HeartBeat Fail시 서비스 Fail-Over 상황에서 소요시간 계산이 맞는지 문의 드립니다.<br><br>        6-1) 관련 파라미터 명 및 Default 값<br>        token timeout (default 1000 ms : 1 초)<br>        KDUMP pcmk_off_timeout (default 60 초)<br>        IPMI pcmk_off_timeout (default 60 초)<br><br><br>        6-2) 위의 2-1)번의 파라미터로 Fail-Over 계산식<br>        [ token timeout ] + ( fence_kdump timeout * retry ) + ( fence_ipmilan timeout * retry )<br> <br>        * 이론상(Theory)<br>        [ 10 ] + [ 90 * 1 ] + [ 60 * 1 ] = MAX 160s( 2min 40s )<br><br>###################################################################<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 4 (Low)</b><br><br><br><folderNumber>74166</folderNumber><br><comment id="a0aA000000HesufIAB"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-09-30T08:23:55Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-09-30T08:23:54Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>&gt; Q6) HeartBeat Fail시 서비스 Fail-Over 상황에서 소요시간 계산이 맞는지 문의 드립니다.<br><br>적어 주신 계산식 중 retry를 1로 지정 해 주셨지만 fencing이 실패 될 경우에 지속적으로 <br>fencing 시도를 하기 때문에 retry 값을 n으로 보셔야 될 것 같습니다. <br><br>즉 Max fail-over time: <br><br>[ token timeout ] + ( fence_kdump timeout * retry ) + ( fence_ipmilan timeout * retry )<br>= 1s + 60s * n + 60s * n <br>= (60 * n + 1)s<br><br>참고로, 서비스 Down time을 계산 하시려면 Standby 노드에서 start 작업이 소요 시간도 고려 해야 <br>될 것 같습니다. <br>======================<br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-09-30T08:23:54Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Heqj5IAB"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-09-30T02:33:54Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-09-30T02:33:53Z</b><br><br>안녕하세요.<br><br>추가로 corosync 정보를 전달 드립니다.<br><br># cat quocld01/corosync.dump <br>config.totemconfig_reload_in_progress (u8) = 0<br>internal_configuration.service.0.name (str) = corosync_cmap<br>internal_configuration.service.0.ver (u32) = 0<br>internal_configuration.service.1.name (str) = corosync_cfg<br>internal_configuration.service.1.ver (u32) = 0<br>internal_configuration.service.2.name (str) = corosync_cpg<br>internal_configuration.service.2.ver (u32) = 0<br>internal_configuration.service.3.name (str) = corosync_quorum<br>internal_configuration.service.3.ver (u32) = 0<br>internal_configuration.service.4.name (str) = corosync_pload<br>internal_configuration.service.4.ver (u32) = 0<br>internal_configuration.service.5.name (str) = corosync_votequorum<br>internal_configuration.service.5.ver (u32) = 0<br>logging.logfile (str) = /var/log/cluster/corosync.log<br>logging.to_logfile (str) = yes<br>logging.to_syslog (str) = yes<br>nodelist.local_node_pos (u32) = 0<br>nodelist.node.0.nodeid (u32) = 1<br>nodelist.node.0.ring0_addr (str) = quocld01-HB<br>nodelist.node.0.ring1_addr (str) = quocld01<br>nodelist.node.1.nodeid (u32) = 2<br>nodelist.node.1.ring0_addr (str) = quocld02-HB<br>nodelist.node.1.ring1_addr (str) = quocld02<br>quorum.provider (str) = corosync_votequorum<br>quorum.two_node (u8) = 1<br>runtime.blackbox.dump_flight_data (str) = 1472621105<br>runtime.blackbox.dump_state (str) = 1472621105<br>runtime.config.totem.consensus (u32) = 12000<br>runtime.config.totem.downcheck (u32) = 1000<br>runtime.config.totem.fail_recv_const (u32) = 2500<br>runtime.config.totem.heartbeat_failures_allowed (u32) = 0<br>runtime.config.totem.hold (u32) = 1894<br>runtime.config.totem.join (u32) = 50<br>runtime.config.totem.max_messages (u32) = 17<br>runtime.config.totem.max_network_delay (u32) = 50<br>runtime.config.totem.merge (u32) = 200<br>runtime.config.totem.miss_count_const (u32) = 5<br>runtime.config.totem.rrp_autorecovery_check_timeout (u32) = 1000<br>runtime.config.totem.rrp_problem_count_mcast_threshold (u32) = 100<br>runtime.config.totem.rrp_problem_count_threshold (u32) = 10<br>runtime.config.totem.rrp_problem_count_timeout (u32) = 2000<br>runtime.config.totem.rrp_token_expired_timeout (u32) = 2380<br>runtime.config.totem.send_join (u32) = 0<br>runtime.config.totem.seqno_unchanged_const (u32) = 30<br>runtime.config.totem.token (u32) = 10000<br>runtime.config.totem.token_retransmit (u32) = 2380<br>runtime.config.totem.token_retransmits_before_loss_const (u32) = 4<br>runtime.config.totem.window_size (u32) = 50<br>runtime.connections.active (u64) = 9<br>runtime.connections.attrd:6503:0x7fb3395690d0.client_pid (u32) = 6503<br>runtime.connections.attrd:6503:0x7fb3395690d0.dispatched (u64) = 550<br>runtime.connections.attrd:6503:0x7fb3395690d0.flow_control (u32) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.flow_control_count (u64) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.invalid_request (u64) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.name (str) = attrd<br>runtime.connections.attrd:6503:0x7fb3395690d0.overload (u64) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.queue_size (u32) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.recv_retries (u64) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.requests (u64) = 278<br>runtime.connections.attrd:6503:0x7fb3395690d0.responses (u64) = 2<br>runtime.connections.attrd:6503:0x7fb3395690d0.send_retries (u64) = 0<br>runtime.connections.attrd:6503:0x7fb3395690d0.service_id (u32) = 2<br>runtime.connections.cib:6500:0x7fb33976eae0.client_pid (u32) = 6500<br>runtime.connections.cib:6500:0x7fb33976eae0.dispatched (u64) = 61<br>runtime.connections.cib:6500:0x7fb33976eae0.flow_control (u32) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.flow_control_count (u64) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.invalid_request (u64) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.name (str) = cib<br>runtime.connections.cib:6500:0x7fb33976eae0.overload (u64) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.queue_size (u32) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.recv_retries (u64) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.requests (u64) = 29<br>runtime.connections.cib:6500:0x7fb33976eae0.responses (u64) = 2<br>runtime.connections.cib:6500:0x7fb33976eae0.send_retries (u64) = 0<br>runtime.connections.cib:6500:0x7fb33976eae0.service_id (u32) = 2<br>runtime.connections.closed (u64) = 3658<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.client_pid (u32) = 51673<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.dispatched (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control (u32) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control_count (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.invalid_request (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.name (str) = corosync-cmapct<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.overload (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.queue_size (u32) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.recv_retries (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.requests (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.responses (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.send_retries (u64) = 0<br>runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.service_id (u32) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.client_pid (u32) = 6505<br>runtime.connections.crmd:6505:0x7fb33966d160.dispatched (u64) = 33<br>runtime.connections.crmd:6505:0x7fb33966d160.flow_control (u32) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.flow_control_count (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.invalid_request (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.name (str) = crmd<br>runtime.connections.crmd:6505:0x7fb33966d160.overload (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.queue_size (u32) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.recv_retries (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.requests (u64) = 9<br>runtime.connections.crmd:6505:0x7fb33966d160.responses (u64) = 2<br>runtime.connections.crmd:6505:0x7fb33966d160.send_retries (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966d160.service_id (u32) = 2<br>runtime.connections.crmd:6505:0x7fb33966ec70.client_pid (u32) = 6505<br>runtime.connections.crmd:6505:0x7fb33966ec70.dispatched (u64) = 1<br>runtime.connections.crmd:6505:0x7fb33966ec70.flow_control (u32) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.flow_control_count (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.invalid_request (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.name (str) = crmd<br>runtime.connections.crmd:6505:0x7fb33966ec70.overload (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.queue_size (u32) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.recv_retries (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.requests (u64) = 3<br>runtime.connections.crmd:6505:0x7fb33966ec70.responses (u64) = 3<br>runtime.connections.crmd:6505:0x7fb33966ec70.send_retries (u64) = 0<br>runtime.connections.crmd:6505:0x7fb33966ec70.service_id (u32) = 3<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.client_pid (u32) = 6496<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.dispatched (u64) = 9<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control_count (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.invalid_request (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.name (str) = pacemakerd<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.overload (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.queue_size (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.recv_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.requests (u64) = 9<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.responses (u64) = 2<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.send_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb33915f0b0.service_id (u32) = 2<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.client_pid (u32) = 6496<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.dispatched (u64) = 1<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control_count (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.invalid_request (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.name (str) = pacemakerd<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.overload (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.queue_size (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.recv_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.requests (u64) = 3<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.responses (u64) = 3<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.send_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339160eb0.service_id (u32) = 3<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.client_pid (u32) = 6496<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.dispatched (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control_count (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.invalid_request (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.name (str) = pacemakerd<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.overload (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.queue_size (u32) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.recv_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.requests (u64) = 1<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.responses (u64) = 1<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.send_retries (u64) = 0<br>runtime.connections.pacemakerd:6496:0x7fb339164f60.service_id (u32) = 1<br>runtime.connections.stonithd:6501:0x7fb339166580.client_pid (u32) = 6501<br>runtime.connections.stonithd:6501:0x7fb339166580.dispatched (u64) = 5<br>runtime.connections.stonithd:6501:0x7fb339166580.flow_control (u32) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.flow_control_count (u64) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.invalid_request (u64) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.name (str) = stonithd<br>runtime.connections.stonithd:6501:0x7fb339166580.overload (u64) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.queue_size (u32) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.recv_retries (u64) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.requests (u64) = 4<br>runtime.connections.stonithd:6501:0x7fb339166580.responses (u64) = 2<br>runtime.connections.stonithd:6501:0x7fb339166580.send_retries (u64) = 0<br>runtime.connections.stonithd:6501:0x7fb339166580.service_id (u32) = 2<br>runtime.services.cfg.0.rx (u64) = 0<br>runtime.services.cfg.0.tx (u64) = 0<br>runtime.services.cfg.1.rx (u64) = 0<br>runtime.services.cfg.1.tx (u64) = 0<br>runtime.services.cfg.2.rx (u64) = 0<br>runtime.services.cfg.2.tx (u64) = 0<br>runtime.services.cfg.3.rx (u64) = 0<br>runtime.services.cfg.3.tx (u64) = 0<br>runtime.services.cfg.service_id (u16) = 1<br>runtime.services.cmap.0.rx (u64) = 3<br>runtime.services.cmap.0.tx (u64) = 2<br>runtime.services.cmap.service_id (u16) = 0<br>runtime.services.cpg.0.rx (u64) = 5<br>runtime.services.cpg.0.tx (u64) = 5<br>runtime.services.cpg.1.rx (u64) = 0<br>runtime.services.cpg.1.tx (u64) = 0<br>runtime.services.cpg.2.rx (u64) = 1<br>runtime.services.cpg.2.tx (u64) = 0<br>runtime.services.cpg.3.rx (u64) = 657<br>runtime.services.cpg.3.tx (u64) = 319<br>runtime.services.cpg.4.rx (u64) = 0<br>runtime.services.cpg.4.tx (u64) = 0<br>runtime.services.cpg.5.rx (u64) = 3<br>runtime.services.cpg.5.tx (u64) = 2<br>runtime.services.cpg.6.rx (u64) = 0<br>runtime.services.cpg.6.tx (u64) = 0<br>runtime.services.cpg.service_id (u16) = 2<br>runtime.services.pload.0.rx (u64) = 0<br>runtime.services.pload.0.tx (u64) = 0<br>runtime.services.pload.1.rx (u64) = 0<br>runtime.services.pload.1.tx (u64) = 0<br>runtime.services.pload.service_id (u16) = 4<br>runtime.services.quorum.service_id (u16) = 3<br>runtime.services.votequorum.0.rx (u64) = 7<br>runtime.services.votequorum.0.tx (u64) = 4<br>runtime.services.votequorum.1.rx (u64) = 0<br>runtime.services.votequorum.1.tx (u64) = 0<br>runtime.services.votequorum.2.rx (u64) = 0<br>runtime.services.votequorum.2.tx (u64) = 0<br>runtime.services.votequorum.3.rx (u64) = 0<br>runtime.services.votequorum.3.tx (u64) = 0<br>runtime.services.votequorum.service_id (u16) = 5<br>runtime.totem.pg.mrp.rrp.0.faulty (u8) = 0<br>runtime.totem.pg.mrp.rrp.1.faulty (u8) = 0<br>runtime.totem.pg.mrp.srp.avg_backlog_calc (u32) = 0<br>runtime.totem.pg.mrp.srp.avg_token_workload (u32) = 0<br>runtime.totem.pg.mrp.srp.commit_entered (u64) = 2<br>runtime.totem.pg.mrp.srp.commit_token_lost (u64) = 0<br>runtime.totem.pg.mrp.srp.consensus_timeouts (u64) = 0<br>runtime.totem.pg.mrp.srp.continuous_gather (u32) = 0<br>runtime.totem.pg.mrp.srp.continuous_sendmsg_failures (u32) = 0<br>runtime.totem.pg.mrp.srp.firewall_enabled_or_nic_failure (u8) = 0<br>runtime.totem.pg.mrp.srp.gather_entered (u64) = 2<br>runtime.totem.pg.mrp.srp.gather_token_lost (u64) = 0<br>runtime.totem.pg.mrp.srp.mcast_retx (u64) = 0<br>runtime.totem.pg.mrp.srp.mcast_rx (u64) = 747<br>runtime.totem.pg.mrp.srp.mcast_tx (u64) = 350<br>runtime.totem.pg.mrp.srp.memb_commit_token_rx (u64) = 4<br>runtime.totem.pg.mrp.srp.memb_commit_token_tx (u64) = 6<br>runtime.totem.pg.mrp.srp.memb_join_rx (u64) = 5<br>runtime.totem.pg.mrp.srp.memb_join_tx (u64) = 2<br>runtime.totem.pg.mrp.srp.memb_merge_detect_rx (u64) = 1917<br>runtime.totem.pg.mrp.srp.memb_merge_detect_tx (u64) = 1917<br>runtime.totem.pg.mrp.srp.members.1.config_version (u64) = 0<br>runtime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(192.169.25.81) r(1) ip(42.8.231.81) <br>runtime.totem.pg.mrp.srp.members.1.join_count (u32) = 1<br>runtime.totem.pg.mrp.srp.members.1.status (str) = joined<br>runtime.totem.pg.mrp.srp.members.2.config_version (u64) = 0<br>runtime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(192.169.25.82) r(1) ip(42.8.231.82) <br>runtime.totem.pg.mrp.srp.members.2.join_count (u32) = 1<br>runtime.totem.pg.mrp.srp.members.2.status (str) = joined<br>runtime.totem.pg.mrp.srp.mtt_rx_token (u32) = 161<br>runtime.totem.pg.mrp.srp.operational_entered (u64) = 2<br>runtime.totem.pg.mrp.srp.operational_token_lost (u64) = 0<br>runtime.totem.pg.mrp.srp.orf_token_rx (u64) = 20755<br>runtime.totem.pg.mrp.srp.orf_token_tx (u64) = 2<br>runtime.totem.pg.mrp.srp.recovery_entered (u64) = 2<br>runtime.totem.pg.mrp.srp.recovery_token_lost (u64) = 0<br>runtime.totem.pg.mrp.srp.rx_msg_dropped (u64) = 0<br>runtime.totem.pg.mrp.srp.token_hold_cancel_rx (u64) = 582<br>runtime.totem.pg.mrp.srp.token_hold_cancel_tx (u64) = 294<br>runtime.totem.pg.msg_queue_avail (u32) = 0<br>runtime.totem.pg.msg_reserved (u32) = 1<br>runtime.votequorum.ev_barrier (u32) = 2<br>runtime.votequorum.highest_node_id (u32) = 2<br>runtime.votequorum.lowest_node_id (u32) = 1<br>runtime.votequorum.this_node_id (u32) = 1<br>runtime.votequorum.two_node (u8) = 1<br>runtime.votequorum.wait_for_all_status (u8) = 0<br>totem.cluster_name (str) = quoc_cluster<br>totem.interface.0.bindnetaddr (str) = quocld01-HB<br>totem.interface.0.mcastaddr (str) = 239.192.0.180<br>totem.interface.0.mcastport (u16) = 5405<br>totem.interface.1.bindnetaddr (str) = quocld01<br>totem.interface.1.mcastaddr (str) = 239.192.0.181<br>totem.interface.1.mcastport (u16) = 5405<br>totem.rrp_mode (str) = passive<br>totem.secauth (str) = off<br>totem.token (u32) = 10000<br>totem.transport (str) = udpu<br>totem.version (u32) = 2<br>uidgid.gid.189 (u8) = 1<br><br><br>고맙습니다.<br><br><publishedDate>2016-09-30T02:33:53Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>SOSREPORT REF CASE ID :  https://access.redhat.com/support/cases/#/case/01676390<br><br><br>Host Name : quocld01, quocld02 ( quocld01-HB, quocld02-HB )<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version :<br>                        corosync-2.3.4-7.el7_2.1.x86_64<br>                        pacemaker-1.1.13-10.el7.x86_64<br>                        pcs-0.9.143-15.el7.x86_64<br>                        fence-agents-all-4.0.11-27.el7.x86_64<br><br>[root@quocld01 ~]# pcs config<br>Cluster Name: quoc_cluster<br>Corosync Nodes:<br> quocld01-HB quocld02-HB<br>Pacemaker Nodes:<br> quocld01-HB quocld02-HB<br><br>Resources:<br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.8.231.1<br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.83<br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_ascs1 (class=ocf provider=sfmi type=MySAP)<br>   Attributes: start=/home/qfpadm/mysap_start stop=/home/qfpadm/mysap_stop monitor=/home/qfpadm/mysap_monitor<br>   Operations: start interval=0s timeout=60 (rsc_ascs1-start-interval-0s)<br>               stop interval=0s timeout=60 (rsc_ascs1-stop-interval-0s)<br>               monitor interval=60s enabled=false (rsc_ascs1-monitor-interval-60s)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.84<br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br><br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;quocld01-HB quocld02-HB&quot; pcmk_off_timeout=90<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld01-HB ipaddr=42.8.240.144 login=redhat passwd=quocld01!1 lanplus=on auth=password delay=15<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld02-HB ipaddr=42.8.240.145 login=redhat passwd=quocld02!1 lanplus=on auth=password<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br><br> Node: quocld01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: quocld02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: quocld01-HB (score:10) (id:location-grp1-quocld01-HB-10)<br>  Resource: grp2<br>    Enabled on: quocld02-HB (score:10) (id:location-grp2-quocld02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: quocld01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: quocld02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_vip1<br>    Enabled on: quocld01-HB (score:10) (id:location-rsc_vip1-quocld01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: quocld02-HB (score:10) (id:location-rsc_vip2-quocld02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: quoc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> last-lrm-refresh: 1455954793<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>###################################################################<br><br><br>########## Fail-Over 관련 Timeout 계산 법 ########## <br><br>서비스가 정상화 되기위해서는<br>fail-over되는 클러스터 노드에서 서비스를 시작하는 시간이 필요하므로<br>각각의 상황별로 Fail-Over 되는 시간에, 별도로 서비스 시작 소요시간을 추가하여야 함을 고려하고 있으며,<br><br>아래  Q6)상황에서 각각의 소요시간 계산이 맞는지 문의 드립니다.<br><br><br>예)<br>삼성화재 ASCS 서비스 시작 소요시간( 2016.08 기준 ) : 약 30~60초<br><br><br>Q6) HeartBeat Fail시 서비스 Fail-Over 상황에서 소요시간 계산이 맞는지 문의 드립니다.<br><br>        6-1) 관련 파라미터 명 및 Default 값<br>        token timeout (default 1000 ms : 1 초)<br>        KDUMP pcmk_off_timeout (default 60 초)<br>        IPMI pcmk_off_timeout (default 60 초)<br><br><br>        6-2) 위의 2-1)번의 파라미터로 Fail-Over 계산식<br>        [ token timeout ] + ( fence_kdump timeout * retry ) + ( fence_ipmilan timeout * retry )<br> <br>        * 이론상(Theory)<br>        [ 10 ] + [ 90 * 1 ] + [ 60 * 1 ] = MAX 160s( 2min 40s )<br><br>###################################################################</environment><cep>false</cep><folderName>ERP-SAP-ASCS_ERS</folderName></case>