======================<br><b>생성계정 : Seung-Pil Kim</b><br><b>생성날짜 : 2016-07-29T06:38:04Z</b><br><b>마지막 답변자 : Meiyan Zheng</b><br><b>마지막 수정 일자 : 2016-08-25T05:55:42Z</b><br><b>id : 500A000000UoJbeIAF</b><br>======================<br><br><b><font size=15>
제목  : [삼성화재][보험ERP][DR계] quocld02 - HA cluster system rebooted.
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>RHEL 7.2 HA로 구성된 시스템 중, 두 번째 노드( quocld02-HB )가 리부팅 되었습니다.<br>관련하여 원인분석이 필요합니다.<br><br>Host Name : quocld02 ( quocld02-HB )<br>Rebooted Time : Jul 28 14:53<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version : <br>======================<br>corosync-2.3.4-7.el7_2.1.x86_64<br>======================<br>pacemaker-1.1.13-10.el7.x86_64<br>======================<br>pcs-0.9.143-15.el7.x86_64<br>======================<br>fence-agents-all-4.0.11-27.el7.x86_64<br>======================<br><br><br><br>HA 노드중 리부팅되어진 quocld02 시스템의 특이한 로그는 하기와 같습니다.<br><br>Jul 28 14:51:53 quocld02 cib[13713]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 cib[13713]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm failed<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm[0x139c270] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw failed<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw[0x147aa20] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:   error: The cib process (13713) terminated with signal 6 (core=0)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to the CIB terminated...<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_ERROR from crmd_cib_connection_destroy() received in state S_IDLE<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: State transition S_IDLE -&gt; S_RECOVERY [ input=I_ERROR cause=C_FSA_INTERNAL origin=crmd_cib_connection_destroy ]<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Fast-tracking shutdown in response to errors<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Not voting in election, we're in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:  notice: Connection to the CIB terminated. Shutting down.<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng failed<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng[0x1a67e80] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: LRMD lost STONITH connection<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_TERMINATE from do_recover() received in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: STONITH connection failed, finalizing 1 pending operations.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 3 (Normal)</b><br><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><folderNumber>74166</folderNumber><br><comment id="a0aA000000HkX4HIAV"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-08-25T05:55:39Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-08-25T05:55:39Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>우선 말씀 하신 대로 케이스를 종료 처리를 해 드리겠습니다. 만약 본 케이스와 관련 하여 <br>추가적인 문의 사항이 있으시면 언제든지 케이스를 재오픈 하실수 있습니다. <br><br><br>좋은 하루 되시기 바랍니다. <br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-08-25T05:55:39Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HkVfKIAV"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-08-25T01:40:06Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-08-25T01:40:06Z</b><br><br>안녕하세요.<br><br>전달 주신 내용은 확인할 수 있는 방법을 찾아보겠습니다.<br>지원에 감사 드립니다.<br><br>우선, 본 기술문의는 종료 하여 주시기 바랍니다.<br>필요시 기술문의를 재개하겠습니다.<br><br>고맙습니다.<br><br>(Zheng, Meiyan에 회신)<br>&gt; 안녕하세요,<br>&gt; <br>&gt; Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br>&gt; <br>&gt; quocld02 재부팅 되기전 각 component에서 &quot;&quot; 와 같은 에러가 지속적으로 <br>&gt; 발생 하고 있었습니다. <br>&gt; <br>&gt; Jul 28 14:51:53 [13716] quocld02      attrd:    error: uid2username:    Cannot get password entry of uid: 0: Success (0)<br>&gt; Jul 28 14:51:53 [13716] quocld02      attrd:    error: crm_abort:       attrd_ipc_dispatch: Triggered fatal assert at main.c:220 : client-&gt;user != NULL<br>&gt; <br>&gt; <br>&gt; 따라서 각 component가 exit 하여 fencing 이슈가 발생 한 것으로 보입니다. <br>&gt; <br>&gt; Jul 28 14:51:53 [13887] quocld01 stonith-ng:   notice: crm_update_peer_state_iter:      crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>&gt; Jul 28 14:51:53 [13891] quocld01       crmd:   notice: peer_update_callback:    Our peer on the DC (quocld02-HB) is dead<br>&gt; Jul 28 14:51:53 [13891] quocld01       crmd:     info: erase_status_tag:        Deleting xpath: //node_state[@uname='quocld02-HB']/transient_attributes<br>&gt; Jul 28 14:51:53 [13887] quocld01 stonith-ng:   notice: crm_reap_dead_member:    Removing quocld02-HB/2 from the<br><br><publishedDate>2016-08-25T01:40:06Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HkK90IAF"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-08-24T07:48:45Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-08-24T07:48:45Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br>quocld02 재부팅 되기전 각 component에서 &quot;&quot; 와 같은 에러가 지속적으로 <br>발생 하고 있었습니다. <br><br>Jul 28 14:51:53 [13716] quocld02      attrd:    error: uid2username:    Cannot get password entry of uid: 0: Success (0)<br>Jul 28 14:51:53 [13716] quocld02      attrd:    error: crm_abort:       attrd_ipc_dispatch: Triggered fatal assert at main.c:220 : client-&gt;user != NULL<br><br><br>따라서 각 component가 exit 하여 fencing 이슈가 발생 한 것으로 보입니다. <br><br>Jul 28 14:51:53 [13887] quocld01 stonith-ng:   notice: crm_update_peer_state_iter:      crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:53 [13891] quocld01       crmd:   notice: peer_update_callback:    Our peer on the DC (quocld02-HB) is dead<br>Jul 28 14:51:53 [13891] quocld01       crmd:     info: erase_status_tag:        Deleting xpath: //node_state[@uname='quocld02-HB']/transient_attributes<br>Jul 28 14:51:53 [13887] quocld01 stonith-ng:   notice: crm_reap_dead_member:    Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:53 [13887] quocld01 stonith-ng:   notice: reap_crm_member: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br><br><br>&quot;Cannot get password entry of uid&quot; 메시지는 getpwuid(uid) 함수로 유저 <br>패스워드를 받을 수 없을 경우에 발생 할수 있습니다. <br><br>lib/common/utils.c:<br><br>#if ENABLE_ACL<br>char *<br>uid2username(uid_t uid)<br>{<br>    struct passwd *pwent = getpwuid(uid);<br><br>    if (pwent == NULL) {<br>        crm_perror(LOG_ERR, &quot;Cannot get password entry of uid: %d&quot;, uid);<br>        return NULL;<br><br>    } else {<br>        return strdup(pwent-&gt;pw_name);<br>    }<br>}<br><br><br>로그를 보시면 아시겠지만 이슈가 발생 하기 직전에 root가 로그인 한적이 있었습니다. 해당 시점에 <br>로그인 하신후 passwd/shadow 또는 nsswitch 파일을 수정 하신적이 있는지 확인 부탁드립니다. <br><br>Jul 28 14:50:46 quocld02 systemd-logind: New session 1544 of user root.<br>Jul 28 14:50:46 quocld02 dbus[3338]: [system] Activating service name='org.freedesktop.problems' (using servicehelper)<br>Jul 28 14:50:46 quocld02 dbus-daemon: dbus[3338]: [system] Activating service name='org.freedesktop.problems' (using servicehelper)<br>Jul 28 14:50:46 quocld02 dbus[3338]: [system] Successfully activated service 'org.freedesktop.problems'<br>Jul 28 14:50:46 quocld02 dbus-daemon: dbus[3338]: [system] Successfully activated service 'org.freedesktop.problems'<br>======================<br>요청 드린 내용을 확인 부탁드립니다. <br>좋은 하루 되시기 바랍니다. <br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-08-24T07:48:45Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Hk4kKIAR"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-08-23T07:57:26Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-08-23T07:57:26Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>먼저 요청 드린 내용에 대해 확인 해 주셔서 감사 드립니다. <br>문의 하신 내용에 대해 현재 분석 중에 있으며 만약 업데이트 사항이 <br>있으면 바로 공유 드리도록 하겠습니다. <br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-08-23T07:57:26Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HjnZkIAJ"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-08-22T08:53:21Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-08-22T08:53:21Z</b><br><br>안녕하세요.<br><br>마지막 답변주신 내용을 확인하는데 시간이 소요되어,<br>회신이 늦었습니다.<br><br>[ Q1 ]<br>&gt; 노드 quocld02 재부팅 되는 원인은 아래와 같이 보입니다. 즉 누군가가 power key 를 눌러 <br>&gt; 시스템이 재부팅 된 것으로 보입니다. <br>&gt; <br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: Power key pressed.<br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: Powering Off...<br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: System is powering down.<br><br>[ A1 ]<br>상기 로그는 quocld01 노드에 의하여,  quocld02가 IPMI를 통하여 fencing 될 때 발생한 메세지 입니다.<br>재현 테스트를 통하여 확인하였습니다.<br>======================<br>[ Q2 ]<br>&gt; 즉 해당 노드에서 uid 189로 되어 있는 유저 이름이나 그룹을 확인 할수 없습니다. <br>&gt; <br>&gt; uid 189는 유저 hacluster 이며 pacemaker 에서 사용 하고 있는 계정입니다. 혹시 <br>&gt; 해당 에러가 발생 하기전에 어떤 작업이 있었는지요? <br>&gt; hacluster 의 password 도 변경 하신적이 있으신가요? <br>&gt; 즉 pacemaker 운영중 hacluster 정보를 변경 하신적<br><br>[ A2 ]<br>- 에러가 발생하기 직전, 어떠한 작업이 있었는지 확인할 수 없었습니다.<br>- hacluster의 paswword를 변경한 적은 없습니다.<br>- 운영중 hacluster의 정보를 변경한 적은 없습니다.<br>======================<br>고맙습니다.<br><br>(Zheng, Meiyan에 회신)<br>&gt; 안녕하세요,<br>&gt; <br>&gt; Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br>&gt; <br>&gt; <br>&gt; 노드 quocld02 재부팅 되는 원인은 아래와 같이 보입니다. 즉 누군가가 power key 를 눌러 <br>&gt; 시스템이 재부팅 된 것으로 보입니다. <br>&gt; <br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: Power key pressed.<br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: Powering Off...<br>&gt; Jul 28 14:53:27 quocld02 systemd-logind: System is powering down.<br>&gt; <br>&gt; <br>&gt; 노드가 재부팅 되기전 아래와 같이 에러가 발생 하고 있는 것으로 보입니다. <br>&gt; <br>&gt; Jul 28 14:51:56 quocld02 cib[788]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>&gt; Jul 28 14:51:56 quocld02 cib[788]:   error: Cannot get name for uid: 189: Success (0)<br>&gt; Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>&gt; Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br>&gt; <br>&gt; 즉 해당 노드에서 uid 189로 되어 있는 유저 이름이나 그룹을 확인 할수 없습니다. <br>&gt; <br>&gt; uid 189는 유저 hacluster 이며 pacemaker 에서 사용 하고 있는 계정입니다. 혹시 <br>&gt; 해당 에러가 발생 하기전에 어떤 작업이 있었는지요? <br>&gt; hacluster 의 password 도 변경 하신적이 있으신가요? <br>&gt; 즉 pacemaker 운영중 hacluster 정보를 변경 하신적<br><br><publishedDate>2016-08-22T08:53:21Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HXAxlIAH"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-08-01T02:28:08Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-08-01T02:28:08Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>노드 quocld02 재부팅 되는 원인은 아래와 같이 보입니다. 즉 누군가가 power key 를 눌러 <br>시스템이 재부팅 된 것으로 보입니다. <br><br>Jul 28 14:53:27 quocld02 systemd-logind: Power key pressed.<br>Jul 28 14:53:27 quocld02 systemd-logind: Powering Off...<br>Jul 28 14:53:27 quocld02 systemd-logind: System is powering down.<br><br><br>노드가 재부팅 되기전 아래와 같이 에러가 발생 하고 있는 것으로 보입니다. <br><br>Jul 28 14:51:56 quocld02 cib[788]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:56 quocld02 cib[788]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br><br>즉 해당 노드에서 uid 189로 되어 있는 유저 이름이나 그룹을 확인 할수 없습니다. <br><br>uid 189는 유저 hacluster 이며 pacemaker 에서 사용 하고 있는 계정입니다. 혹시 <br>해당 에러가 발생 하기전에 어떤 작업이 있었는지요? <br>hacluster 의 password 도 변경 하신적이 있으신가요? <br>즉 pacemaker 운영중 hacluster 정보를 변경 하신적이 있으신가요?<br> <br><br>확인 부탁드립니다. <br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-08-01T02:28:08Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000HWt1oIAD"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-07-29T06:40:48Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-07-29T06:40:48Z</b><br><br>######################################################################################################<br>#<br>#<br>SYSTEM INFO : quocld02<br>#<br>######################################################################################################<br>======================<br><br>[root@quocld02:~]#ls -al  /var/lib/pacemaker/<br>total 12<br>drwxr-x---   6 hacluster haclient   57 Jul  8 15:11 .<br>drwxr-xr-x. 75 root      root     4096 Jul 29 03:12 ..<br>drwxr-x---   2 hacluster haclient    6 Oct  9  2015 blackbox<br>drwxr-x---   2 hacluster haclient 4096 Jul 28 15:10 cib<br>drwxr-x---   2 hacluster haclient    6 Oct  9  2015 cores<br>drwxr-x---   2 hacluster haclient 4096 Jul 21 15:12 pengine<br>[root@quocld02:~]#ls -al  /var/lib/pacemaker/cib/<br>total 544<br>drwxr-x--- 2 hacluster haclient  4096 Jul 28 15:10 .<br>drwxr-x--- 6 hacluster haclient    57 Jul  8 15:11 ..<br>-rw------- 1 hacluster haclient 10153 Jul 12 14:27 cib-10.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-10.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 12 14:27 cib-11.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-11.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 12 16:02 cib-12.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 16:02 cib-12.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 12 16:02 cib-13.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 16:02 cib-13.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 14 10:44 cib-14.raw<br>-rw------- 1 hacluster haclient    32 Jul 14 10:44 cib-14.raw.sig<br>-rw------- 1 hacluster haclient 10023 Jul 15 17:49 cib-15.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:49 cib-15.raw.sig<br>-rw------- 1 hacluster haclient 10023 Jul 15 17:50 cib-16.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:50 cib-16.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:52 cib-17.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:52 cib-17.raw.sig<br>-rw------- 1 hacluster haclient 10025 Jul 15 17:53 cib-18.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:53 cib-18.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:53 cib-19.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:53 cib-19.raw.sig<br>-rw------- 1 hacluster haclient   258 Jul 12 14:18 cib-1.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:18 cib-1.raw.sig<br>-rw------- 1 hacluster haclient 10025 Jul 15 17:54 cib-20.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:54 cib-20.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:54 cib-21.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:54 cib-21.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 18:51 cib-22.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 18:51 cib-22.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 18:51 cib-23.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 18:51 cib-23.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 15 19:21 cib-24.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 19:21 cib-24.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 19:21 cib-25.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 19:21 cib-25.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 21 14:57 cib-26.raw<br>-rw------- 1 hacluster haclient    32 Jul 21 14:57 cib-26.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 21 14:57 cib-27.raw<br>-rw------- 1 hacluster haclient    32 Jul 21 14:57 cib-27.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:51 cib-28.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:51 cib-28.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 28 14:51 cib-29.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:51 cib-29.raw.sig<br>-rw------- 1 hacluster haclient   425 Jul 12 14:18 cib-2.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:18 cib-2.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 28 14:51 cib-30.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:51 cib-30.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 28 14:51 cib-31.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:51 cib-31.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:52 cib-32.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:52 cib-32.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:52 cib-33.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:52 cib-33.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 28 15:10 cib-34.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 15:10 cib-34.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 15:10 cib-35.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 15:10 cib-35.raw.sig<br>-rw------- 1 hacluster haclient   726 Jul 12 14:19 cib-3.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-3.raw.sig<br>-rw------- 1 hacluster haclient   841 Jul 12 14:19 cib-4.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-4.raw.sig<br>-rw------- 1 hacluster haclient   940 Jul 12 14:19 cib-5.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-5.raw.sig<br>-rw------- 1 hacluster haclient 10019 Jul 12 14:21 cib-6.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:21 cib-6.raw.sig<br>-rw------- 1 hacluster haclient 10031 Jul 12 14:21 cib-7.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:21 cib-7.raw.sig<br>-rw------- 1 hacluster haclient 10153 Jul 12 14:26 cib-8.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:26 cib-8.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 12 14:27 cib-9.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-9.raw.sig<br>-rw-r----- 1 hacluster haclient     2 Jul 28 15:10 cib.last<br>-rw------- 1 hacluster haclient  9907 Jul 28 15:10 cib.xml<br>-rw------- 1 hacluster haclient    32 Jul 28 15:10 cib.xml.sig<br><br><publishedDate>2016-07-29T06:40:48Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000HWt15IAD"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-07-29T06:39:25Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-07-29T06:39:25Z</b><br><br>######################################################################################################<br>#<br>#<br>SYSTEM INFO : quocld01<br>#<br>######################################################################################################<br><br><br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: Removing all quocld02-HB attributes for attrd_peer_change_cb<br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: Lost attribute writer quocld02-HB<br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:53 quocld01 attrd[13889]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld01 attrd[13889]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:53 quocld01 cib[13885]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:53 quocld01 cib[13885]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:53 quocld01 stonith-ng[13887]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:53 quocld01 crmd[13891]:  notice: Our peer on the DC (quocld02-HB) is dead<br>Jul 28 14:51:53 quocld01 stonith-ng[13887]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:53 quocld01 stonith-ng[13887]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:53 quocld01 crmd[13891]:  notice: State transition S_NOT_DC -&gt; S_ELECTION [ input=I_ELECTION cause=C_CRMD_STATUS_CALLBACK origin=peer_update_callback ]<br>Jul 28 14:51:53 quocld01 crmd[13891]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Jul 28 14:51:53 quocld01 stonith-ng[13887]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld01 stonith-ng[13887]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld01 cib[13885]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld01 crmd[13891]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Jul 28 14:51:53 quocld01 crmd[13891]:  notice: Notifications disabled<br>Jul 28 14:51:54 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:54 quocld01 cib[13885]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:54 quocld01 cib[13885]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:54 quocld01 cib[13885]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:54 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:56 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:56 quocld01 cib[13885]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:56 quocld01 cib[13885]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:56 quocld01 cib[13885]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:56 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:56 quocld01 pengine[13890]: warning: Node quocld02-HB will be fenced because our peer process is no longer available<br>Jul 28 14:51:56 quocld01 pengine[13890]: warning: Node quocld02-HB is unclean<br>Jul 28 14:51:56 quocld01 pengine[13890]: warning: Scheduling Node quocld02-HB for STONITH<br>Jul 28 14:51:56 quocld01 pengine[13890]:  notice: Stop    rsc_ping:0#011(quocld02-HB)<br>Jul 28 14:51:56 quocld01 pengine[13890]:  notice: Stop    ipmilan_stonith1#011(quocld02-HB)<br>Jul 28 14:51:56 quocld01 pengine[13890]:  notice: Move    rsc_vip2#011(Started quocld02-HB -&gt; quocld01-HB)<br>Jul 28 14:51:56 quocld01 pengine[13890]: warning: Calculated Transition 0: /var/lib/pacemaker/pengine/pe-warn-1.bz2<br>Jul 28 14:51:56 quocld01 crmd[13891]:  notice: Executing reboot fencing operation (39) on quocld02-HB (timeout=60000)<br>Jul 28 14:51:56 quocld01 stonith-ng[13887]:  notice: Client crmd.13891.754de807 wants to fence (reboot) 'quocld02-HB' with device '(any)'<br>Jul 28 14:51:56 quocld01 stonith-ng[13887]:  notice: Initiating remote operation reboot for quocld02-HB: ba8f01af-a351-49ce-ba3a-2a47aa958fa8 (0)<br>Jul 28 14:51:56 quocld01 stonith-ng[13887]:  notice: kdump_stonith can fence (reboot) quocld02-HB: static-list<br>Jul 28 14:51:56 quocld01 stonith-ng[13887]:  notice: ipmilan_stonith2 can fence (reboot) quocld02-HB: static-list<br>Jul 28 14:51:56 quocld01 stonith-ng[13887]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Jul 28 14:51:56 quocld01 fence_kdump[27044]: waiting for message from '192.169.25.82'<br>Jul 28 14:51:58 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:51:58 quocld01 cib[13885]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:51:58 quocld01 cib[13885]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:51:58 quocld01 cib[13885]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:58 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:52:01 quocld01 crmd[13891]:  notice: Transition aborted: Peer Halt (source=do_te_invoke:158, 0)<br>Jul 28 14:52:02 quocld01 crmd[13891]:  notice: Transition aborted: Node join (source=do_dc_join_offer_one:245, 0)<br>Jul 28 14:52:06 quocld01 cib[13885]:  notice: Local CIB 0.96.47.bf6caedc24af25a60f46fdf8f3bc966e differs from quocld02-HB: 0.96.5.ef38929867e20c93ece4444ee6e3868b 0xa049e0<br>Jul 28 14:52:56 quocld01 fence_kdump[27044]: timeout after 60 seconds<br>Jul 28 14:52:57 quocld01 fence_kdump[27415]: waiting for message from '192.169.25.82'<br>Jul 28 14:53:26 quocld01 stonith-ng[13887]:  notice: Child process 27415 performing action 'off' timed out with signal 15<br>Jul 28 14:53:26 quocld01 stonith-ng[13887]:   error: Operation 'reboot' [27415] (call 2 from crmd.13891) for host 'quocld02-HB' with device 'kdump_stonith' returned: -62 (Timer expired)<br>Jul 28 14:53:26 quocld01 stonith-ng[13887]:  notice: Call to kdump_stonith for quocld02-HB on behalf of crmd.13891@quocld01-HB: Timer expired (-62)<br>Jul 28 14:53:27 quocld01 crmd[13891]: warning: No match for shutdown action on 1<br>Jul 28 14:53:35 quocld01 corosync[13868]: [TOTEM ] A processor failed, forming new configuration.<br>Jul 28 14:53:39 quocld01 stonith-ng[13887]:  notice: Operation 'reboot' [27610] (call 2 from crmd.13891) for host 'quocld02-HB' with device 'ipmilan_stonith2' returned: 0 (OK)<br>Jul 28 14:53:41 quocld01 attrd[13889]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:53:41 quocld01 cib[13885]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:53:41 quocld01 attrd[13889]:  notice: Removing all quocld02-HB attributes for attrd_peer_change_cb<br>Jul 28 14:53:41 quocld01 cib[13885]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:53:41 quocld01 attrd[13889]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:53:41 quocld01 attrd[13889]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:53:41 quocld01 cib[13885]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:53:41 quocld01 stonith-ng[13887]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:53:41 quocld01 stonith-ng[13887]:  notice: Removing quocld02-HB/2 from the membership list<br>Jul 28 14:53:41 quocld01 stonith-ng[13887]:  notice: Purged 1 peers with id=2 and/or uname=quocld02-HB from the membership cache<br>Jul 28 14:53:41 quocld01 pacemakerd[13883]:  notice: crm_reap_unseen_nodes: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:53:41 quocld01 stonith-ng[13887]:  notice: Call to ipmilan_stonith2 for quocld02-HB on behalf of crmd.13891@quocld01-HB: OK (0)<br>Jul 28 14:53:41 quocld01 crmd[13891]:   error: We have more confirmed nodes than our membership does: 2 vs. 1<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: crm_reap_unseen_nodes: Node quocld02-HB[2] - state is now lost (was member)<br>Jul 28 14:53:41 quocld01 corosync[13868]: [TOTEM ] A new membership (192.169.25.81:64) was formed. Members left: 2<br>Jul 28 14:53:41 quocld01 corosync[13868]: [TOTEM ] Failed to receive the leave message. failed: 2<br>Jul 28 14:53:41 quocld01 corosync[13868]: [QUORUM] Members[1]: 1<br>Jul 28 14:53:41 quocld01 corosync[13868]: [MAIN  ] Completed service synchronization, ready to provide service.<br>Jul 28 14:53:41 quocld01 stonith-ng[13887]:  notice: Operation reboot of quocld02-HB by quocld01-HB for crmd.13891@quocld01-HB.ba8f01af: OK<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: Stonith operation 2/39:0:0:16326a63-1ce6-4f99-ac6f-2f9d5c89c853: OK (0)<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: Peer quocld02-HB was terminated (reboot) by quocld01-HB for quocld01-HB: OK (ref=ba8f01af-a351-49ce-ba3a-2a47aa958fa8) by client crmd.13891<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: Transition 0 (Complete=11, Pending=0, Fired=0, Skipped=1, Incomplete=3, Source=/var/lib/pacemaker/pengine/pe-warn-1.bz2): Stopped<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Jul 28 14:53:41 quocld01 crmd[13891]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Jul 28 14:53:41 quocld01 crmd[13891]:  notice: Notifications disabled<br>Jul 28 14:53:44 quocld01 pengine[13890]:  notice: Start   rsc_vip2#011(quocld01-HB)<br>Jul 28 14:53:44 quocld01 pengine[13890]:  notice: Calculated Transition 1: /var/lib/pacemaker/pengine/pe-input-6.bz2<br>Jul 28 14:53:44 quocld01 crmd[13891]:  notice: Initiating action 26: start rsc_vip2_start_0 on quocld01-HB (local)<br>Jul 28 14:53:45 quocld01 IPaddr2(rsc_vip2)[27806]: INFO: Adding inet address 42.8.231.84/24 with broadcast address 42.8.231.255 to device bond0<br>Jul 28 14:53:45 quocld01 IPaddr2(rsc_vip2)[27806]: INFO: Bringing device bond0 up<br>Jul 28 14:53:45 quocld01 IPaddr2(rsc_vip2)[27806]: INFO: /usr/libexec/heartbeat/send_arp -i 200 -r 5 -p /var/run/resource-agents/send_arp-42.8.231.84 bond0 42.8.231.84 auto not_used not_used<br>Jul 28 14:53:45 quocld01 crmd[13891]:  notice: Operation rsc_vip2_start_0: ok (node=quocld01-HB, call=40, rc=0, cib-update=99, confirmed=true)<br>Jul 28 14:53:45 quocld01 crmd[13891]:  notice: Initiating action 27: monitor rsc_vip2_monitor_10000 on quocld01-HB (local)<br>Jul 28 14:53:45 quocld01 crmd[13891]:  notice: Transition 1 (Complete=4, Pending=0, Fired=0, Skipped=0, Incomplete=0, Source=/var/lib/pacemaker/pengine/pe-input-6.bz2): Complete<br>Jul 28 14:53:45 quocld01 crmd[13891]:  notice: State transition S_TRANSITION_ENGINE -&gt; S_IDLE [ input=I_TE_SUCCESS cause=C_FSA_INTERNAL origin=notify_crmd ]<br>Jul 28 14:53:46 quocld01 ntpd[3376]: Listen normally on 8 bond0 42.8.231.84 UDP 123<br><br> <br><br> <br><br>[root@quocld01:~]# ls -al  /var/lib/pacemaker/<br>total 12<br>drwxr-x---   6 hacluster haclient   57 Jul  8 15:11 .<br>drwxr-xr-x. 75 root      root     4096 Jul 29 03:06 ..<br>drwxr-x---   2 hacluster haclient    6 Oct  9  2015 blackbox<br>drwxr-x---   2 hacluster haclient 4096 Jul 28 15:12 cib<br>drwxr-x---   2 hacluster haclient    6 Oct  9  2015 cores<br>drwxr-x---   2 hacluster haclient 4096 Jul 28 15:12 pengine<br>[root@quocld01:~]# ls -al  /var/lib/pacemaker/cib/<br>total 552<br>drwxr-x--- 2 hacluster haclient  4096 Jul 28 15:12 .<br>drwxr-x--- 6 hacluster haclient    57 Jul  8 15:11 ..<br>-rw------- 1 hacluster haclient 10035 Jul 12 14:27 cib-10.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-10.raw.sig<br>-rw------- 1 hacluster haclient 10153 Jul 12 14:27 cib-11.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-11.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 12 14:27 cib-12.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:27 cib-12.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 12 16:02 cib-13.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 16:02 cib-13.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 12 16:02 cib-14.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 16:02 cib-14.raw.sig<br>-rw------- 1 hacluster haclient 10023 Jul 14 10:44 cib-15.raw<br>-rw------- 1 hacluster haclient    32 Jul 14 10:44 cib-15.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 14 10:44 cib-16.raw<br>-rw------- 1 hacluster haclient    32 Jul 14 10:44 cib-16.raw.sig<br>-rw------- 1 hacluster haclient 10035 Jul 14 10:44 cib-17.raw<br>-rw------- 1 hacluster haclient    32 Jul 14 10:44 cib-17.raw.sig<br>-rw------- 1 hacluster haclient 10023 Jul 15 17:48 cib-18.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:48 cib-18.raw.sig<br>-rw------- 1 hacluster haclient 10023 Jul 15 17:49 cib-19.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:49 cib-19.raw.sig<br>-rw------- 1 hacluster haclient   258 Jul 12 14:18 cib-1.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:18 cib-1.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:51 cib-20.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:51 cib-20.raw.sig<br>-rw------- 1 hacluster haclient 10025 Jul 15 17:52 cib-21.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:52 cib-21.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:52 cib-22.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:52 cib-22.raw.sig<br>-rw------- 1 hacluster haclient 10025 Jul 15 17:53 cib-23.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:53 cib-23.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 17:53 cib-24.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 17:53 cib-24.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 15 18:49 cib-25.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 18:49 cib-25.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 18:50 cib-26.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 18:50 cib-26.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 18:50 cib-27.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 18:50 cib-27.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 19:06 cib-28.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 19:06 cib-28.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 19:22 cib-29.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 19:22 cib-29.raw.sig<br>-rw------- 1 hacluster haclient   425 Jul 12 14:18 cib-2.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:18 cib-2.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 15 19:22 cib-30.raw<br>-rw------- 1 hacluster haclient    32 Jul 15 19:22 cib-30.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 21 14:57 cib-31.raw<br>-rw------- 1 hacluster haclient    32 Jul 21 14:57 cib-31.raw.sig<br>-rw------- 1 hacluster haclient  9895 Jul 21 14:58 cib-32.raw<br>-rw------- 1 hacluster haclient    32 Jul 21 14:58 cib-32.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:51 cib-33.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:51 cib-33.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:52 cib-34.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:52 cib-34.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 14:53 cib-35.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 14:53 cib-35.raw.sig<br>-rw------- 1 hacluster haclient  9907 Jul 28 15:12 cib-36.raw<br>-rw------- 1 hacluster haclient    32 Jul 28 15:12 cib-36.raw.sig<br>-rw------- 1 hacluster haclient   622 Jul 12 14:19 cib-3.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-3.raw.sig<br>-rw------- 1 hacluster haclient   726 Jul 12 14:19 cib-4.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-4.raw.sig<br>-rw------- 1 hacluster haclient   940 Jul 12 14:19 cib-5.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-5.raw.sig<br>-rw------- 1 hacluster haclient   940 Jul 12 14:19 cib-6.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:19 cib-6.raw.sig<br>-rw------- 1 hacluster haclient 10019 Jul 12 14:21 cib-7.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:21 cib-7.raw.sig<br>-rw------- 1 hacluster haclient 10019 Jul 12 14:21 cib-8.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:21 cib-8.raw.sig<br>-rw------- 1 hacluster haclient 10153 Jul 12 14:26 cib-9.raw<br>-rw------- 1 hacluster haclient    32 Jul 12 14:26 cib-9.raw.sig<br>-rw-r----- 1 hacluster haclient     2 Jul 28 15:12 cib.last<br>-rw------- 1 hacluster haclient  9907 Jul 28 15:12 cib.xml<br>-rw------- 1 hacluster haclient    32 Jul 28 15:12 cib.xml.sig<br>[root@quocld01:~]#<br><br> <br><br>######################################################################################################<br>#<br>#<br>SYSTEM INFO : quocld02<br>#<br>######################################################################################################<br><br>Jul 28 14:50:46 quocld02 dbus-daemon: dbus[3338]: [system] Successfully activated service 'org.freedesktop.problems'<br>Jul 28 14:51:53 quocld02 attrd[13716]:   error: Cannot get password entry of uid: 0: Success (0)<br>Jul 28 14:51:53 quocld02 attrd[13716]:   error: attrd_ipc_dispatch: Triggered fatal assert at main.c:220 : client-&gt;user != NULL<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:   error: The attrd process (13716) terminated with signal 6 (core=0)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: attrd<br>Jul 28 14:51:53 quocld02 attrd[778]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:53 quocld02 attrd[778]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 attrd[778]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:53 quocld02 attrd[778]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld02 attrd[778]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld02 cib[13713]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 cib[13713]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm failed<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm[0x139c270] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw failed<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw[0x147aa20] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:   error: The cib process (13713) terminated with signal 6 (core=0)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to the CIB terminated...<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_ERROR from crmd_cib_connection_destroy() received in state S_IDLE<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: State transition S_IDLE -&gt; S_RECOVERY [ input=I_ERROR cause=C_FSA_INTERNAL origin=crmd_cib_connection_destroy ]<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Fast-tracking shutdown in response to errors<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Not voting in election, we're in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:  notice: Connection to the CIB terminated. Shutting down.<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng failed<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng[0x1a67e80] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: LRMD lost STONITH connection<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_TERMINATE from do_recover() received in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: STONITH connection failed, finalizing 1 pending operations.<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: Stopped 1 recurring operations at shutdown (2 ops remaining)<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: Recurring action rsc_ping:34 (rsc_ping_monitor_10000) incomplete at shutdown<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: Recurring action rsc_vip2:36 (rsc_vip2_monitor_10000) incomplete at shutdown<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: 3 resources were active at shutdown.<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: Disconnected from the LRM<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: Disconnecting from Corosync<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Could not recover from internal error<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: stonith-ng<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:   error: The crmd process (13718) exited: Generic Pacemaker error (201)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: crmd<br>Jul 28 14:51:53 quocld02 cib[779]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:53 quocld02 cib[779]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 cib[779]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:53 quocld02 cib[779]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br>Jul 28 14:51:53 quocld02 stonith-ng[780]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:53 quocld02 stonith-ng[780]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:53 quocld02 crmd[781]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:53 quocld02 crmd[781]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 crmd[781]:  notice: CRM Git Version: 1.1.13-10.el7 (44eb2dd)<br>Jul 28 14:51:53 quocld02 crmd[781]:   error: /var/lib/pacemaker/pengine must be owned and r/w by user hacluster<br>Jul 28 14:51:53 quocld02 crmd[781]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:53 quocld02 stonith-ng[780]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld02 cib[779]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:53 quocld02 stonith-ng[780]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld02 cib[779]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:53 quocld02 cib[779]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:53 quocld02 cib[779]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:51:53 quocld02 cib[779]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:51:54 quocld02 cib[779]:   error: Cluster user hacluster does not exist<br>Jul 28 14:51:54 quocld02 cib[779]: warning: Could not find group for user hacluster<br>Jul 28 14:51:54 quocld02 cib[779]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:54 quocld02 cib[779]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:54 quocld02 pacemakerd[13711]:   error: The cib process (779) terminated with signal 6 (core=0)<br>Jul 28 14:51:54 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:54 quocld02 cib[786]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:54 quocld02 cib[786]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:54 quocld02 cib[786]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:54 quocld02 cib[786]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br>Jul 28 14:51:54 quocld02 cib[786]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:54 quocld02 cib[786]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:54 quocld02 cib[786]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br><br>Jul 28 14:51:54 quocld02 cib[786]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:51:54 quocld02 cib[786]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:51:55 quocld02 attrd[778]:  notice: Connection to cib_rw closed: Transport endpoint is not connected (-107)<br>Jul 28 14:51:56 quocld02 cib[786]:   error: Cluster user hacluster does not exist<br>Jul 28 14:51:56 quocld02 cib[786]: warning: Could not find group for user hacluster<br>Jul 28 14:51:56 quocld02 cib[786]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:56 quocld02 cib[786]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:56 quocld02 pacemakerd[13711]:   error: The cib process (786) terminated with signal 6 (core=0)<br>Jul 28 14:51:56 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:56 quocld02 cib[788]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:56 quocld02 cib[788]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:56 quocld02 cib[788]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br>Jul 28 14:51:56 quocld02 crmd[781]:  notice: Connection to cib_shm closed: Transport endpoint is not connected (-107)<br>Jul 28 14:51:56 quocld02 crmd[781]: warning: Couldn't complete CIB registration 1 times... pause and retry<br>Jul 28 14:51:56 quocld02 cib[788]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:56 quocld02 cib[788]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:56 quocld02 cib[788]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:56 quocld02 cib[788]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:51:56 quocld02 cib[788]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:51:58 quocld02 attrd[778]:  notice: Connection to cib_rw closed: Transport endpoint is not connected (-107)<br>Jul 28 14:51:58 quocld02 cib[788]:   error: Cluster user hacluster does not exist<br>Jul 28 14:51:58 quocld02 cib[788]: warning: Could not find group for user hacluster<br>Jul 28 14:51:58 quocld02 cib[788]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:58 quocld02 cib[788]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:58 quocld02 pacemakerd[13711]:   error: The cib process (788) terminated with signal 6 (core=0)<br>Jul 28 14:51:58 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:58 quocld02 cib[795]:  notice: Additional logging available in /var/log/cluster/corosync.log<br>Jul 28 14:51:58 quocld02 cib[795]:   error: Cannot get name for uid: 189: Success (0)<br>Jul 28 14:51:58 quocld02 cib[795]:   error: /var/lib/pacemaker/cib must be owned and r/w by user hacluster<br>Jul 28 14:51:58 quocld02 cib[795]:   error: /var/lib/pacemaker/cib/cib.xml must be owned and r/w by user hacluster<br>Jul 28 14:51:58 quocld02 cib[795]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:51:58 quocld02 cib[795]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:51:58 quocld02 cib[795]:  notice: crm_update_peer_proc: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:51:58 quocld02 cib[795]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:51:58 quocld02 cib[795]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:51:59 quocld02 stonith-ng[780]:  notice: Watching for stonith topology changes<br>Jul 28 14:51:59 quocld02 stonith-ng[780]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:51:59 quocld02 stonith-ng[780]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:52:00 quocld02 attrd[778]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:52:00 quocld02 attrd[778]:  notice: crm_update_peer_proc: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:52:00 quocld02 attrd[778]:  notice: Processing sync-response from quocld01-HB<br>Jul 28 14:52:00 quocld02 crmd[781]:  notice: Connection to cib_shm closed: Transport endpoint is not connected (-107)<br>Jul 28 14:52:00 quocld02 stonith-ng[780]:  notice: Added 'kdump_stonith' to the device list (1 active devices)<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: Connecting to cluster infrastructure: corosync<br>Jul 28 14:52:01 quocld02 crmd[781]: warning: Node names with capitals are discouraged, consider changing 'quocld02-HB' to something else<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: Quorum acquired<br>Jul 28 14:52:01 quocld02 stonith-ng[780]:  notice: Added 'ipmilan_stonith1' to the device list (2 active devices)<br>Jul 28 14:52:01 quocld02 crmd[781]: warning: Node names with capitals are discouraged, consider changing 'quocld01-HB' to something else<br>Jul 28 14:52:01 quocld02 stonith-ng[780]:  notice: kdump_stonith can fence (reboot) quocld02-HB: static-list<br>Jul 28 14:52:01 quocld02 stonith-ng[780]:  notice: ipmilan_stonith1 can not fence (reboot) quocld02-HB: static-list<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: Notifications disabled<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: pcmk_quorum_notification: Node quocld01-HB[1] - state is now member (was (null))<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: pcmk_quorum_notification: Node quocld02-HB[2] - state is now member (was (null))<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: The local CRM is operational<br>Jul 28 14:52:01 quocld02 crmd[781]:  notice: State transition S_STARTING -&gt; S_PENDING [ input=I_PENDING cause=C_FSA_INTERNAL origin=do_started ]<br>Jul 28 14:52:06 quocld02 attrd[778]:  notice: Updating all attributes after cib_refresh_notify event<br>Jul 28 14:52:31 quocld02 crmd[781]:  notice: State transition S_PENDING -&gt; S_NOT_DC [ input=I_NOT_DC cause=C_HA_MESSAGE origin=do_cl_join_finalize_respond ]<br>Jul 28 14:53:27 quocld02 systemd-logind: Power key pressed.<br>Jul 28 14:53:27 quocld02 systemd-logind: Powering Off...<br>Jul 28 14:53:27 quocld02 systemd-logind: System is powering down.<br><br> <br><br><br>[root@quocld02:~]#ls -al  /var/lib/pacemaker/<br>total 12<br>drwxr-x---   6 hacluster haclient   57 Jul  8 15:11 .<br>drwxr-xr-x. 75 root      root     4096 Jul 29 03:12 ..<br>drwxr-x---   2 hacluster haclient    6 Oct  9  2015 blackbox<br>drwxr-x---   2 hacluster haclient 4096 Jul 28 15:10 cib<br>drwxr-x---   2 hacluster haclient    6 Oct  9  20<br><br><publishedDate>2016-07-29T06:39:25Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>관련하여 원인분석이 필요합니다.<br><br>Host Name : quocld02 ( quocld02-HB )<br>Rebooted Time : Jul 28 14:53<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version : <br>======================<br>corosync-2.3.4-7.el7_2.1.x86_64<br>======================<br>pacemaker-1.1.13-10.el7.x86_64<br>======================<br>pcs-0.9.143-15.el7.x86_64<br>======================<br>fence-agents-all-4.0.11-27.el7.x86_64<br>======================<br><br><br><br>HA 노드중 리부팅되어진 quocld02 시스템의 특이한 로그는 하기와 같습니다.<br><br>Jul 28 14:51:53 quocld02 cib[13713]:   error: Cannot get password entry of uid: 189: Success (0)<br>Jul 28 14:51:53 quocld02 cib[13713]:   error: cib_common_callback: Triggered fatal assert at callbacks.c:289 : cib_client-&gt;user != NULL<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm failed<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to cib_shm[0x139c270] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw failed<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:   error: Connection to cib_rw[0x147aa20] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:   error: The cib process (13713) terminated with signal 6 (core=0)<br>Jul 28 14:51:53 quocld02 pacemakerd[13711]:  notice: Respawning failed child process: cib<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: Connection to the CIB terminated...<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_ERROR from crmd_cib_connection_destroy() received in state S_IDLE<br>Jul 28 14:51:53 quocld02 crmd[13718]:  notice: State transition S_IDLE -&gt; S_RECOVERY [ input=I_ERROR cause=C_FSA_INTERNAL origin=crmd_cib_connection_destroy ]<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Fast-tracking shutdown in response to errors<br>Jul 28 14:51:53 quocld02 crmd[13718]: warning: Not voting in election, we're in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 stonith-ng[13714]:  notice: Connection to the CIB terminated. Shutting down.<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng failed<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: Connection to stonith-ng[0x1a67e80] closed (I/O condition=17)<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: LRMD lost STONITH connection<br>Jul 28 14:51:53 quocld02 crmd[13718]:   error: FSA: Input I_TERMINATE from do_recover() received in state S_RECOVERY<br>Jul 28 14:51:53 quocld02 lrmd[13715]:   error: STONITH connection failed, finalizing 1 pending operations.</issue><cep>false</cep><folderName>ERP-SAP-ASCS_ERS</folderName></case>