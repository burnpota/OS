======================<br><b>생성계정 : support os</b><br><b>생성날짜 : 2019-02-26T04:10:45Z</b><br><b>마지막 답변자 : Xingbin Li</b><br><b>마지막 수정 일자 : 2019-03-08T03:27:07Z</b><br><b>id : 5002K00000dM4QoQAK</b><br>======================<br><br><b><font size=15>
제목  : Ping이 빠지거나 핑이 가지 않는 현상- NW 이슈가 발생
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하십니까 타임게이트 오선우입니다<br>타임게으트 / 오선우 / 수석보 / 010-4630-3171 / sw.oh@time-gate.com<br><br>삼성화재 고객사에 ping이 정상적으로 가지 않고 빠지는 증상이 있어 분석 요청드립니다<br><br>제가 1차적으로 봤을때 특이점을 발견하지 못하였습니다<br><br>OS설정 또는 sosreport 내용상에 특이점이나 이슈될만한 부분이 있는지 분석 부탁드립니다<br><br>하기 내용은 고객이 문의한 내용입니다 함께 참고 부탁드립니다<br><br>대상 장비 sosreport 첨부드립니다<br><br>감사합니다<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.4</b><br><b>타입  : RCA Only</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 3 (Normal)</b><br><hostname>mwb2bp01~04</hostname><br><br><br><comment id="a0a2K00000PWaJOQA1"><br>======================<br><b>생성계정 : Li, Xingbin</b><br><b>생성날짜 : 2019-03-08T03:26:44Z</b><br><b>마지막 답변자 : Li, Xingbin</b><br><b>마지막 수정 일자 : 2019-03-08T03:26:44Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service입니다.<br><br>본 케이스를 종료처리 하고자 합니다.<br>만약 본 케이스와 관련하여 추가 질문이 있으시다면 언제든지 재오픈하실 수 있습니다.  <br>  <br>케이스가 처리 완료되면 가능하게 고객 설문조사 메일이 발생됩니다. <br>고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다. <br>향후 기술 지원 서비스의 품질 향상을 위해, 소중한 시간을 내어 주시면 대단히 감사드리겠습니다.  <br>  <br>감사합니다.<br><br><publishedDate>2019-03-08T03:26:44Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PRDxIQAX"><br>======================<br><b>생성계정 : Li, Xingbin</b><br><b>생성날짜 : 2019-03-04T07:57:17Z</b><br><b>마지막 답변자 : Li, Xingbin</b><br><b>마지막 수정 일자 : 2019-03-04T07:57:17Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 입니다.<br><br>본 케이스와 연관되어 추가적으로 문의하실 사항이 있으신가요?<br><br>본 안내 후 일정기간 답변이 없다면 본 케이스는 자동으로 종료 상태가 됨을 알려 드립니다.<br><br>만약 추가적인 지원이 필요하시다면, 연락 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2019-03-04T07:57:17Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PQDRvQAP"><br>======================<br><b>생성계정 : Li, Xingbin</b><br><b>생성날짜 : 2019-02-27T02:30:03Z</b><br><b>마지막 답변자 : Li, Xingbin</b><br><b>마지막 수정 일자 : 2019-02-27T02:30:03Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>sosreport를 확인해본 바에 의하면 OS 단에서는 특이한 사항은 발견할 수 없었습니다.<br><br> - mwb2bp01~mwb2bp04 서버 모두 failover 작동에 문제점을 발견할 수 없었습니다.<br>   아래는 각 서버의 마지막 재부팅후에 진행한 failover 로그입니다. <br>   01~03은 마지막 재부팅후에 failover 테스트를 진행하신 것으로 보이며, <br>   04는 failover 테스트를 진행하시지 않은 것으로 보입니다. <br>   따라서 04번 서버는 재부팅전 failover 로그를 발췌하였습니다.<br>~~~~<br>[mwb2bp01]<br><br>Feb 25 20:30:57 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:30:57 mwb2bp01 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:30:57 mwb2bp01 kernel: bond0: making interface p1p1 the new active one<br>Feb 25 20:31:00 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:31:00 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:31:00 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:31:00 mwb2bp01 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br>Feb 25 20:31:42 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:31:42 mwb2bp01 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:31:45 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:31:45 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:31:45 mwb2bp01 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:31:45 mwb2bp01 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br><br>[mwb2bp02] - failover 작동에 문제가 없어보입니다.<br><br>Feb 25 20:07:19 mwb2bp02 kernel: tg3 0000:12:00.0 p1p1: Link is down<br>Feb 25 20:07:19 mwb2bp02 kernel: bond0: link status definitely down for interface p1p1, disabling it<br>Feb 25 20:07:19 mwb2bp02 kernel: bond0: making interface p3p1 the new active one<br>Feb 25 20:09:10 mwb2bp02 kernel: tg3 0000:12:00.0 p1p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:09:10 mwb2bp02 kernel: tg3 0000:12:00.0 p1p1: Flow control is on for TX and on for RX<br>Feb 25 20:09:10 mwb2bp02 kernel: tg3 0000:12:00.0 p1p1: EEE is disabled<br>Feb 25 20:09:11 mwb2bp02 kernel: bond0: link status definitely up for interface p1p1, 1000 Mbps full duplex<br>Feb 25 20:12:33 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:12:33 mwb2bp02 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:12:33 mwb2bp02 kernel: bond0: making interface p1p1 the new active one<br>Feb 25 20:13:39 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:13:39 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:13:39 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:13:39 mwb2bp02 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br>Feb 25 20:31:38 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:31:38 mwb2bp02 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:31:41 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:31:41 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:31:41 mwb2bp02 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:31:41 mwb2bp02 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br><br>[mwb2bp03]<br><br>Feb 25 20:06:20 mwb2bp03 kernel: tg3 0000:12:00.0 p1p1: Link is down<br>Feb 25 20:06:20 mwb2bp03 kernel: bond0: link status definitely down for interface p1p1, disabling it<br>Feb 25 20:06:20 mwb2bp03 kernel: bond0: making interface p3p1 the new active one<br>Feb 25 20:09:04 mwb2bp03 kernel: tg3 0000:12:00.0 p1p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:09:04 mwb2bp03 kernel: tg3 0000:12:00.0 p1p1: Flow control is on for TX and on for RX<br>Feb 25 20:09:04 mwb2bp03 kernel: tg3 0000:12:00.0 p1p1: EEE is disabled<br>Feb 25 20:09:04 mwb2bp03 kernel: bond0: link status definitely up for interface p1p1, 1000 Mbps full duplex<br>Feb 25 20:31:45 mwb2bp03 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:31:45 mwb2bp03 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:31:45 mwb2bp03 kernel: bond0: making interface p1p1 the new active one<br>Feb 25 20:31:48 mwb2bp03 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:31:48 mwb2bp03 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:31:48 mwb2bp03 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:31:48 mwb2bp03 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br><br>[mwb2bp04] <br><br>Feb 25 19:07:47 mwb2bp04 kernel: bond0: link status definitely down for interface p1p1, disabling it<br>Feb 25 19:07:47 mwb2bp04 kernel: bond0: making interface p3p1 the new active one<br>Feb 25 19:07:52 mwb2bp04 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 19:07:52 mwb2bp04 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 19:07:52 mwb2bp04 kernel: bond0: now running without any active interface!<br>Feb 25 20:31:35 mwb2bp04 kernel: tg3 0000:13:00.0 p3p1: Link is down<br>Feb 25 20:31:35 mwb2bp04 kernel: bond0: link status definitely down for interface p3p1, disabling it<br>Feb 25 20:31:38 mwb2bp04 kernel: tg3 0000:13:00.0 p3p1: Link is up at 1000 Mbps, full duplex<br>Feb 25 20:31:38 mwb2bp04 kernel: tg3 0000:13:00.0 p3p1: Flow control is on for TX and on for RX<br>Feb 25 20:31:38 mwb2bp04 kernel: tg3 0000:13:00.0 p3p1: EEE is disabled<br>Feb 25 20:31:38 mwb2bp04 kernel: bond0: link status definitely up for interface p3p1, 1000 Mbps full duplex<br>~~~~<br><br> - 말씀해주신  Packet Drop 현상도 확인하였습니다, <br>   추가로 failover 중에 패킷 드롭이 발생했는지 확인해볼 필요가 있습니다. <br>~~~~<br>$ cat ../*/proc/net/dev | column -t | grep -e bond -e p3p1 -e p1p1<br><br>face     |bytes         packets      errs      drop  fifo  frame  compressed  multicast|bytes  packets     errs      drop  fifo  colls  carrier  compressed<br>p3p1:    1072922211186  14952995417  0         4     0     0      0           4568249080       82902395    1154373   0     0     0      0        0           0<br>bond0:   2526817928411  35874908984  1         44    0     1      0           9095496732       5155601315  71622067  0     0     0      0        0           0<br>p1p1:    1453895717225  20921913567  1         40    0     1      0           4527247652       5072698920  70467694  0     0     0      0        0           0<br><br>face     |bytes         packets      errs      drop  fifo  frame  compressed  multicast|bytes  packets     errs      drop  fifo  colls  carrier  compressed<br>p3p1:    1100670613163  15335092811  0         13    0     0      0           4697589793       22480661    293658    0     0     0      0        0           0<br>bond0:   2200485120803  30658275346  0         63    0     0      0           9391341802       6602051598  94068437  0     0     0      0        0           0<br>p1p1:    1099814507640  15323182535  0         50    0     0      0           4693752009       6579570937  93774779  0     0     0      0        0           0<br><br>face     |bytes         packets      errs      drop  fifo  frame  compressed  multicast|bytes  packets     errs      drop  fifo  colls  carrier  compressed<br>p3p1:    1104612138338  15393549862  0         15    0     0      0           4705540264       180253231   2534036   0     0     0      0        0           0<br>bond0:   2205850777220  30740048642  0         128   0     0      0           9396748641       5605218602  78354053  0     0     0      0        0           0<br>p1p1:    1101238638882  15346498780  0         113   0     0      0           4691208377       5424965371  75820017  0     0     0      0        0           0<br><br>face     |bytes         packets      errs      drop  fifo  frame  compressed  multicast|bytes  packets     errs      drop  fifo  colls  carrier  compressed<br>p3p1:    1063868043776  14816275245  0         17    0     0      0           4556279689       3584        0         0     0     0      0        0           0<br>bond0:   2255313011602  31625747155  0         249   0     0      0           9112918181       2858462167  35925111  0     0     0      0        0           0<br>p1p1:    1191444967826  16809471910  0         232   0     0      0           4556638492       2858458583  35925111  0     0     0      0        0           0<br>~~~<br><br>OS 단에서는 아직 원인파악이 어려워 이슈 발생시 아래와 같은 데이터 수집이 필요합니다.<br><br>1. failover 테스트 전 패킷 drop 량 확인.<br><br>  $ cat /proc/net/dev | column -t &gt; /tmp/droptest_before<br><br>2. ping test 시작<br><br>  $ ping &lt;gateway&gt;<br><br>3. 다른 터미털 창에서 tcpdump 시작<br><br>  $ tcpdump -n -s 0 -i eth0 -w /tmp/$(hostname)-$(date +&quot;%Y-%m-%d-%H-%M-%S&quot;).pcap<br><br>4. 케이블 절체로 failover 테스트 시작, ping 이 안되는 현상이 발생하면 <br>   ping이 회복되기 까지 기다리신후 &lt;ctrl + c&gt; 키로 tcpdump 중지.<br><br>5. failover 테스트후 패킷 drop 량 확인.<br><br>  $ cat /proc/net/dev | column -t &gt; /tmp/droptest_after<br><br>테스트후 수집된 /tmp/droptest_before, /tmp/droptest_after, /tmp/xxxxx-xxxxxx.pcap파일을 본 케이스에 첨부해주시기 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2019-02-27T02:30:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PPxvRQAT"><br>======================<br><b>생성계정 : Li, Xingbin</b><br><b>생성날짜 : 2019-02-26T08:25:04Z</b><br><b>마지막 답변자 : Li, Xingbin</b><br><b>마지막 수정 일자 : 2019-02-26T08:25:04Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>저는 Platform Support 를 담당하고 있는 Technical Support Engineer 이성빈입니다.<br>올려주신 문의내용은 현재 자세히 살펴보고 있는중이며, 관련된 내용이 확인이 되는대로 업데이트 드리도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2019-02-26T08:25:04Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PPvfKQAT"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2019-02-26T04:45:17Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2019-02-26T04:45:17Z</b><br><br>참고적으로 Link Failure Count: 는 고객이 선을 절체하여 테스트하는 과정에서 나온것 같습니다<br><br>네트워크에 Packet Drop도 발견되었다는 연락을 받았습니다 참고 부탁드립니다<br><br><publishedDate>2019-02-26T04:45:17Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000PPvOzQAL"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2019-02-26T04:12:55Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2019-02-26T04:12:55Z</b><br><br>신규 구축중인 서버의 NW 이슈가 발생하여 확인 요청드립니다. (작업은 2/25월 18시~22시 사이에 진행하였습니다.)<br><br> <br><br>우선 sosreport 는 HP FTP 이트에 업로드 하였습니다.<br><br> <br><br>URL : http://210.116.50.10/ (contract/hpkorea)<br><br>경로 : /SFMI/20190226/서버명(mwb2bp01~04)<br><br> <br><br>현재 상태는 HPE DL380 G10 서버의 1G UTP NIC에 bond0로 NW 구성한 상태이며,<br><br> <br><br>스위치는 아래와 같이 L4 스위치에 직접 연동 되어있습니다.<br><br> <br><br>L4에서 L3의 라우팅 기능까지 하고 있고 방화벽에 연동하기 위해 중간에 L3를 거치는 구조라고 합니다.<br><br> <br><br>서버-bond0(p1p1) - L4_1 - L3_1 - 방화벽<br><br>서버-bond0(p3p1) - L4_2 - L3_2 - 방화벽 <br><br> <br><br>서버의 active 케이블을 절체하면 standby로 넘어가지만 Ping이 빠지거나 핑이 가지 않는 현상이 발생하고 있습니다.<br><br> <br><br>링크는 up이지만 제대로 통신이 안되는것 같은데 NW 상태 점검 요청드립니다.<br><br><publishedDate>2019-02-26T04:12:55Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>