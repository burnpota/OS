======================<br><b>생성계정 : Seung-Pil Kim</b><br><b>생성날짜 : 2016-12-28T04:33:39Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2017-01-28T11:04:17Z</b><br><b>id : 500A000000W7bGmIAJ</b><br>======================<br><br><b><font size=15>
제목  : [삼성화재][보험ERP][운영계][quocld01][HA] pacemaker구성된 시스템에서 서비스 Network(VIP) Fail시 Fail-Over에 관련한 설정값 및 소요시간 계산 방식 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>########## Fail-Over관련  소요시간 계산 방식 ########## <br><br>서비스가 정상화 되기위해서는<br>fail-over되는 클러스터 노드에서 서비스를 시작하는 시간이 필요하므로<br>각각의 상황별로 Fail-Over 되는 시간에, 별도로 서비스 시작 소요시간을 추가하여야 함을 고려하고 있으며,<br><br>아래 상황에서, Q1, Q2, Q3과 관련하여 문의 드립니다.<br><br><br>예)<br>삼성화재 ASCS 서비스 시작 소요시간( 2016.08 기준 ) : 약 30~60초<br><br><br>Q1) 시스템 quocld01, quocld02에 구성된 현재의 pacemaker 설정값으로, pacemaker의 Pingd가 리소스 실패로 판단하는 ICMP ping fail 임계값(횟수나 시간)이 얼마입니까?<br><br>Q2) pingd에서 대상으로 지정된 서비스 네트워크의 Gateway에대하여 Ping Fail시, 서비스 Fail-Over 될 때 소요시간 계산이 맞는지 Q2-1), Q2-2) 문의 드립니다.<br><br><br>        Q2-1) 시스템에 구성된 현재의 설정값으로 서비스 네트워크(VIP) 실패시 Fail-Over 소요시간 계산방식은, 아래의 내용이 맞습니까? 아니면 다른 계산식이 있습니까?<br><br>        [ pingd monitor interval + ( pingd monitor interval * migration-threshold ) ] + ( rsc_ASCS stop timeout * retry ) ] + ( fence_kdump timeout * retry ) + ( fence_ipmilan timeout * retry )<br><br><br>        Q2-2) 앞의 질문 &quot;Q2-1&quot;의 계산방식에따르면, 현재 시스템 구성에따른 Fail-Over 소요시간이 아래와 같이 나오는데 맞습니까?<br><br>        [ 10 + ( 10 * 3 ) ] + [ 60 * 1 ] + [ 90 * 1 ] + [ 60 * 1 ] = MAX 250s( 4min 10s )<br><br>Q3) migration-threshold=3으로 설정되어있는데, 이는 pingd에도 적용 되는지요?<br><br><br>###################################################################<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>####################################################################<br>SOSREPORT REF CASE ID :  https://access.redhat.com/support/cases/#/case/01676390<br><br><br>Host Name : quocld01, quocld02 ( quocld01-HB, quocld02-HB )<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version :<br>                        corosync-2.3.4-7.el7_2.1.x86_64<br>                        pacemaker-1.1.13-10.el7.x86_64<br>                        pcs-0.9.143-15.el7.x86_64<br>                        fence-agents-all-4.0.11-27.el7.x86_64<br><br>[root@quocld01 ~]# pcs config<br>Cluster Name: quoc_cluster<br>Corosync Nodes:<br> quocld01-HB quocld02-HB<br>Pacemaker Nodes:<br> quocld01-HB quocld02-HB<br><br>Resources:<br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.8.231.1<br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.83<br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_ascs1 (class=ocf provider=sfmi type=MySAP)<br>   Attributes: start=/home/qfpadm/mysap_start stop=/home/qfpadm/mysap_stop monitor=/home/qfpadm/mysap_monitor<br>   Operations: start interval=0s timeout=60 (rsc_ascs1-start-interval-0s)<br>               stop interval=0s timeout=60 (rsc_ascs1-stop-interval-0s)<br>               monitor interval=60s enabled=false (rsc_ascs1-monitor-interval-60s)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.84<br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br><br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;quocld01-HB quocld02-HB&quot; pcmk_off_timeout=90<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld01-HB ipaddr=42.8.240.144 login=redhat passwd=quocld01!1 lanplus=on auth=password delay=15<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld02-HB ipaddr=42.8.240.145 login=redhat passwd=quocld02!1 lanplus=on auth=password<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br><br> Node: quocld01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: quocld02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: quocld01-HB (score:10) (id:location-grp1-quocld01-HB-10)<br>  Resource: grp2<br>    Enabled on: quocld02-HB (score:10) (id:location-grp2-quocld02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: quocld01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: quocld02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_vip1<br>    Enabled on: quocld01-HB (score:10) (id:location-rsc_vip1-quocld01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: quocld02-HB (score:10) (id:location-rsc_vip2-quocld02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: quoc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> last-lrm-refresh: 1455954793<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>###################################################################<br>======================<br><br>###################################################################<br>##########             corosync 설정값          ##########<br># cat quocld01/corosync.dump config.totemconfig_reload_in_progress (u8) = 0 internal_configuration.service.0.name (str) = corosync_cmap internal_configuration.service.0.ver (u32) = 0 internal_configuration.service.1.name (str) = corosync_cfg internal_configuration.service.1.ver (u32) = 0 internal_configuration.service.2.name (str) = corosync_cpg internal_configuration.service.2.ver (u32) = 0 internal_configuration.service.3.name (str) = corosync_quorum internal_configuration.service.3.ver (u32) = 0 internal_configuration.service.4.name (str) = corosync_pload internal_configuration.service.4.ver (u32) = 0 internal_configuration.service.5.name (str) = corosync_votequorum internal_configuration.service.5.ver (u32) = 0 logging.logfile (str) = /var/log/cluster/corosync.log logging.to_logfile (str) = yes logging.to_syslog (str) = yes nodelist.local_node_pos (u32) = 0 nodelist.node.0.nodeid (u32) = 1 nodelist.node.0.ring0_addr (str) = quocld01-HB <br>nodelist.node.0.ring1_addr (str) = quocld01 <br>nodelist.node.1.nodeid (u32) = 2 <br>nodelist.node.1.ring0_addr (str) = quocld02-HB <br>nodelist.node.1.ring1_addr (str) = quocld02 <br>quorum.provider (str) = corosync_votequorum <br>quorum.two_node (u8) = 1 <br>runtime.blackbox.dump_flight_data (str) = 1472621105 runtime.blackbox.dump_state (str) = 1472621105 runtime.config.totem.consensus (u32) = 12000 runtime.config.totem.downcheck (u32) = 1000 runtime.config.totem.fail_recv_const (u32) = 2500 runtime.config.totem.heartbeat_failures_allowed (u32) = 0 runtime.config.totem.hold (u32) = 1894 runtime.config.totem.join (u32) = 50 runtime.config.totem.max_messages (u32) = 17 runtime.config.totem.max_network_delay (u32) = 50 runtime.config.totem.merge (u32) = 200 runtime.config.totem.miss_count_const (u32) = 5 runtime.config.totem.rrp_autorecovery_check_timeout (u32) = 1000 runtime.config.totem.rrp_problem_count_mcast_threshold (u32) = 100 runtime.config.totem.rrp_problem_count_threshold (u32) = 10 runtime.config.totem.rrp_problem_count_timeout (u32) = 2000 runtime.config.totem.rrp_token_expired_timeout (u32) = 2380 runtime.config.totem.send_join (u32) = 0 runtime.config.totem.seqno_unchanged_const (u32) = 30 runtime.config.totem.token (u32) = 10000 runtime.config.totem.token_retransmit (u32) = 2380 runtime.config.totem.token_retransmits_before_loss_const (u32) = 4 runtime.config.totem.window_size (u32) = 50 runtime.connections.active (u64) = 9 runtime.connections.attrd:6503:0x7fb3395690d0.client_pid (u32) = 6503 runtime.connections.attrd:6503:0x7fb3395690d0.dispatched (u64) = 550 runtime.connections.attrd:6503:0x7fb3395690d0.flow_control (u32) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.flow_control_count (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.invalid_request (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.name (str) = attrd runtime.connections.attrd:6503:0x7fb3395690d0.overload (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.queue_size (u32) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.recv_retries (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.requests (u64) = 278 runtime.connections.attrd:6503:0x7fb3395690d0.responses (u64) = 2 runtime.connections.attrd:6503:0x7fb3395690d0.send_retries (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.service_id (u32) = 2 runtime.connections.cib:6500:0x7fb33976eae0.client_pid (u32) = 6500 runtime.connections.cib:6500:0x7fb33976eae0.dispatched (u64) = 61 runtime.connections.cib:6500:0x7fb33976eae0.flow_control (u32) = 0 runtime.connections.cib:6500:0x7fb33976eae0.flow_control_count (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.invalid_request (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.name (str) = cib runtime.connections.cib:6500:0x7fb33976eae0.overload (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.queue_size (u32) = 0 runtime.connections.cib:6500:0x7fb33976eae0.recv_retries (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.requests (u64) = 29 runtime.connections.cib:6500:0x7fb33976eae0.responses (u64) = 2 runtime.connections.cib:6500:0x7fb33976eae0.send_retries (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.service_id (u32) = 2 runtime.connections.closed (u64) = 3658 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.client_pid (u32) = 51673 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.dispatched (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control (u32) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control_count (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.invalid_request (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.name (str) = corosync-cmapct runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.overload (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.queue_size (u32) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.recv_retries (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.requests (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.responses (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.send_retries (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.service_id (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.client_pid (u32) = 6505 runtime.connections.crmd:6505:0x7fb33966d160.dispatched (u64) = 33 runtime.connections.crmd:6505:0x7fb33966d160.flow_control (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.flow_control_count (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.invalid_request (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.name (str) = crmd runtime.connections.crmd:6505:0x7fb33966d160.overload (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.queue_size (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.recv_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.requests (u64) = 9 runtime.connections.crmd:6505:0x7fb33966d160.responses (u64) = 2 runtime.connections.crmd:6505:0x7fb33966d160.send_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.service_id (u32) = 2 runtime.connections.crmd:6505:0x7fb33966ec70.client_pid (u32) = 6505 runtime.connections.crmd:6505:0x7fb33966ec70.dispatched (u64) = 1 runtime.connections.crmd:6505:0x7fb33966ec70.flow_control (u32) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.flow_control_count (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.invalid_request (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.name (str) = crmd runtime.connections.crmd:6505:0x7fb33966ec70.overload (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.queue_size (u32) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.recv_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.requests (u64) = 3 runtime.connections.crmd:6505:0x7fb33966ec70.responses (u64) = 3 runtime.connections.crmd:6505:0x7fb33966ec70.send_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.service_id (u32) = 3 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.dispatched (u64) = 9 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb33915f0b0.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.requests (u64) = 9 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.responses (u64) = 2 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.service_id (u32) = 2 runtime.connections.pacemakerd:6496:0x7fb339160eb0.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb339160eb0.dispatched (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb339160eb0.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.requests (u64) = 3 runtime.connections.pacemakerd:6496:0x7fb339160eb0.responses (u64) = 3 runtime.connections.pacemakerd:6496:0x7fb339160eb0.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.service_id (u32) = 3 runtime.connections.pacemakerd:6496:0x7fb339164f60.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb339164f60.dispatched (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb339164f60.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.requests (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339164f60.responses (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339164f60.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.service_id (u32) = 1 runtime.connections.stonithd:6501:0x7fb339166580.client_pid (u32) = 6501 runtime.connections.stonithd:6501:0x7fb339166580.dispatched (u64) = 5 runtime.connections.stonithd:6501:0x7fb339166580.flow_control (u32) = 0 runtime.connections.stonithd:6501:0x7fb339166580.flow_control_count (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.invalid_request (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.name (str) = stonithd runtime.connections.stonithd:6501:0x7fb339166580.overload (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.queue_size (u32) = 0 runtime.connections.stonithd:6501:0x7fb339166580.recv_retries (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.requests (u64) = 4 runtime.connections.stonithd:6501:0x7fb339166580.responses (u64) = 2 runtime.connections.stonithd:6501:0x7fb339166580.send_retries (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.service_id (u32) = 2 runtime.services.cfg.0.rx (u64) = 0 runtime.services.cfg.0.tx (u64) = 0 runtime.services.cfg.1.rx (u64) = 0 runtime.services.cfg.1.tx (u64) = 0 runtime.services.cfg.2.rx (u64) = 0 runtime.services.cfg.2.tx (u64) = 0 runtime.services.cfg.3.rx (u64) = 0 runtime.services.cfg.3.tx (u64) = 0 runtime.services.cfg.service_id (u16) = 1 runtime.services.cmap.0.rx (u64) = 3 runtime.services.cmap.0.tx (u64) = 2 runtime.services.cmap.service_id (u16) = 0 runtime.services.cpg.0.rx (u64) = 5 runtime.services.cpg.0.tx (u64) = 5 runtime.services.cpg.1.rx (u64) = 0 runtime.services.cpg.1.tx (u64) = 0 runtime.services.cpg.2.rx (u64) = 1 runtime.services.cpg.2.tx (u64) = 0 runtime.services.cpg.3.rx (u64) = 657 runtime.services.cpg.3.tx (u64) = 319 runtime.services.cpg.4.rx (u64) = 0 runtime.services.cpg.4.tx (u64) = 0 runtime.services.cpg.5.rx (u64) = 3 runtime.services.cpg.5.tx (u64) = 2 runtime.services.cpg.6.rx (u64) = 0 <br>runtime.services.cpg.6.tx (u64) = 0 <br>runtime.services.cpg.service_id (u16) = 2 runtime.services.pload.0.rx (u64) = 0<br> runtime.services.pload.0.tx (u64) = 0 runtime.services.pload.1.rx (u64) = 0 <br>runtime.services.pload.1.tx (u64) = 0 <br>runtime.services.pload.service_id (u16) = 4 <br>runtime.services.quorum.service_id (u16) = 3 <br>runtime.services.votequorum.0.rx (u64) = 7 <br>runtime.services.votequorum.0.tx (u64) = 4 <br>runtime.services.votequorum.1.rx (u64) = 0 <br>runtime.services.votequorum.1.tx (u64) = 0 <br>runtime.services.votequorum.2.rx (u64) = 0 <br>runtime.services.votequorum.2.tx (u64) = 0 <br>runtime.services.votequorum.3.rx (u64) = 0 <br>runtime.services.votequorum.3.tx (u64) = 0 <br>runtime.services.votequorum.service_id (u16) = 5 <br>runtime.totem.pg.mrp.rrp.0.faulty (u8) = 0 <br>runtime.totem.pg.mrp.rrp.1.faulty (u8) = 0 <br>runtime.totem.pg.mrp.srp.avg_backlog_calc (u32) = 0 <br>runtime.totem.pg.mrp.srp.avg_token_workload (u32) = 0 <br>runtime.totem.pg.mrp.srp.commit_entered (u64) = 2 <br>runtime.totem.pg.mrp.srp.commit_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.consensus_timeouts (u64) = 0 runtime.totem.pg.mrp.srp.continuous_gather (u32) = 0 runtime.totem.pg.mrp.srp.continuous_sendmsg_failures (u32) = 0 runtime.totem.pg.mrp.srp.firewall_enabled_or_nic_failure (u8) = 0 runtime.totem.pg.mrp.srp.gather_entered (u64) = 2 <br>runtime.totem.pg.mrp.srp.gather_token_lost (u64) = 0 runtime.totem.pg.mrp.srp.mcast_retx (u64) = 0 <br>runtime.totem.pg.mrp.srp.mcast_rx (u64) = 747 <br>runtime.totem.pg.mrp.srp.mcast_tx (u64) = 350 <br>runtime.totem.pg.mrp.srp.memb_commit_token_rx (u64) = 4 <br>runtime.totem.pg.mrp.srp.memb_commit_token_tx (u64) = 6 <br>runtime.totem.pg.mrp.srp.memb_join_rx (u64) = 5 runtime.totem.pg.mrp.srp.memb_join_tx (u64) = 2 runtime.totem.pg.mrp.srp.memb_merge_detect_rx (u64) = 1917 runtime.totem.pg.mrp.srp.memb_merge_detect_tx (u64) = 1917 runtime.totem.pg.mrp.srp.members.1.config_version (u64) = 0 <br>runtime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(192.169.25.81) r(1) ip(42.8.231.81) <br>runtime.totem.pg.mrp.srp.members.1.join_count (u32) = 1 runtime.totem.pg.mrp.srp.members.1.status (str) = joined runtime.totem.pg.mrp.srp.members.2.config_version (u64) = 0 <br>runtime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(192.169.25.82) r(1) ip(42.8.231.82) <br>runtime.totem.pg.mrp.srp.members.2.join_count (u32) = 1 <br>runtime.totem.pg.mrp.srp.members.2.status (str) = joined <br>runtime.totem.pg.mrp.srp.mtt_rx_token (u32) = 161 runtime.totem.pg.mrp.srp.operational_entered (u64) = 2 runtime.totem.pg.mrp.srp.operational_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.orf_token_rx (u64) = 20755 runtime.totem.pg.mrp.srp.orf_token_tx (u64) = 2 <br>runtime.totem.pg.mrp.srp.recovery_entered (u64) = 2 runtime.totem.pg.mrp.srp.recovery_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.rx_msg_dropped (u64) = 0 runtime.totem.pg.mrp.srp.token_hold_cancel_rx (u64) = 582 runtime.totem.pg.mrp.srp.token_hold_cancel_tx (u64) = 294 <br>runtime.totem.pg.msg_queue_avail (u32) = 0 runtime.totem.pg.msg_reserved (u32) = 1 <br>runtime.votequorum.ev_barrier (u32) = 2 runtime.votequorum.highest_node_id (u32) = 2 <br>runtime.votequorum.lowest_node_id (u32) = 1 runtime.votequorum.this_node_id (u32) = 1 <br>runtime.votequorum.two_node (u8) = 1 runtime.votequorum.wait_for_all_status (u8) = 0 <br>totem.cluster_name (str) = quoc_cluster totem.interface.0.bindnetaddr (str) = quocld01-HB <br>totem.interface.0.mcastaddr (str) = 239.192.0.180 totem.interface.0.mcastport (u16) = 5405 <br>totem.interface.1.bindnetaddr (str) = quocld01 totem.interface.1.mcastaddr (str) = 239.192.0.181 <br>totem.interface.1.mcastport (u16) = 5405 <br>totem.rrp_mode (str) = passive <br>totem.secauth (str) = off <br>totem.token (u32) = 10000 <br>totem.transport (str) = udpu totem.version (u32) = 2 <br>uidgid.gid.189 (u8) = 1<br>###################################################################<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>타입  : Other</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 4 (Low)</b><br><br><br><folderNumber>74166</folderNumber><br><comment id="a0aA000000Idqd6IAB"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2017-01-13T07:34:21Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2017-01-13T07:46:54Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>문의주신 pingd의 attempt값을 변경할 경우 네트워크의 장애를 감지하는데 걸리는 시간에 대한 부분을 정리드립니다.<br><br>우선 가정은 pingd의 attempts값을 7이라고 설정했을 경우이며 기타 환경은 아래와 같습니다.<br>pingd attemtps : 7<br>pingd timeout : 2초<br>monitor interval : 5초<br><br>위와 같은 가정에서 네트워크 장애가 발생시에 장애를 인지하는데 걸리는 시간은 최악의 경우와 최고의 경우로 나눠질 것 같습니다.<br><br>최악의 경우는 pingd의 attempts 7를 하나의 세트로 가정할 때.. 첫번째 attempt가 성공하게 되면 해당 세트는 성공으로 보게되고 이때가 가장 최악의 경우로 볼 수 잇을 것 같습니다.<br>이경우에 걸리는 시간은 ( attempts - 2 ) + pingd monitor interval time + ( attempts - 1  + pingd timeout )으로<br>(7-2) + 5 + (7-1+2) = 18초<br><br>최고의 경우는 pingd의 첫 attempts에서 장애를 인지한 경우로, (attempts -1 + pingd timeout)이 되며, 해당 시간은 7 - 1  + 2 = 8초 입니다.<br>======================<br>감사합니다.<br><br><publishedDate>2017-01-13T07:34:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Idq62IAB"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2017-01-13T06:17:21Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2017-01-13T06:17:21Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br><br><br>일전에 말씀 드린 timeout 은 op monitor 의 timeout이 아니며 pingd의 파라미터의 <br>timeout입니다. <br><br># pcs resource describe pingd<br>...<br>  attempts: Number of ping attempts, per host, before declaring it dead<br>  timeout: How long, in seconds, to wait before declaring a ping lost<br><br>pingd timeout 디폴트로 2초로 되어 있으며 attempts 3으로 되어 있을 경우 <br><br>[pingd timeout * pingd attempts ] = [2 * 3] = 6초 입니다. <br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2017-01-13T06:17:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Ida1jIAB"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2017-01-12T03:59:55Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2017-01-12T03:59:55Z</b><br><br>자세한  답변 감사드립니다.<br><br>1. 저희 같은 경우 현재 설정이 <br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.8.231.1<br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br><br>이렇게 되어 있어서  attempt default=3  로 계산 했을 경우 <br><br>monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)  이 설정에 의해서 <br>pingd 실패로 인해서 fail 로 감지하는  시간이 <br>[timeout * attempts ] = [60 * 3] = 180 초 = 3분<br><br>attempts 를 10 으로 변경했을 경우<br><br>[timeout * attepmts] = [60 * 10] =600 초 = 10분 <br><br>이 맞는 지요?<br><br><publishedDate>2017-01-12T03:59:55Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000IcpyEIAR"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2017-01-09T06:22:19Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2017-01-09T06:22:19Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br><br>1)<br>네, 맞습니다. 제 테스트 환경에서 attempts를 10으로 설정 하였습니다. <br><br><br>2)<br>migration-threadhold 는 monitor fail에 적용 됩니다. stop 실패시에는 <br>디폴트로 fencing이 발생 하게 되며 start fail 시는 fail-count 가 <br>INFINITY로 됩니다. <br><br>참고 자료:<br>http://clusterlabs.org/doc/en-US/Pacemaker/1.0/html/Pacemaker_Explained/s-failure-migration.html<br><br>~~~<br>There are two exceptions to the migration threshold concept and occur when a resource either fails to start or fails to stop. Start failures cause the failcount to be set to INFINITY and thus always cause the resource to move immediately.<br>~~~<br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2017-01-09T06:22:19Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000IcLkXIAV"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2017-01-05T07:28:16Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2017-01-05T07:28:16Z</b><br><br>답변 감사드립니다.<br><br>1 .계산식에 대해서 다시 한번 문의 드립니다.<br>테스트 해서 보내주신 자료를 보면 <br><br>ㄱ. icmp 비활성화 시점:<br># date; firewall-cmd --direct --add-rule ipv4 filter OUTPUT 0 -p icmp -j DROP<br>Tue Jan  3 02:12:44 EST 2017<br><br><br>ㄴ. ping 실패로 인식 된 시점 <br>ping(ping)[18547]:      2017/01/03_02:13:04 WARNING: 10.66.193.254 is inactive: PING 10.66.193.254 (10.66.193.254) 56(84) bytes of data.<br><br>--- 10.66.193.254 ping statistics ---<br>10 packets transmitted, 0 received, 100% packet loss, time 8999ms<br>Jan 03 02:13:04 [4259] pcmk1      attrd:     info: attrd_peer_update:   Setting pingd[pcmk1]: 1000 -&gt; 0 from pcmk1<br><br><br>ㄷ.  VirtualIP failover 성공된 시점 <br>Jan 03 02:13:12 [4901] dhcp-193-111.pek.redhat.com       crmd:   notice: process_lrm_event:<br>Result of start operation for VirtualIP on pcmk2: 0 (ok) | call=51 key=VirtualIP_start_0 confirmed=true cib-update=104<br><br><br>위에 결과를 보면 아래의 시간이 산출 됩니다.<br><br>ㄱ. icmp 비활성화 시점: Tue Jan  3 02:12:44 EST 2017<br>ㄴ. ping 실패로 인식 된 시점 :Jan 03 02:13:04<br>ㄷ.  VirtualIP failover 성공된 시점 :Jan 03 02:13:12<br><br>icmp 비활성화 시점에서 pingd 실패로 인식된 시점까지 20초가 소요되었습니다.<br><br>[ pingd attempts * pingd timeout ] =20<br>=[ 10 * pingd timeout ] =20<br><br>따라서 attempt 10 로 셋팅하셨으면 계산식에 의해서 timeout 2 가 된다는 뜻인가요?<br><br>2. migration-threadhold 는 리소스 페일이라고 하셨는데<br>저희가 클러스터 테스트에서 서비스 망을 제거하게 되면<br><br>pingd 에서 실패 후 resource 로 등록된 스크립트가 stop 를 실행하게 되는데 stop 에서 실패가 됩니다.<br>이때도 migration-threadhold 가 적용되는지요?<br><br><br>자세한 답변에 감사 드립니다.<br><br><publishedDate>2017-01-05T07:28:16Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000IcL7jIAF"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2017-01-05T05:56:36Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2017-01-05T05:56:36Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br><br><br>1)<br>일전에 보내 드린 항목중에 <br>[ pingd attempts * pingd timeout ] 이 맞습니다. <br><br># pcs resource describe pingd 를 보시면 attempts는 <br>ping 시도 횟수이며 timeout는 매개 ping이 timeout 시간을 대기 하고 <br>만약 시간을 초과 하였다면 실패로 판단 합니다. <br><br>  attempts: Number of ping attempts, per host, before declaring it dead<br>  timeout: How long, in seconds, to wait before declaring a ping lost<br><br><br>2)<br>migration-threshold 는 리소스 fail이 발생 할시에 적용 됩니다. 만약 리소스가 <br>fail 되었다면 DC 노드에서 해당 리소스에 대한 failcount가 증가 되었다는 로그가 <br>발생 하게 됩니다. <br>pingd 같은 경우에는 ping이 실패 되지만 리소스 자체가 fail 되지 않고 score만 <br>변하게 되어 migration-threshold 설정이 적용 되지 않습니다. <br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2017-01-05T05:56:36Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Ic2x8IAB"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2017-01-04T02:07:15Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2017-01-04T02:07:15Z</b><br><br>감사합니다. 몇가지 궁금하게 있어서 다시 질문 드립니다.<br><br>1. pingd 계산식 <br><br>pingd Time 계산식이<br><br>Time &lt; [pingd monitor interval] + [ pingd attempts * pingd timeout ] + VirtualIP stop + VirtualIP start<br><br>항목 중에  [ pingd attempts * pingd timeout ] 이 부분이  [ pingd attempts + pingd timeout ] 이 아닌지요?<br><br>2. migration threshold 적용시점<br><br>&quot;migration-threshold 는 pingd에 적용 되지 않습니다.  ping 이 실패 될시 <br>pingd 리소스가 실패되는게 아니라 resource score 만 변하기 때문입니다.&quot;<br><br>이렇게 답변을 받았는데 그렇다면 migration-threshold 가 적용되는 상황은 언제인가요?<br>만약 임의로 발생 시킬경우 migration-threshold 가 적용되었다는 것을 로그나 status 에서 확인 할수 있는 방법은<br>무엇인가요?<br>======================<br><br><br>(Zheng, Meiyan에 회신)<br>&gt; 안녕하세요,<br>&gt; <br>&gt; Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br>&gt; <br>&gt; <br>&gt; Q1)<br>&gt; pingd 가 리소스 실패로 판단하는 ICMP ping fail 임계값은 pingd 의 <br>&gt; 파라미터 attempts로 지정 할수 있습니다. <br>&gt; <br>&gt; attempts의 디폴트 값은 3입니다. <br>&gt; <br>&gt; <br>&gt; 2) <br>&gt; ping 실패시 VirtualIP가 failover 소요 시간은 아래와 같이 산정 할수 <br>&gt; 있습니다. <br>&gt; <br>&gt; Time &lt; [pingd monitor interval] + [ pingd attempts * pingd timeout ]<br>&gt;        + VirtualIP stop + VirtualIP start<br>&gt;   <br>&gt; <br>&gt; 테스트 결과:<br>&gt; ** 제 테스트 환경에서 attempts를 10으로 설정 하였습니다. <br>&gt; <br>&gt; ㄱ. icmp 비활성화 시점:<br>&gt; # date; firewall-cmd --direct --add-rule ipv4 filter OUTPUT 0 -p icmp -j DROP<br>&gt; Tue Jan  3 02:12:44 EST 2017<br>&gt; <br>&gt; <br>&gt; ㄴ. ping 실패로 인식 된 시점 <br>&gt; ping(ping)[18547]:      2017/01/03_02:13:04 WARNING: 10.66.193.254 is inactive: PING 10.66.193.254 (10.66.193.254) 56(84) bytes of data.<br>&gt; <br>&gt; --- 10.66.193.254 ping statistics ---<br>&gt; 10 packets transmitted, 0 received, 100% packet loss, time 8999ms<br>&gt; Jan 03 02:13:04 [4259] pcmk1      attrd:     info: attrd_peer_update:   Setting pingd[pcmk1]: 1000 -&gt; 0 from pcmk1<br>&gt; <br>&gt; <br>&gt; ㄷ.  VirtualIP failover 성공된 시점 <br>&gt; Jan 03 02:13:12 [4901] dhcp-193-111.pek.redhat.com       crmd:   noti<br><br><publishedDate>2017-01-04T02:07:15Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000IXfE9IAL"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2017-01-03T07:49:14Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2017-01-03T07:49:14Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br><br><br>Q1)<br>pingd 가 리소스 실패로 판단하는 ICMP ping fail 임계값은 pingd 의 <br>파라미터 attempts로 지정 할수 있습니다. <br><br>attempts의 디폴트 값은 3입니다. <br><br><br>2) <br>ping 실패시 VirtualIP가 failover 소요 시간은 아래와 같이 산정 할수 <br>있습니다. <br><br>Time &lt; [pingd monitor interval] + [ pingd attempts * pingd timeout ]<br>       + VirtualIP stop + VirtualIP start<br>  <br><br>테스트 결과:<br>** 제 테스트 환경에서 attempts를 10으로 설정 하였습니다. <br><br>ㄱ. icmp 비활성화 시점:<br># date; firewall-cmd --direct --add-rule ipv4 filter OUTPUT 0 -p icmp -j DROP<br>Tue Jan  3 02:12:44 EST 2017<br><br><br>ㄴ. ping 실패로 인식 된 시점 <br>ping(ping)[18547]:      2017/01/03_02:13:04 WARNING: 10.66.193.254 is inactive: PING 10.66.193.254 (10.66.193.254) 56(84) bytes of data.<br><br>--- 10.66.193.254 ping statistics ---<br>10 packets transmitted, 0 received, 100% packet loss, time 8999ms<br>Jan 03 02:13:04 [4259] pcmk1      attrd:     info: attrd_peer_update:   Setting pingd[pcmk1]: 1000 -&gt; 0 from pcmk1<br><br><br>ㄷ.  VirtualIP failover 성공된 시점 <br>Jan 03 02:13:12 [4901] dhcp-193-111.pek.redhat.com       crmd:   notice: process_lrm_event:<br>Result of start operation for VirtualIP on pcmk2: 0 (ok) | call=51 key=VirtualIP_start_0 confirmed=true cib-update=104<br>======================<br>3) <br>migration-threshold 는 pingd에 적용 되지 않습니다.  ping 이 실패 될시 <br>pingd 리소스가 실패되는게 아니라 resource score 만 변하기 때문입니다. <br>======================<br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2017-01-03T07:49:14Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000IXGoCIAX"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2016-12-29T03:18:43Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2016-12-29T03:18:43Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다.<br><br><br>저는 정미연이라고 하며 앞으로 본 케이스를 담당하게 되었습니다. <br><br>문의 주신 내용에 대해 현재 확인중에 있으며 확인 되는대로 코멘트 남겨 <br>드리도록 하겠습니다. <br><br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2016-12-29T03:18:43Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000IX7dNIAT"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-12-28T04:34:58Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-12-28T04:34:58Z</b><br><br>안녕하세요.<br><br>본 기술문의와 관련한 sosreport는 아래의 case에 업로드되어 있으니, 참고하여 주시기 바랍니다.<br>SOSREPORT REF CASE ID :  https://access.redhat.com/support/cases/#/case/01676390<br><br><br>고맙습니다.<br><br><publishedDate>2016-12-28T04:34:58Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br><br>서비스가 정상화 되기위해서는<br>fail-over되는 클러스터 노드에서 서비스를 시작하는 시간이 필요하므로<br>각각의 상황별로 Fail-Over 되는 시간에, 별도로 서비스 시작 소요시간을 추가하여야 함을 고려하고 있으며,<br><br>아래 상황에서, Q1, Q2, Q3과 관련하여 문의 드립니다.<br><br><br>예)<br>삼성화재 ASCS 서비스 시작 소요시간( 2016.08 기준 ) : 약 30~60초<br><br><br>Q1) 시스템 quocld01, quocld02에 구성된 현재의 pacemaker 설정값으로, pacemaker의 Pingd가 리소스 실패로 판단하는 ICMP ping fail 임계값(횟수나 시간)이 얼마입니까?<br><br>Q2) pingd에서 대상으로 지정된 서비스 네트워크의 Gateway에대하여 Ping Fail시, 서비스 Fail-Over 될 때 소요시간 계산이 맞는지 Q2-1), Q2-2) 문의 드립니다.<br><br><br>        Q2-1) 시스템에 구성된 현재의 설정값으로 서비스 네트워크(VIP) 실패시 Fail-Over 소요시간 계산방식은, 아래의 내용이 맞습니까? 아니면 다른 계산식이 있습니까?<br><br>        [ pingd monitor interval + ( pingd monitor interval * migration-threshold ) ] + ( rsc_ASCS stop timeout * retry ) ] + ( fence_kdump timeout * retry ) + ( fence_ipmilan timeout * retry )<br><br><br>        Q2-2) 앞의 질문 &quot;Q2-1&quot;의 계산방식에따르면, 현재 시스템 구성에따른 Fail-Over 소요시간이 아래와 같이 나오는데 맞습니까?<br><br>        [ 10 + ( 10 * 3 ) ] + [ 60 * 1 ] + [ 90 * 1 ] + [ 60 * 1 ] = MAX 250s( 4min 10s )<br><br>Q3) migration-threshold=3으로 설정되어있는데, 이는 pingd에도 적용 되는지요?<br><br><br>###################################################################</issue><environment>####################################################################<br>SOSREPORT REF CASE ID :  https://access.redhat.com/support/cases/#/case/01676390<br><br><br>Host Name : quocld01, quocld02 ( quocld01-HB, quocld02-HB )<br>RHEL Version : Red Hat Enterprise Linux Server release 7.2 (Maipo)<br>Kernel Version : 3.10.0-327.el7.x86_64<br>HA Package Version :<br>                        corosync-2.3.4-7.el7_2.1.x86_64<br>                        pacemaker-1.1.13-10.el7.x86_64<br>                        pcs-0.9.143-15.el7.x86_64<br>                        fence-agents-all-4.0.11-27.el7.x86_64<br><br>[root@quocld01 ~]# pcs config<br>Cluster Name: quoc_cluster<br>Corosync Nodes:<br> quocld01-HB quocld02-HB<br>Pacemaker Nodes:<br> quocld01-HB quocld02-HB<br><br>Resources:<br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.8.231.1<br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.83<br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_ascs1 (class=ocf provider=sfmi type=MySAP)<br>   Attributes: start=/home/qfpadm/mysap_start stop=/home/qfpadm/mysap_stop monitor=/home/qfpadm/mysap_monitor<br>   Operations: start interval=0s timeout=60 (rsc_ascs1-start-interval-0s)<br>               stop interval=0s timeout=60 (rsc_ascs1-stop-interval-0s)<br>               monitor interval=60s enabled=false (rsc_ascs1-monitor-interval-60s)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.8.231.84<br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br><br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;quocld01-HB quocld02-HB&quot; pcmk_off_timeout=90<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld01-HB ipaddr=42.8.240.144 login=redhat passwd=quocld01!1 lanplus=on auth=password delay=15<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=quocld02-HB ipaddr=42.8.240.145 login=redhat passwd=quocld02!1 lanplus=on auth=password<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br><br> Node: quocld01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: quocld02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: quocld01-HB (score:10) (id:location-grp1-quocld01-HB-10)<br>  Resource: grp2<br>    Enabled on: quocld02-HB (score:10) (id:location-grp2-quocld02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: quocld01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: quocld02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_vip1<br>    Enabled on: quocld01-HB (score:10) (id:location-rsc_vip1-quocld01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: quocld02-HB (score:10) (id:location-rsc_vip2-quocld02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: quoc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> last-lrm-refresh: 1455954793<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>###################################################################<br>======================<br><br>###################################################################<br>##########             corosync 설정값          ##########<br># cat quocld01/corosync.dump config.totemconfig_reload_in_progress (u8) = 0 internal_configuration.service.0.name (str) = corosync_cmap internal_configuration.service.0.ver (u32) = 0 internal_configuration.service.1.name (str) = corosync_cfg internal_configuration.service.1.ver (u32) = 0 internal_configuration.service.2.name (str) = corosync_cpg internal_configuration.service.2.ver (u32) = 0 internal_configuration.service.3.name (str) = corosync_quorum internal_configuration.service.3.ver (u32) = 0 internal_configuration.service.4.name (str) = corosync_pload internal_configuration.service.4.ver (u32) = 0 internal_configuration.service.5.name (str) = corosync_votequorum internal_configuration.service.5.ver (u32) = 0 logging.logfile (str) = /var/log/cluster/corosync.log logging.to_logfile (str) = yes logging.to_syslog (str) = yes nodelist.local_node_pos (u32) = 0 nodelist.node.0.nodeid (u32) = 1 nodelist.node.0.ring0_addr (str) = quocld01-HB <br>nodelist.node.0.ring1_addr (str) = quocld01 <br>nodelist.node.1.nodeid (u32) = 2 <br>nodelist.node.1.ring0_addr (str) = quocld02-HB <br>nodelist.node.1.ring1_addr (str) = quocld02 <br>quorum.provider (str) = corosync_votequorum <br>quorum.two_node (u8) = 1 <br>runtime.blackbox.dump_flight_data (str) = 1472621105 runtime.blackbox.dump_state (str) = 1472621105 runtime.config.totem.consensus (u32) = 12000 runtime.config.totem.downcheck (u32) = 1000 runtime.config.totem.fail_recv_const (u32) = 2500 runtime.config.totem.heartbeat_failures_allowed (u32) = 0 runtime.config.totem.hold (u32) = 1894 runtime.config.totem.join (u32) = 50 runtime.config.totem.max_messages (u32) = 17 runtime.config.totem.max_network_delay (u32) = 50 runtime.config.totem.merge (u32) = 200 runtime.config.totem.miss_count_const (u32) = 5 runtime.config.totem.rrp_autorecovery_check_timeout (u32) = 1000 runtime.config.totem.rrp_problem_count_mcast_threshold (u32) = 100 runtime.config.totem.rrp_problem_count_threshold (u32) = 10 runtime.config.totem.rrp_problem_count_timeout (u32) = 2000 runtime.config.totem.rrp_token_expired_timeout (u32) = 2380 runtime.config.totem.send_join (u32) = 0 runtime.config.totem.seqno_unchanged_const (u32) = 30 runtime.config.totem.token (u32) = 10000 runtime.config.totem.token_retransmit (u32) = 2380 runtime.config.totem.token_retransmits_before_loss_const (u32) = 4 runtime.config.totem.window_size (u32) = 50 runtime.connections.active (u64) = 9 runtime.connections.attrd:6503:0x7fb3395690d0.client_pid (u32) = 6503 runtime.connections.attrd:6503:0x7fb3395690d0.dispatched (u64) = 550 runtime.connections.attrd:6503:0x7fb3395690d0.flow_control (u32) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.flow_control_count (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.invalid_request (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.name (str) = attrd runtime.connections.attrd:6503:0x7fb3395690d0.overload (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.queue_size (u32) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.recv_retries (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.requests (u64) = 278 runtime.connections.attrd:6503:0x7fb3395690d0.responses (u64) = 2 runtime.connections.attrd:6503:0x7fb3395690d0.send_retries (u64) = 0 runtime.connections.attrd:6503:0x7fb3395690d0.service_id (u32) = 2 runtime.connections.cib:6500:0x7fb33976eae0.client_pid (u32) = 6500 runtime.connections.cib:6500:0x7fb33976eae0.dispatched (u64) = 61 runtime.connections.cib:6500:0x7fb33976eae0.flow_control (u32) = 0 runtime.connections.cib:6500:0x7fb33976eae0.flow_control_count (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.invalid_request (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.name (str) = cib runtime.connections.cib:6500:0x7fb33976eae0.overload (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.queue_size (u32) = 0 runtime.connections.cib:6500:0x7fb33976eae0.recv_retries (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.requests (u64) = 29 runtime.connections.cib:6500:0x7fb33976eae0.responses (u64) = 2 runtime.connections.cib:6500:0x7fb33976eae0.send_retries (u64) = 0 runtime.connections.cib:6500:0x7fb33976eae0.service_id (u32) = 2 runtime.connections.closed (u64) = 3658 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.client_pid (u32) = 51673 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.dispatched (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control (u32) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.flow_control_count (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.invalid_request (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.name (str) = corosync-cmapct runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.overload (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.queue_size (u32) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.recv_retries (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.requests (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.responses (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.send_retries (u64) = 0 runtime.connections.corosync-cmapct:51673:0x7fb3396729e0.service_id (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.client_pid (u32) = 6505 runtime.connections.crmd:6505:0x7fb33966d160.dispatched (u64) = 33 runtime.connections.crmd:6505:0x7fb33966d160.flow_control (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.flow_control_count (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.invalid_request (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.name (str) = crmd runtime.connections.crmd:6505:0x7fb33966d160.overload (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.queue_size (u32) = 0 runtime.connections.crmd:6505:0x7fb33966d160.recv_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.requests (u64) = 9 runtime.connections.crmd:6505:0x7fb33966d160.responses (u64) = 2 runtime.connections.crmd:6505:0x7fb33966d160.send_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966d160.service_id (u32) = 2 runtime.connections.crmd:6505:0x7fb33966ec70.client_pid (u32) = 6505 runtime.connections.crmd:6505:0x7fb33966ec70.dispatched (u64) = 1 runtime.connections.crmd:6505:0x7fb33966ec70.flow_control (u32) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.flow_control_count (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.invalid_request (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.name (str) = crmd runtime.connections.crmd:6505:0x7fb33966ec70.overload (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.queue_size (u32) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.recv_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.requests (u64) = 3 runtime.connections.crmd:6505:0x7fb33966ec70.responses (u64) = 3 runtime.connections.crmd:6505:0x7fb33966ec70.send_retries (u64) = 0 runtime.connections.crmd:6505:0x7fb33966ec70.service_id (u32) = 3 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.dispatched (u64) = 9 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb33915f0b0.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.requests (u64) = 9 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.responses (u64) = 2 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb33915f0b0.service_id (u32) = 2 runtime.connections.pacemakerd:6496:0x7fb339160eb0.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb339160eb0.dispatched (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb339160eb0.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.requests (u64) = 3 runtime.connections.pacemakerd:6496:0x7fb339160eb0.responses (u64) = 3 runtime.connections.pacemakerd:6496:0x7fb339160eb0.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339160eb0.service_id (u32) = 3 runtime.connections.pacemakerd:6496:0x7fb339164f60.client_pid (u32) = 6496 runtime.connections.pacemakerd:6496:0x7fb339164f60.dispatched (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.flow_control_count (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.invalid_request (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.name (str) = pacemakerd runtime.connections.pacemakerd:6496:0x7fb339164f60.overload (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.queue_size (u32) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.recv_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.requests (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339164f60.responses (u64) = 1 runtime.connections.pacemakerd:6496:0x7fb339164f60.send_retries (u64) = 0 runtime.connections.pacemakerd:6496:0x7fb339164f60.service_id (u32) = 1 runtime.connections.stonithd:6501:0x7fb339166580.client_pid (u32) = 6501 runtime.connections.stonithd:6501:0x7fb339166580.dispatched (u64) = 5 runtime.connections.stonithd:6501:0x7fb339166580.flow_control (u32) = 0 runtime.connections.stonithd:6501:0x7fb339166580.flow_control_count (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.invalid_request (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.name (str) = stonithd runtime.connections.stonithd:6501:0x7fb339166580.overload (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.queue_size (u32) = 0 runtime.connections.stonithd:6501:0x7fb339166580.recv_retries (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.requests (u64) = 4 runtime.connections.stonithd:6501:0x7fb339166580.responses (u64) = 2 runtime.connections.stonithd:6501:0x7fb339166580.send_retries (u64) = 0 runtime.connections.stonithd:6501:0x7fb339166580.service_id (u32) = 2 runtime.services.cfg.0.rx (u64) = 0 runtime.services.cfg.0.tx (u64) = 0 runtime.services.cfg.1.rx (u64) = 0 runtime.services.cfg.1.tx (u64) = 0 runtime.services.cfg.2.rx (u64) = 0 runtime.services.cfg.2.tx (u64) = 0 runtime.services.cfg.3.rx (u64) = 0 runtime.services.cfg.3.tx (u64) = 0 runtime.services.cfg.service_id (u16) = 1 runtime.services.cmap.0.rx (u64) = 3 runtime.services.cmap.0.tx (u64) = 2 runtime.services.cmap.service_id (u16) = 0 runtime.services.cpg.0.rx (u64) = 5 runtime.services.cpg.0.tx (u64) = 5 runtime.services.cpg.1.rx (u64) = 0 runtime.services.cpg.1.tx (u64) = 0 runtime.services.cpg.2.rx (u64) = 1 runtime.services.cpg.2.tx (u64) = 0 runtime.services.cpg.3.rx (u64) = 657 runtime.services.cpg.3.tx (u64) = 319 runtime.services.cpg.4.rx (u64) = 0 runtime.services.cpg.4.tx (u64) = 0 runtime.services.cpg.5.rx (u64) = 3 runtime.services.cpg.5.tx (u64) = 2 runtime.services.cpg.6.rx (u64) = 0 <br>runtime.services.cpg.6.tx (u64) = 0 <br>runtime.services.cpg.service_id (u16) = 2 runtime.services.pload.0.rx (u64) = 0<br> runtime.services.pload.0.tx (u64) = 0 runtime.services.pload.1.rx (u64) = 0 <br>runtime.services.pload.1.tx (u64) = 0 <br>runtime.services.pload.service_id (u16) = 4 <br>runtime.services.quorum.service_id (u16) = 3 <br>runtime.services.votequorum.0.rx (u64) = 7 <br>runtime.services.votequorum.0.tx (u64) = 4 <br>runtime.services.votequorum.1.rx (u64) = 0 <br>runtime.services.votequorum.1.tx (u64) = 0 <br>runtime.services.votequorum.2.rx (u64) = 0 <br>runtime.services.votequorum.2.tx (u64) = 0 <br>runtime.services.votequorum.3.rx (u64) = 0 <br>runtime.services.votequorum.3.tx (u64) = 0 <br>runtime.services.votequorum.service_id (u16) = 5 <br>runtime.totem.pg.mrp.rrp.0.faulty (u8) = 0 <br>runtime.totem.pg.mrp.rrp.1.faulty (u8) = 0 <br>runtime.totem.pg.mrp.srp.avg_backlog_calc (u32) = 0 <br>runtime.totem.pg.mrp.srp.avg_token_workload (u32) = 0 <br>runtime.totem.pg.mrp.srp.commit_entered (u64) = 2 <br>runtime.totem.pg.mrp.srp.commit_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.consensus_timeouts (u64) = 0 runtime.totem.pg.mrp.srp.continuous_gather (u32) = 0 runtime.totem.pg.mrp.srp.continuous_sendmsg_failures (u32) = 0 runtime.totem.pg.mrp.srp.firewall_enabled_or_nic_failure (u8) = 0 runtime.totem.pg.mrp.srp.gather_entered (u64) = 2 <br>runtime.totem.pg.mrp.srp.gather_token_lost (u64) = 0 runtime.totem.pg.mrp.srp.mcast_retx (u64) = 0 <br>runtime.totem.pg.mrp.srp.mcast_rx (u64) = 747 <br>runtime.totem.pg.mrp.srp.mcast_tx (u64) = 350 <br>runtime.totem.pg.mrp.srp.memb_commit_token_rx (u64) = 4 <br>runtime.totem.pg.mrp.srp.memb_commit_token_tx (u64) = 6 <br>runtime.totem.pg.mrp.srp.memb_join_rx (u64) = 5 runtime.totem.pg.mrp.srp.memb_join_tx (u64) = 2 runtime.totem.pg.mrp.srp.memb_merge_detect_rx (u64) = 1917 runtime.totem.pg.mrp.srp.memb_merge_detect_tx (u64) = 1917 runtime.totem.pg.mrp.srp.members.1.config_version (u64) = 0 <br>runtime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(192.169.25.81) r(1) ip(42.8.231.81) <br>runtime.totem.pg.mrp.srp.members.1.join_count (u32) = 1 runtime.totem.pg.mrp.srp.members.1.status (str) = joined runtime.totem.pg.mrp.srp.members.2.config_version (u64) = 0 <br>runtime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(192.169.25.82) r(1) ip(42.8.231.82) <br>runtime.totem.pg.mrp.srp.members.2.join_count (u32) = 1 <br>runtime.totem.pg.mrp.srp.members.2.status (str) = joined <br>runtime.totem.pg.mrp.srp.mtt_rx_token (u32) = 161 runtime.totem.pg.mrp.srp.operational_entered (u64) = 2 runtime.totem.pg.mrp.srp.operational_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.orf_token_rx (u64) = 20755 runtime.totem.pg.mrp.srp.orf_token_tx (u64) = 2 <br>runtime.totem.pg.mrp.srp.recovery_entered (u64) = 2 runtime.totem.pg.mrp.srp.recovery_token_lost (u64) = 0 <br>runtime.totem.pg.mrp.srp.rx_msg_dropped (u64) = 0 runtime.totem.pg.mrp.srp.token_hold_cancel_rx (u64) = 582 runtime.totem.pg.mrp.srp.token_hold_cancel_tx (u64) = 294 <br>runtime.totem.pg.msg_queue_avail (u32) = 0 runtime.totem.pg.msg_reserved (u32) = 1 <br>runtime.votequorum.ev_barrier (u32) = 2 runtime.votequorum.highest_node_id (u32) = 2 <br>runtime.votequorum.lowest_node_id (u32) = 1 runtime.votequorum.this_node_id (u32) = 1 <br>runtime.votequorum.two_node (u8) = 1 runtime.votequorum.wait_for_all_status (u8) = 0 <br>totem.cluster_name (str) = quoc_cluster totem.interface.0.bindnetaddr (str) = quocld01-HB <br>totem.interface.0.mcastaddr (str) = 239.192.0.180 totem.interface.0.mcastport (u16) = 5405 <br>totem.interface.1.bindnetaddr (str) = quocld01 totem.interface.1.mcastaddr (str) = 239.192.0.181 <br>totem.interface.1.mcastport (u16) = 5405 <br>totem.rrp_mode (str) = passive <br>totem.secauth (str) = off <br>totem.token (u32) = 10000 <br>totem.transport (str) = udpu totem.version (u32) = 2 <br>uidgid.gid.189 (u8) = 1<br>###################################################################</environment><cep>false</cep><folderName>ERP-SAP-ASCS_ERS</folderName></case>