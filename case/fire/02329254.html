======================<br><b>생성계정 : support os</b><br><b>생성날짜 : 2019-03-04T09:40:08Z</b><br><b>마지막 답변자 : Jay Shin</b><br><b>마지막 수정 일자 : 2019-03-11T02:06:20Z</b><br><b>id : 5002K00000dMukcQAC</b><br>======================<br><br><b><font size=15>
제목  : [분석문의]core file 분석 부탁드립니다.
</font></b><br><br>======================<br><actionPlan>올려주신 vmcore 를 확인한 결과, nvidia 타사 모듈에서 사용하는 함수 사용이 콜트레이스에서 확인되며,<br>이미 free 된 메모리 영역을 액세스하면서 잘못된 주소를 접근하여 시스템 커널이 패닉되었습니다.<br>이는 타사 모듈의 사용시에 모듈의 잘못된 함수 구현에 의한 use-afer-free 일 가능성이 있으며, nvidia 업체에 추가 분석을 의뢰해 볼 필요가 있습니다.<br>혹은 nvidia 모듈을 제거하고도 문제가 지속되면 재차 vmcore 를 수집해주시기 바랍니다.</actionPlan><b>사전문의<br></b><br>안녕하세요. 타임게이트 송기호입니다.<br>금일 서버가 갑작스런 리부팅이 되었습니다.<br>리부팅후 core 파일이 발생하여 dropbox incoming 에 <br>파일명 : 0304dump.tar.bz2, sosreport-aiopsvcd01-20190304144801.tar.xz 로 업로드 하였으니, 분석 부탁드립니다.<br><br>좋은하루되세요<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.4</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 3 (Normal)</b><br><hostname>aiopsvcd01</hostname><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags/><br><br><comment id="a0a2K00000PWuVGQA1"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2019-03-11T02:06:17Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2019-03-11T02:06:17Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>본 기술 문의의 상태를 처리완료 상태로 변경하고자 합니다.<br><br>만약 본 기술 문의와 관련하여 추가 질문이 있으시다면 언제든지 본 기술 문의를 *재개*하실 수 있습니다.<br><br>기술 문의가 처리 완료되면 *고객 설문조사* 메일이 발송됩니다. 고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해, 소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>만약 본 이슈와 관련하여 추가적인 문의 사항이 있으실 경우 연락 주시기 바랍니다.<br><br>감사합니다.<br><br>Jay Shin,<br>Software Maintenance Engineer, Brisbane, APAC<br>Red Hat Customer Experience and Engagement<br><br><publishedDate>2019-03-11T02:06:17Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PWYysQAH"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2019-03-08T00:20:48Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2019-03-08T00:20:48Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 입니다.<br><br>본 케이스와 연관되어 더 궁금한 사항이 있거나 추가적인 지원이 필요하신가요?<br><br>본 안내 후 일주일 이내에 별다른 답변이 없다면 자동으로 종료 상태가 된다는 것을 알려 드립니다.<br><br>만약 추가적인 지원이 필요하다면, 연락 부탁드립니다.<br><br>감사합니다.<br><br>Jay Shin,<br>Software Maintenance Engineer, Brisbane, APAC<br>Red Hat Customer Experience and Engagement<br><br><publishedDate>2019-03-08T00:20:48Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PRVBvQAP"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2019-03-05T06:18:34Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2019-03-05T06:18:33Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>해당 메모리 영역을 삭제하고 재사용하는 주체는 nvidia 모듈일 가능성이 높습니다.<br>이 경우, 커널 패치를 통해서는 해당 문제를 해결할 수 없으며, nvidia 에 추가 분석을 의뢰하시어<br>해당 드라이버 버전에 알려진 버그가 있는지 확인이 필요합니다.<br><br>혹은 nvidia 모듈을 제거하고도 문제가 재발되면 최신 버전의 커널로 동작하여<br>동일한 문제가 발생되는지 확인을 거쳐, 커널에 버그가 있는 경우 kernel-debug 를 동작시켜서 <br>vmcore 를 수집하여 추가 분석을 진행할 수 있습니다.<br><br>감사합니다.<br><br>Jay Shin,<br>Senior TSE, Brisbane, APAC<br>Red Hat Customer Experience and Engagement<br><br><publishedDate>2019-03-05T06:18:33Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000PRV3CQAX"><br>======================<br><b>생성계정 : os, support</b><br><b>생성날짜 : 2019-03-05T06:07:23Z</b><br><b>마지막 답변자 : os, support</b><br><b>마지막 수정 일자 : 2019-03-05T06:07:23Z</b><br><br>안녕하세요<br>use after free 현상에 대한 문의드립니다.<br><br>use after free 현상은 OS에서 패치 등을 통해 해결가능한 문제 인가요?<br><br><publishedDate>2019-03-05T06:07:23Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000PRTB9QAP"><br>======================<br><b>생성계정 : Shin, Jay</b><br><b>생성날짜 : 2019-03-05T02:26:11Z</b><br><b>마지막 답변자 : Shin, Jay</b><br><b>마지막 수정 일자 : 2019-03-05T02:26:11Z</b><br><br>안녕하세요<br><br>Red Hat Global Support Services 를 이용해 주셔서 감사합니다.<br><br>올려주신 vmcore 를 확인한 결과, nvidia 타사 모듈에서 사용하는 함수 사용이 콜트레이스에서 확인되며,<br>이미 free 된 메모리 영역을 액세스하면서 잘못된 주소를 접근하여 시스템 커널이 패닉되었습니다.<br>이는 타사 모듈의 사용시에 모듈의 잘못된 함수 구현에 의한 use-afer-free 일 가능성이 있으며, nvidia 업체에 추가 분석을 의뢰해 볼 필요가 있습니다.<br>혹은 nvidia 모듈을 제거하고도 문제가 지속되면 재차 vmcore 를 수집해주시기 바랍니다.<br><br>아래는 분석 결과입니다.<br><br>crash&gt; sys<br>      KERNEL: /cores/retrace/repos/kernel/x86_64/usr/lib/debug/lib/modules/3.10.0-693.el7.x86_64/vmlinux<br>    DUMPFILE: /cores/retrace/tasks/618996740/crash/vmcore  [PARTIAL DUMP]<br>        CPUS: 24<br>        DATE: Mon Mar  4 13:31:20 2019<br>      UPTIME: 145 days, 03:23:47<br>LOAD AVERAGE: 0.30, 0.14, 0.15<br>       TASKS: 1981<br>    NODENAME: aiopsvcd01<br>     RELEASE: 3.10.0-693.el7.x86_64<br>     VERSION: #1 SMP Thu Jul 6 19:56:57 EDT 2017<br>     MACHINE: x86_64  (3000 Mhz)<br>      MEMORY: 127.7 GB<br>       PANIC: &quot;general protection fault: 0000 [#1] SMP &quot;<br><br>crash&gt; sys -i<br>        DMI_BIOS_VENDOR: HPE<br>       DMI_BIOS_VERSION: U30<br>          DMI_BIOS_DATE: 06/20/2018<br>         DMI_SYS_VENDOR: HPE<br>       DMI_PRODUCT_NAME: ProLiant DL380 Gen10<br>    DMI_PRODUCT_VERSION: <br>     DMI_PRODUCT_SERIAL: SGH834TSBS<br>       DMI_PRODUCT_UUID: 37383638-3330-4753-4838-333454534253<br>       DMI_BOARD_VENDOR: HPE<br>         DMI_BOARD_NAME: ProLiant DL380 Gen10<br>      DMI_BOARD_VERSION: <br>       DMI_BOARD_SERIAL: PFARAAMLMB34B6<br>    DMI_BOARD_ASSET_TAG:         <br>     DMI_CHASSIS_VENDOR: HPE<br>       DMI_CHASSIS_TYPE: 23<br>    DMI_CHASSIS_VERSION: <br>     DMI_CHASSIS_SERIAL: SGH834TSBS<br>  DMI_CHASSIS_ASSET_TAG:         <br><br>crash&gt; ps -S<br>  RU: 27<br>  IN: 1951<br>  ZO: 2<br>  UN: 1<br><br>[12510674.473021] general protection fault: 0000 [#1] SMP <br>[12510674.473751] Modules linked in: cfg80211 rfkill ipt_REJECT nf_reject_ipv4 xt_NFLOG nfnetlink_log dummy vport_vxlan vxlan ip6_udp_tunnel udp_tunnel openvswitch nf_conntrack_ipv6 nf_nat_ipv6 nf_defrag_ipv6 ip_set_hash_ip xt_set ip_set xt_statistic xt_comment xt_mark rbd libceph dns_resolver xt_nat veth ipt_MASQUERADE nf_nat_masquerade_ipv4 nf_conntrack_netlink nfnetlink iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 xt_addrtype iptable_filter xt_conntrack nf_nat nf_conntrack br_netfilter bridge stp llc overlay(T) nvidia_uvm(POE) nvidia_drm(POE) nvidia_modeset(POE) nvidia(POE) ipmi_devintf vfio_iommu_type1 vfio bonding seos(POE) skx_edac edac_core intel_powerclamp coretemp intel_rapl iosf_mbi kvm irqbypass crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd<br>[12510674.478172]  ses hpwdt hpilo enclosure shpchp sg joydev ipmi_ssif pcspkr lpc_ich mei_me ipmi_si mei ipmi_msghandler acpi_power_meter nfsd binfmt_misc auth_rpcgss nfs_acl lockd grace sunrpc ip_tables xfs libcrc32c sd_mod crc_t10dif crct10dif_generic sr_mod cdrom mgag200 i2c_algo_bit drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops ttm crct10dif_pclmul crct10dif_common crc32c_intel drm serio_raw ahci libahci smartpqi(OE) libata scsi_transport_sas ixgbe(OE) tg3(OE) dca i2c_core ptp pps_core wmi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: ipmi_devintf]<br>[12510674.481992] CPU: 2 PID: 7715 Comm: nvc:[driver] Tainted: P    B      OE  ------------ T 3.10.0-693.el7.x86_64 #1<br>[12510674.482758] Hardware name: HPE ProLiant DL380 Gen10/ProLiant DL380 Gen10, BIOS U30 06/20/2018<br>[12510674.483524] task: ffff880c5d2c4f10 ti: ffff880670e7c000 task.ti: ffff880670e7c000<br>[12510674.484299] RIP: 0010:[&lt;ffffffff811de860&gt;]  [&lt;ffffffff811de860&gt;] kmem_cache_free+0x70/0x200<br>[12510674.485081] RSP: 0018:ffff880fffe83c20  EFLAGS: 00010287<br>[12510674.485848] RAX: ffffea0002091140 RBX: ffff880082445000 RCX: ffff880ff9d981c8<br>[12510674.486611] RDX: 001fffff00000080 RSI: ffff880082445000 RDI: ffff880ffb229d00<br>[12510674.487360] RBP: ffff880fffe83c38 R08: 0000000000000000 R09: ffffffff81185477<br>[12510674.488107] R10: ffff101ffb0a8a00 R11: ffffea0002091140 R12: ffff881ffcdc8200<br>[12510674.488846] R13: 0000000000000000 R14: 000000000000006c R15: 00000000000000c9<br>[12510674.489580] FS:  00007ff701789740(0000) GS:ffff880fffe80000(0000) knlGS:0000000000000000<br>[12510674.490330] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033<br>[12510674.491069] CR2: 00007ff6fe3d6e30 CR3: 00000000019f2000 CR4: 00000000003407e0<br>[12510674.491805] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000<br>[12510674.492528] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400<br>[12510674.493236] Stack:<br>[12510674.493935]  ffff880ffc9e8960 ffff880082445000 0000000000000000 ffff880fffe83c48<br>[12510674.494643]  ffffffff81185477 ffff880fffe83c70 ffffffff81185879 ffff880ff9d98000<br>[12510674.495344]  ffff880c9ae3ace0 0000000000000000 ffff880fffe83cd0 ffffffffc0001f50<br>[12510674.496052] Call Trace:<br>[12510674.496748]  &lt;IRQ&gt; <br>[12510674.496753] <br>[12510674.497435]  [&lt;ffffffff81185477&gt;] mempool_free_slab+0x17/0x20<br>[12510674.498110]  [&lt;ffffffff81185879&gt;] mempool_free+0x49/0x90<br>[12510674.498779]  [&lt;ffffffffc0001f50&gt;] dec_pending+0x180/0x350 [dm_mod]<br>[12510674.499438]  [&lt;ffffffff81185477&gt;] ? mempool_free_slab+0x17/0x20<br>[12510674.500094]  [&lt;ffffffffc0002c3e&gt;] clone_endio+0x6e/0x100 [dm_mod]<br>[12510674.500739]  [&lt;ffffffff8123c914&gt;] bio_endio+0x64/0xa0<br>[12510674.501376]  [&lt;ffffffff812f91f0&gt;] blk_update_request+0x90/0x360<br>[12510674.502005]  [&lt;ffffffff8146e404&gt;] scsi_end_request+0x34/0x1e0<br>[12510674.502621]  [&lt;ffffffff8146e778&gt;] scsi_io_completion+0x168/0x6a0<br>[12510674.503225]  [&lt;ffffffff81464425&gt;] scsi_finish_command+0xd5/0x130<br>[12510674.503821]  [&lt;ffffffff8146dcc2&gt;] scsi_softirq_done+0x132/0x160<br>[12510674.504407]  [&lt;ffffffff81300510&gt;] blk_done_softirq+0x90/0xc0<br>[12510674.504986]  [&lt;ffffffff81090b3f&gt;] __do_softirq+0xef/0x280<br>[12510674.505551]  [&lt;ffffffff816b6a5c&gt;] call_softirq+0x1c/0x30<br>[12510674.506101]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0<br>[12510674.506647]  [&lt;ffffffff81090ec5&gt;] irq_exit+0x105/0x110<br>[12510674.507179]  [&lt;ffffffff816b75f6&gt;] do_IRQ+0x56/0xe0<br>[12510674.507696]  [&lt;ffffffff816ac1ed&gt;] common_interrupt+0x6d/0x6d<br>[12510674.508199]  &lt;EOI&gt; <br>[12510674.508206] <br>[12510674.508851]  [&lt;ffffffffc0cc906b&gt;] ? _nv018914rm+0x1b/0x70 [nvidia]<br>[12510674.509498]  [&lt;ffffffffc0cc9064&gt;] ? _nv018914rm+0x14/0x70 [nvidia]<br>[12510674.510084]  [&lt;ffffffffc0ccf013&gt;] ? _nv019096rm+0x843/0x1120 [nvidia]<br>[12510674.510639]  [&lt;ffffffffc0fd7526&gt;] ? _nv001111rm+0x5d6/0x620 [nvidia]<br>[12510674.511194]  [&lt;ffffffffc0cd214a&gt;] ? _nv010339rm+0xaa/0xc0 [nvidia]<br>[12510674.511737]  [&lt;ffffffffc0cd4d70&gt;] ? _nv011781rm+0x60/0x80 [nvidia]<br>[12510674.512261]  [&lt;ffffffffc0cd4cea&gt;] ? _nv011780rm+0x4a/0x70 [nvidia]<br>[12510674.512773]  [&lt;ffffffffc0d403cf&gt;] ? _nv023216rm+0xef/0x630 [nvidia]<br>[12510674.513256]  [&lt;ffffffffc0f9c41d&gt;] ? _nv003640rm+0xd/0x20 [nvidia]<br>[12510674.513727]  [&lt;ffffffffc0f86c75&gt;] ? _nv004256rm+0x15/0x80 [nvidia]<br>[12510674.514193]  [&lt;ffffffffc0f84834&gt;] ? _nv012005rm+0x194/0x290 [nvidia]<br>[12510674.514644]  [&lt;ffffffffc0fd6ba8&gt;] ? _nv035031rm+0xf8/0x1a0 [nvidia]<br>[12510674.515097]  [&lt;ffffffffc0f82b68&gt;] ? _nv035030rm+0x218/0x2c0 [nvidia]<br>[12510674.515536]  [&lt;ffffffffc0fd69ac&gt;] ? _nv033838rm+0x1c/0x30 [nvidia]<br>[12510674.515971]  [&lt;ffffffffc0ff5f47&gt;] ? _nv028982rm+0x157/0x180 [nvidia]<br>[12510674.516386]  [&lt;ffffffffc1079f9d&gt;] ? rm_gpu_ops_address_space_destroy+0xed/0x100 [nvidia]<br>[12510674.516784]  [&lt;ffffffffc09def02&gt;] ? nvUvmInterfaceAddressSpaceDestroy+0x22/0x30 [nvidia]<br>[12510674.517129]  [&lt;ffffffffc19dcb9f&gt;] ? remove_gpu+0x28f/0x310 [nvidia_uvm]<br>[12510674.517469]  [&lt;ffffffffc19dcdb1&gt;] ? uvm_gpu_release_locked+0x21/0x30 [nvidia_uvm]<br>[12510674.517806]  [&lt;ffffffffc19e1a9c&gt;] ? uvm_va_space_destroy+0x36c/0x3e0 [nvidia_uvm]<br>[12510674.518137]  [&lt;ffffffffc19d36c1&gt;] ? uvm_release+0x11/0x20 [nvidia_uvm]<br>[12510674.518466]  [&lt;ffffffff81202fb9&gt;] ? __fput+0xe9/0x260<br>[12510674.518796]  [&lt;ffffffff8120321e&gt;] ? ____fput+0xe/0x10<br>[12510674.519120]  [&lt;ffffffff810ad265&gt;] ? task_work_run+0xc5/0xf0<br>[12510674.519439]  [&lt;ffffffff8108da01&gt;] ? do_exit+0x2d1/0xa40<br>[12510674.519754]  [&lt;ffffffff810c4800&gt;] ? wake_up_state+0x10/0x20<br>[12510674.520065]  [&lt;ffffffff8109bc5e&gt;] ? signal_wake_up_state+0x1e/0x30<br>[12510674.520371]  [&lt;ffffffff8109d052&gt;] ? zap_other_threads+0x92/0xc0<br>[12510674.520681]  [&lt;ffffffff8108e1ef&gt;] ? do_group_exit+0x3f/0xa0<br>[12510674.520987]  [&lt;ffffffff8108e264&gt;] ? SyS_exit_group+0x14/0x20<br>[12510674.521302]  [&lt;ffffffffc071c48b&gt;] ? my_exit_group+0x7b/0x90 [seos]<br>[12510674.521611]  [&lt;ffffffff816b4fc9&gt;] ? system_call_fastpath+0x16/0x1b<br>[12510674.521919] Code: 01 d0 48 c1 e8 0c 48 c1 e0 06 48 03 05 a2 53 84 00 48 8b 10 80 e6 80 0f 85 59 01 00 00 49 89 c3 4c 8b 17 65 4c 03 15 68 e8 e2 7e &lt;49&gt; 8b 52 08 4d 39 5a 10 0f 85 50 01 00 00 48 63 47 20 49 8b 0a <br>[12510674.522616] RIP  [&lt;ffffffff811de860&gt;] kmem_cache_free+0x70/0x200<br>[12510674.522952]  RSP &lt;ffff880fffe83c20&gt;<br><br><br>crash&gt; bt<br>PID: 7715   TASK: ffff880c5d2c4f10  CPU: 2   COMMAND: &quot;nvc:[driver]&quot;<br> #0 [ffff880fffe839a0] machine_kexec at ffffffff8105c4cb<br> #1 [ffff880fffe83a00] __crash_kexec at ffffffff81104a32<br> #2 [ffff880fffe83ad0] crash_kexec at ffffffff81104b20<br> #3 [ffff880fffe83ae8] oops_end at ffffffff816ad278<br> #4 [ffff880fffe83b10] die at ffffffff8102e97b<br> #5 [ffff880fffe83b40] do_general_protection at ffffffff816acbfe<br> #6 [ffff880fffe83b70] general_protection at ffffffff816ac4a8<br>    [exception RIP: kmem_cache_free+112]<br>    RIP: ffffffff811de860  RSP: ffff880fffe83c20  RFLAGS: 00010287<br>    RAX: ffffea0002091140  RBX: ffff880082445000  RCX: ffff880ff9d981c8<br>    RDX: 001fffff00000080  RSI: ffff880082445000  RDI: ffff880ffb229d00<br>    RBP: ffff880fffe83c38   R8: 0000000000000000   R9: ffffffff81185477<br>    R10: ffff101ffb0a8a00  R11: ffffea0002091140  R12: ffff881ffcdc8200<br>    R13: 0000000000000000  R14: 000000000000006c  R15: 00000000000000c9<br>    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018<br> #7 [ffff880fffe83c40] mempool_free_slab at ffffffff81185477<br> #8 [ffff880fffe83c50] mempool_free at ffffffff81185879<br> #9 [ffff880fffe83c78] dec_pending at ffffffffc0001f50 [dm_mod]<br>#10 [ffff880fffe83cd8] clone_endio at ffffffffc0002c3e [dm_mod]<br>#11 [ffff880fffe83d10] bio_endio at ffffffff8123c914<br>#12 [ffff880fffe83d38] blk_update_request at ffffffff812f91f0<br>#13 [ffff880fffe83d78] scsi_end_request at ffffffff8146e404<br>#14 [ffff880fffe83db8] scsi_io_completion at ffffffff8146e778<br>#15 [ffff880fffe83e18] scsi_finish_command at ffffffff81464425<br>#16 [ffff880fffe83e48] scsi_softirq_done at ffffffff8146dcc2<br>#17 [ffff880fffe83e78] blk_done_softirq at ffffffff81300510<br>#18 [ffff880fffe83eb8] __do_softirq at ffffffff81090b3f<br>#19 [ffff880fffe83f28] call_softirq at ffffffff816b6a5c<br>#20 [ffff880fffe83f40] do_softirq at ffffffff8102d3c5<br>#21 [ffff880fffe83f60] irq_exit at ffffffff81090ec5<br>#22 [ffff880fffe83f78] do_IRQ at ffffffff816b75f6<br>--- &lt;IRQ stack&gt; ---<br>#23 [ffff880670e7f968] ret_from_intr at ffffffff816ac1ed<br>    [exception RIP: _nv018914rm+27]<br>    RIP: ffffffffc0cc906b  RSP: ffff880670e7fa18  RFLAGS: 00000246<br>    RAX: 0000000000001000  RBX: 0000000000000246  RCX: ffff881ffc464808<br>    RDX: 0000000000000594  RSI: ffff881700683008  RDI: ffff88127dec0008<br>    RBP: ffff880d1b6adb68   R8: ffffffffc113d750   R9: ffff880d1b6adb0c<br>    R10: ffff88017fc03300  R11: 0000000000000000  R12: ffffffffc0d2c192<br>    R13: ffff880d1b6adaf0  R14: ffffffffc09da2ca  R15: ffff880670e7f978<br>    ORIG_RAX: ffffffffffffff9f  CS: 0010  SS: 0018<br>#24 [ffff880670e7fa30] _nv019096rm at ffffffffc0ccf013 [nvidia]<br>#25 [ffff880670e7fa60] _nv001111rm at ffffffffc0fd7526 [nvidia]<br>#26 [ffff880670e7fa90] _nv010339rm at ffffffffc0cd214a [nvidia]<br>#27 [ffff880670e7fae0] _nv011781rm at ffffffffc0cd4d70 [nvidia]<br>#28 [ffff880670e7fb10] _nv011780rm at ffffffffc0cd4cea [nvidia]<br>#29 [ffff880670e7fb20] _nv023216rm at ffffffffc0d403cf [nvidia]<br>#30 [ffff880670e7fb50] _nv003640rm at ffffffffc0f9c41d [nvidia]<br>#31 [ffff880670e7fb60] _nv004256rm at ffffffffc0f86c75 [nvidia]<br>#32 [ffff880670e7fb80] _nv012005rm at ffffffffc0f84834 [nvidia]<br>#33 [ffff880670e7fbb0] _nv035031rm at ffffffffc0fd6ba8 [nvidia]<br>#34 [ffff880670e7fbe0] _nv035030rm at ffffffffc0f82b68 [nvidia]<br>#35 [ffff880670e7fc10] _nv033838rm at ffffffffc0fd69ac [nvidia]<br>#36 [ffff880670e7fc20] _nv028982rm at ffffffffc0ff5f47 [nvidia]<br>#37 [ffff880670e7fc50] rm_gpu_ops_address_space_destroy at ffffffffc1079f9d [nvidia]<br>#38 [ffff880670e7fd10] nvUvmInterfaceAddressSpaceDestroy at ffffffffc09def02 [nvidia]<br>#39 [ffff880670e7fd30] remove_gpu at ffffffffc19dcb9f [nvidia_uvm]<br>#40 [ffff880670e7fd58] uvm_gpu_release_locked at ffffffffc19dcdb1 [nvidia_uvm]<br>#41 [ffff880670e7fd68] uvm_va_space_destroy at ffffffffc19e1a9c [nvidia_uvm]<br>#42 [ffff880670e7fde0] uvm_release at ffffffffc19d36c1 [nvidia_uvm]<br>#43 [ffff880670e7fdf0] __fput at ffffffff81202fb9<br>#44 [ffff880670e7fe38] ____fput at ffffffff8120321e<br>#45 [ffff880670e7fe48] task_work_run at ffffffff810ad265<br>#46 [ffff880670e7fe88] do_exit at ffffffff8108da01<br>#47 [ffff880670e7ff20] do_group_exit at ffffffff8108e1ef<br>#48 [ffff880670e7ff50] sys_exit_group at ffffffff8108e264<br>#49 [ffff880670e7ff60] my_exit_group at ffffffffc071c48b [seos]<br>#50 [ffff880670e7ff80] system_call_fastpath at ffffffff816b4fc9<br>    RIP: 00007ff700a59529  RSP: 00007ffe66c766c0  RFLAGS: 00000293<br>    RAX: 00000000000000e7  RBX: ffffffff816b4fc9  RCX: ffffffffffffffff<br>    RDX: 0000000000000000  RSI: 0000000000000000  RDI: 0000000000000000<br>    RBP: 0000000000000050   R8: 000000000000003c   R9: 00000000000000e7<br>    R10: ffffffffffffff80  R11: 0000000000000246  R12: 00007ffe66c767b0<br>    R13: 000055e5b40650a0  R14: ffffffffc071c48b  R15: ffff880670e7ff78<br>    ORIG_RAX: 00000000000000e7  CS: 0033  SS: 002b<br><br>crash&gt; dis -rl ffffffff811de860 | tail<br>/usr/src/debug/kernel-3.10.0-693.el7/linux-3.10.0-693.el7.x86_64/include/linux/page_ref.h: 25<br>0xffffffff811de849 &lt;kmem_cache_free+89&gt;:<br>and    $0x80,%dh<br>0xffffffff811de84c &lt;kmem_cache_free+92&gt;:<br>jne    0xffffffff811de9ab &lt;kmem_cache_free+443&gt;<br>/usr/src/debug/kernel-3.10.0-693.el7/linux-3.10.0-693.el7.x86_64/include/linux/page_ref.h: 27<br>0xffffffff811de852 &lt;kmem_cache_free+98&gt;:<br>mov    %rax,%r11<br>/usr/src/debug/kernel-3.10.0-693.el7/linux-3.10.0-693.el7.x86_64/mm/slub.c: 2700<br>0xffffffff811de855 &lt;kmem_cache_free+101&gt;:<br>mov    (%rdi),%r10<br>0xffffffff811de858 &lt;kmem_cache_free+104&gt;:<br>add    %gs:0x7ee2e868(%rip),%r10        # 0xd0c8<br>/usr/src/debug/kernel-3.10.0-693.el7/linux-3.10.0-693.el7.x86_64/mm/slub.c: 2702<br>0xffffffff811de860 &lt;kmem_cache_free+112&gt;:<br>mov    0x8(%r10),%rdx<br><br>crash&gt; kmem ffff101ffb0a8a00<br>kmem: cannot determine page for ffff101ffb0a8a00<br>ffff101ffb0a8a00: physical address not found in mem map<br><br>crash&gt; kmem ffff880ffb229d00<br>CACHE             OBJSIZE  ALLOCATED     TOTAL  SLABS  SSIZE  NAME<br>ffff88017fc02000      216       3299      4320    135     8k  kmem_cache<br>  SLAB              MEMORY            NODE  TOTAL  ALLOCATED  FREE<br>  ffffea003fec8a00  ffff880ffb228000     0     32         19    13<br>  FREE / [ALLOCATED]<br>   ffff880ffb229d00  (cpu 1 cache) &lt;&lt;-------------------------------------------- freed<br><br>      PAGE         PHYSICAL      MAPPING       INDEX CNT FLAGS<br>ffffea003fec8a40  ffb229000                0        0  0 2fffff00008000 tail<br><br>crash&gt; mod -t<br>NAME            TAINTS<br>tg3             OE<br>ixgbe           OE<br>smartpqi        OE<br>nvidia_drm      POE<br>overlay         T<br>seos            POE<br>nvidia          POE<br>nvidia_uvm      POE<br>nvidia_modeset  POE<br><br>crash&gt; module.version ffffffffc185ae40<br>  version = 0xffff88203ff45778 &quot;410.48&quot;<br><br>crash&gt; ps -p 7715<br>PID: 0      TASK: ffffffff819f9480  CPU: 0   COMMAND: &quot;swapper/0&quot;<br> PID: 1      TASK: ffff8810a9e90000  CPU: 16  COMMAND: &quot;systemd&quot;<br>  PID: 151312  TASK: ffff8819e3624f10  CPU: 3   COMMAND: &quot;dockerd&quot;<br>   PID: 151419  TASK: ffff8819e3626eb0  CPU: 14  COMMAND: &quot;docker-containe&quot;<br>    PID: 7675   TASK: ffff880ffd2b2f70  CPU: 7   COMMAND: &quot;docker-containe&quot;<br>     PID: 7688   TASK: ffff881ffc6f4f10  CPU: 15  COMMAND: &quot;nvidia-containe&quot;<br>      PID: 7706   TASK: ffff881a44b7cf10  CPU: 22  COMMAND: &quot;nvidia-containe&quot;<br>       PID: 7715   TASK: ffff880c5d2c4f10  CPU: 2   COMMAND: &quot;nvc:[driver]&quot;<br><br>crash&gt; kmem -i<br>                 PAGES        TOTAL      PERCENTAGE<br>    TOTAL MEM  32793919     125.1 GB         ----<br>         FREE   363009       1.4 GB    1% of TOTAL MEM<br>         USED  32430910     123.7 GB   98% of TOTAL MEM<br>       SHARED  27053300     103.2 GB   82% of TOTAL MEM<br>      BUFFERS     1294       5.1 MB    0% of TOTAL MEM<br>       CACHED  30141499       115 GB   91% of TOTAL MEM<br>         SLAB  1297454       4.9 GB    3% of TOTAL MEM<br><br>   TOTAL HUGE        0            0         ----<br>    HUGE FREE        0            0    0% of TOTAL HUGE<br><br>   TOTAL SWAP        0            0         ----<br>    SWAP USED        0            0    0% of TOTAL SWAP<br>    SWAP FREE        0            0    0% of TOTAL SWAP<br><br> COMMIT LIMIT  16396959      62.5 GB         ----<br>    COMMITTED  2325752       8.9 GB   14% of TOTAL LIMIT<br><br>만약 본 이슈와 관련하여 추가적인 문의 사항이 있으실 경우 연락 주시기 바랍니다.<br><br>감사합니다.<br><br>Jay Shin,<br>Senior TSE, Brisbane, APAC<br>Red Hat Customer Experience and Engagement<br><br><publishedDate>2019-03-05T02:26:11Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>