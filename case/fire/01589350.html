======================<br><b>생성계정 : Seung-Pil Kim</b><br><b>생성날짜 : 2016-02-24T14:22:12Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2016-04-03T10:15:11Z</b><br><b>id : 500A000000TdJewIAF</b><br>======================<br><br><b><font size=15>
제목  : [삼성화재][보험ERP][가동계] fence_kdump: failed to get node 'NODE01' AND stonith-ng:   error: Operation 'reboot' [2051] (call 2 from crmd.90546) for host 'NODE01' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)
</font></b><br><br>======================<br><b>사전문의<br></b><br>어떤 문제/오류/결함이 발생했습니까? 기대하시는 결과는 무엇입니까?<br><br>##################################################<br># Failed Info AND Messages<br>##################################################<br>요약 :<br>Cluster node's STONITH is configured  fence_kdump and fence_ipmilan. Cluseter node is not fenced by fence_kdump,  But It is fenced by power.<br>1) 클러스터에 설정된 STONITH에는 LEVEL1의 fence_kdump 와 LEVEL2의 fence_ipmilan 이 있음<br>2) node2는 정상적으로 fence_kdump 가 수행되어 vmcore가 생성됨<br>3) node1은 fence_kdump 가 설정되어 있음에도, fence_ipmilan이 수행되어 power fencing 되고 vmcore가 생성되지 않음.<br><br><br>###### node1 was fenced LOG<br><br>Feb 20 19:46:18 eccclp01 root: ############################## TEST - HA03 [ START ] ##############################<br>Feb 20 19:46:18 eccclp01 systemd-logind: Removed session 2133.<br>Feb 20 19:47:01 eccclp01 systemd: Started Session 2134 of user root.<br>Feb 20 19:47:01 eccclp01 systemd: Starting Session 2134 of user root.<br>Feb 20 20:02:46 eccclp01 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;7.4<br><br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br>Feb 20 19:46:18 eccclp02 root: ############################## TEST - HA03 [ START ] ##############################<br>Feb 20 19:46:18 eccclp02 systemd-logind: Removed session 2025.<br>Feb 20 19:47:01 eccclp02 systemd: Started Session 2026 of user root.<br>Feb 20 19:47:01 eccclp02 systemd: Starting Session 2026 of user root.<br>Feb 20 19:47:57 eccclp02 corosync[87147]: [TOTEM ] A processor failed, forming new configuration.<br>Feb 20 19:47:58 eccclp02 systemd-logind: New session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd: Started Session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd: Starting Session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd-logind: Removed session 2027.<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [TOTEM ] A new membership (192.168.25.22:620) was formed. Members left: 1<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [TOTEM ] Failed to receive the leave message. failed: 1<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [QUORUM] Members[1]: 2<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [MAIN  ] Completed service synchronization, ready to provide service.<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Removing all eccclp01-HB attributes for attrd_peer_change_cb<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Lost attribute writer eccclp01-HB<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 pacemakerd[90379]:  notice: crm_reap_unseen_nodes: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: crm_reap_unseen_nodes: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 crmd[90546]: warning: Our DC node (eccclp01-HB) left the cluster<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: State transition S_NOT_DC -&gt; S_ELECTION [ input=I_ELECTION cause=C_FSA_INTERNAL origin=reap_dead_nodes ]<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Feb 20 19:47:58 eccclp02 crmd[90546]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: Notifications disabled<br>Feb 20 19:48:00 eccclp02 pengine[90544]: warning: Node eccclp01-HB will be fenced because the node is no longer part of the cluster<br>Feb 20 19:48:00 eccclp02 pengine[90544]: warning: Node eccclp01-HB is unclean<br>Feb 20 19:48:00 eccclp02 crmd[90546]:  notice: Executing reboot fencing operation (40) on eccclp01-HB (timeout=60000)<br>Feb 20 19:48:00 eccclp02 crmd[90546]:  notice: Initiating action 34: start kdump_stonith_start_0 on eccclp02-HB (local)<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: Client crmd.90546.e5cea6dd wants to fence (reboot) 'eccclp01-HB' with device '(any)'<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: Initiating remote operation reboot for eccclp01-HB: f1429d36-4ba7-4c28-8564-c51a8ff279c9 (0)<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: kdump_stonith can fence (reboot) eccclp01-HB: static-list<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: ipmilan_stonith1 can fence (reboot) eccclp01-HB: static-list<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 19:48:00 eccclp02 fence_kdump[2049]: failed to get node 'eccclp01-HB'<br>Feb 20 19:48:01 eccclp02 systemd: Started Session 2028 of user root.<br>Feb 20 19:48:01 eccclp02 systemd: Starting Session 2028 of user root.<br>Feb 20 19:48:01 eccclp02 fence_kdump[2051]: failed to get node 'eccclp01-HB'<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:   error: Operation 'reboot' [2051] (call 2 from crmd.90546) for host 'eccclp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]: warning: kdump_stonith:2051 [ [error]: failed to get node 'eccclp01-HB' ]<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:  notice: Call to kdump_stonith for eccclp01-HB on behalf of crmd.90546@eccclp02-HB: Generic Pacemaker error (-201)<br>Feb 20 19:48:02 eccclp02 crmd[90546]:  notice: Operation kdump_stonith_start_0: ok (node=eccclp02-HB, call=63, rc=0, cib-update=64, confirmed=true)<br>Feb 20 19:48:02 eccclp02 crmd[90546]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on eccclp02-HB (local)<br>Feb 20 19:48:13 eccclp02 fence_kdump[101793]: received valid message from '192.168.25.21'<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Operation 'reboot' [2064] (call 2 from crmd.90546) for host 'eccclp01-HB' with device 'ipmilan_stonith1' returned: 0 (OK)<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Call to ipmilan_stonith1 for eccclp01-HB on behalf of crmd.90546@eccclp02-HB: OK (0)<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Operation reboot of eccclp01-HB by eccclp02-HB for crmd.90546@eccclp02-HB.f1429d36: OK<br>Feb 20 19:48:29 eccclp02 crmd[90546]:  notice: Stonith operation 2/40:0:0:a16507a7-716b-4b7b-b1e3-0f0fd8f6958c: OK (0)<br>Feb 20 19:48:29 eccclp02 crmd[90546]:  notice: Peer eccclp01-HB was terminated (reboot) by eccclp02-HB for eccclp02-HB: OK (ref=f1429d36-4ba7-4c28-8564-c51a8ff279c9) by client crmd.90546<br><br><br>###### node2 was fenced LOG<br>Feb 20 20:42:55 eccclp01 corosync[17526]: [TOTEM ] A processor failed, forming new configuration.<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [TOTEM ] A new membership (192.168.25.21:628) was formed. Members left: 2<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [TOTEM ] Failed to receive the leave message. failed: 2<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Removing all eccclp02-HB attributes for attrd_peer_change_cb<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: Our peer on the DC (eccclp02-HB) is dead<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Lost attribute writer eccclp02-HB<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [QUORUM] Members[1]: 1<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [MAIN  ] Completed service synchronization, ready to provide service.<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: State transition S_NOT_DC -&gt; S_ELECTION [ input=I_ELECTION cause=C_CRMD_STATUS_CALLBACK origin=peer_update_callback ]<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: crm_reap_unseen_nodes: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Feb 20 20:42:56 eccclp01 pacemakerd[17541]:  notice: crm_reap_unseen_nodes: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 crmd[17550]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: Notifications disabled<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Node eccclp02-HB will be fenced because the node is no longer part of the cluster<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Node eccclp02-HB is unclean<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_ping:1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_ping:1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_vip1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_dummy1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_vip2_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_dummy2_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action kdump_stonith_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action ipmilan_stonith1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action ipmilan_stonith1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Scheduling Node eccclp02-HB for STONITH<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Stop    rsc_ping:1#011(eccclp02-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_vip1#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_dummy1#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_vip2#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_dummy2#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    kdump_stonith#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Stop    ipmilan_stonith1#011(eccclp02-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Calculated Transition 0: /var/lib/pacemaker/pengine/pe-warn-10.bz2<br>Feb 20 20:42:58 eccclp01 crmd[17550]:  notice: Executing reboot fencing operation (40) on eccclp02-HB (timeout=60000)<br>Feb 20 20:42:58 eccclp01 crmd[17550]:  notice: Initiating action 34: start kdump_stonith_start_0 on eccclp01-HB (local)<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: Client crmd.17550.73aec11b wants to fence (reboot) 'eccclp02-HB' with device '(any)'<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: Initiating remote operation reboot for eccclp02-HB: 626f57a5-0bf4-4e12-872a-05b6493bde48 (0)<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: kdump_stonith can fence (reboot) eccclp02-HB: static-list<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: ipmilan_stonith2 can fence (reboot) eccclp02-HB: static-list<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 20:42:58 eccclp01 fence_kdump[22178]: waiting for message from '192.168.25.22'<br>Feb 20 20:43:01 eccclp01 systemd: Started Session 72 of user root.<br>Feb 20 20:43:01 eccclp01 systemd: Starting Session 72 of user root.<br>Feb 20 20:43:11 eccclp01 fence_kdump[22178]: received valid message from '192.168.25.22'<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Operation 'reboot' [22178] (call 2 from crmd.17550) for host 'eccclp02-HB' with device 'kdump_stonith' returned: 0 (OK)<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Call to kdump_stonith for eccclp02-HB on behalf of crmd.17550@eccclp01-HB: OK (0)<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Operation reboot of eccclp02-HB by eccclp01-HB for crmd.17550@eccclp01-HB.626f57a5: OK<br>Feb 20 20:43:11 eccclp01 crmd[17550]:  notice: Stonith operation 2/40:0:0:10ed0f8e-e7da-48b1-b142-9ead4591f55f: OK (0)<br>Feb 20 20:43:11 eccclp01 crmd[17550]:  notice: Peer eccclp02-HB was terminated (reboot) by eccclp01-HB for eccclp01-HB: OK (ref=626f57a5-0bf4-4e12-872a-05b6493bde48) by client crmd.17550<br><br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br>Feb 20 20:42:07 eccclp02 systemd: Starting Session 2124 of user root.<br>Feb 20 20:42:07 eccclp02 systemd-logind: Removed session 2124.<br>Feb 20 20:47:42 eccclp02 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;7.4.7&quot; x-pid=&quot;3181&quot; x-info=&quot;http://www.rsyslog.com&quot;] start<br>Feb 20 20:47:35 eccclp02 journal: Runtime journal is using 8.0M (max allowed 4.0G, trying to leave 4.0G free of 62.6G available → current limit 4.0G).<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpuset<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpu<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpuacct<br>Feb 20 20:47:35 eccclp02 kernel: Linux version 3.10.0-327.el7.x86_64 (mockbuild@x86-034.build.eng.bos.redhat.com) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Oct 29 17:29:29 EDT 2015<br>Feb 20 20:47:35 eccclp02 kernel: Command line: BOOT_IMAGE=/vmlinuz-3.10.0-327.el7.x86_64 root=/dev/mapper/vg00-root_lv ro crashkernel=512M rd.lvm.lv=vg00/root_lv rd.lvm.lv=vg00/swap_lv nomodeset biosdevname=1 rhgb quiet LANG=en_US.UTF-8 nmi_watchdog=0 transparent_hugepage=never elevator=deadline rdloaddriver=megaraid_sas rdloaddriver=lpfc vga=794<br>======================<br><br>======================<br><br>[root@eccclp01 ]# cat pcs_config <br>Cluster Name: eccc_cluster<br>Corosync Nodes:<br> eccclp01-HB eccclp02-HB <br>Pacemaker Nodes:<br> eccclp01-HB eccclp02-HB <br><br>Resources: <br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.1.231.1 <br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.1.231.23 <br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_dummy1 (class=ocf provider=heartbeat type=Dummy)<br>   Operations: start interval=0s timeout=20 (rsc_dummy1-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_dummy1-stop-interval-0s)<br>               monitor interval=10 timeout=20 (rsc_dummy1-monitor-interval-10)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.1.231.24 <br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br>  Resource: rsc_dummy2 (class=ocf provider=heartbeat type=Dummy)<br>   Operations: start interval=0s timeout=20 (rsc_dummy2-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_dummy2-stop-interval-0s)<br>               monitor interval=10 timeout=20 (rsc_dummy2-monitor-interval-10)<br><br>Stonith Devices: <br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;eccclp01-HB eccclp02-HB&quot; <br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp01-HB ipaddr=42.1.240.114 login=redhat passwd=eccclp01!1 lanplus=on auth=password delay=15 <br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp02-HB ipaddr=42.1.240.115 login=redhat passwd=eccclp02!1 lanplus=on auth=password <br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels: <br><br> Node: eccclp01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: eccclp02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: eccclp01-HB (score:10) (id:location-grp1-eccclp01-HB-10)<br>  Resource: grp2<br>    Enabled on: eccclp02-HB (score:10) (id:location-grp2-eccclp02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: eccclp01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: eccclp02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_dummy1<br>    Enabled on: eccclp01-HB (score:10) (id:location-rsc_dummy1-eccclp01-HB-10)<br>  Resource: rsc_dummy2<br>    Enabled on: eccclp02-HB (score:10) (id:location-rsc_dummy2-eccclp02-HB-10)<br>  Resource: rsc_vip1<br>    Enabled on: eccclp01-HB (score:10) (id:location-rsc_vip1-eccclp01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: eccclp02-HB (score:10) (id:location-rsc_vip2-eccclp02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: eccc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>======================<br>[root@eccclp01 ]# cat /etc/kdump.conf <br>path /CRASH<br>core_collector makedumpfile -c -d 31 -F<br>fence_kdump_nodes 192.168.25.22<br><br><br>[root@eccclp02 ]# cat /etc/kdump.conf <br>path /CRASH<br>core_collector makedumpfile -c -d 31 -F<br>fence_kdump_nodes 192.168.25.21<br><br><br>[root@eccclp01 ]# cat /etc/hosts <br><br># 2016.01.18 - Red Hat for HA ( HeartBeat )<br>192.168.25.21<br>eccclp01-HB<br>192.168.25.22<br>eccclp02-HB<br><br># 2016.01.18 - Red Hat for HA ( Service )<br>42.1.231.21<br>eccclp01<br>42.1.231.21<br>eccclp02<br><br>어디서 문제가 발생했습니까? 어떤 환경에서 발생했습니까?<br><br>HA on RHEL7<br><br>언제 문제가 발생했습니까? 이러한 문제가 자주 발생합니까? 반복적으로 발생합니까? 특정 시간에 발생합니까?<br><br>##################################################<br># Re-produced<br>##################################################<br>It is appeared when node1 is fenced by node2 through kdump.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>계정 번호  : 1596892</b><br><b>심각도  : 3 (Normal)</b><br><br><br>======================<br><b>생성계정 : Bradley, Shane</b><br><b>생성날짜 : 2016-03-11T16:10:08Z</b><br><b>마지막 답변자 : Bradley, Shane</b><br><b>마지막 수정 일자 : 2016-03-11T16:10:08Z</b><br><br>Lets try the following to see if we can get more information from the fence_kdump. <br><br>1) On bisalp02 log this message before crashing the other node<br>   # logger &quot;Crashing bisalp01.&quot;<br><br>2) Stop pacemaker on node bisalp01 so that it is no longer a member and fencing will not<br>   occur when you simulate a crash.<br><br>3) Do the simulate crash with /proc/sysrqtrigger on node bisalp01<br><br>4) Then try running fence_kdump manual from bisalp02.<br><br>   # fence_kdump -n bisalp01-HB -o off -t 120 -vvvvv<br><br>   If that fails try with ip address, you might need to crash twice.<br>   # fence_kdump -n  192.168.25.91 -o off -t 120 -vvvvv<br><br><br>   Copy and paste the command and output to ticket.<br><br>5) Then after you have simulated both commands.<br><br>   Copy the logs from bisalp02 from the time that bisalp01 till the end of logs.<br>   Start copy right before our first logger statement from step one.<br><br><publishedDate>2016-03-11T16:10:08Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GicDOIAZ"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-03-11T06:09:50Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-03-11T06:09:49Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>답변 감사합니다. 보내주신 데이터를 기반으로 확인하고 다시 업데이트 드리도록 하겠습니다.<br>======================<br>감사합니다.<br><br><publishedDate>2016-03-11T06:09:49Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GibNsIAJ"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-03-11T04:03:14Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-03-11T04:03:14Z</b><br><br>Hi,<br>Here are answer for your questions.<br><br>1) How are they testing this? <br>What are there reproducer steps to cause the error for fence_kdump to occur?<br><br> TEST SCENARIO is like below.<br><br> <br><br>eccclp01( bisalp01 )<br>======================<br>eccclp02( bisalp02 ) <br>1-1) <br>pcs cluster start <br>======================<br>pcs cluster start <br>1-2)<br>pcs status <br>======================<br><br><br>pcs status <br>1-3) <br>echo c &gt; /proc/sysrq-trigger <br>pcs status <br>1-4)<br>CRASH KERNEL MODE <br>======================<br>pcs status &amp;&amp; tail -f /var/log/messages <br>1-5)<br>ls /CRASH/127-*<br><br>&quot;NIC Link down&quot; messages is a result of NIC failure testing.<br>so, You can ignore that messages.<br><br>2) Can you verify which clusters can crash into a kdump kernel and then be fenced? <br>Over 10 set clusters, Almost cluster node01 failed that making a vmcore.<br>At that time, node02 has got a messages like on this case.<br><br>4) Can you tarball your /var/log directory up on both nodes and attach to ticket?<br>I attached.<br>sosreport-bisalp01-20160220-var_log.tar.bz2 (7.8 MB)<br>sosreport-bisalp02-20160220-var_log.tar.bz2 (12.2 MB)<br><br><br>thanks your efforts.<br><br>(Bradley, Shane에 회신)<br>&gt; The issue is that fence_kdump failed to run on one of the cluster nodes but did work on another. <br>&gt; <br>&gt; -------------------------------------------------------------------------------------------------<br>&gt; The cib.xml snippet from sosreport<br>&gt; -------------------------------------------------------------------------------------------------<br>&gt; <br>&gt; The configuration of the fencing devices and levels look correct. <br>&gt;       &lt;primitive class=&quot;stonith&quot; id=&quot;kdump_stonith&quot; type=&quot;fence_kdump&quot;&gt;<br>&gt;         &lt;instance_attributes id=&quot;kdump_stonith-instance_attributes&quot;&gt;<br>&gt;           &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_host_check&quot; name=&quot;pcmk_host_check&quot; value=&quot;static-list&quot;/&gt;<br>&gt;           &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_monitor_action&quot; name=&quot;pcmk_monitor_action&quot; value=&quot;metadata&quot;/&gt;<br>&gt;           &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_status_action&quot; name=&quot;pcmk_status_action&quot; value=&quot;metadata&quot;/&gt;<br>&gt;           &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_reboot_action&quot; name=&quot;pcmk_reboot_actio<br><br><publishedDate>2016-03-11T04:03:14Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GdyGDIAZ"><br>======================<br><b>생성계정 : Bradley, Shane</b><br><b>생성날짜 : 2016-03-09T19:27:26Z</b><br><b>마지막 답변자 : Bradley, Shane</b><br><b>마지막 수정 일자 : 2016-03-09T19:27:26Z</b><br><br>The issue is that fence_kdump failed to run on one of the cluster nodes but did work on another. <br><br>-------------------------------------------------------------------------------------------------<br>The cib.xml snippet from sosreport<br>-------------------------------------------------------------------------------------------------<br><br>The configuration of the fencing devices and levels look correct. <br>      &lt;primitive class=&quot;stonith&quot; id=&quot;kdump_stonith&quot; type=&quot;fence_kdump&quot;&gt;<br>        &lt;instance_attributes id=&quot;kdump_stonith-instance_attributes&quot;&gt;<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_host_check&quot; name=&quot;pcmk_host_check&quot; value=&quot;static-list&quot;/&gt;<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_monitor_action&quot; name=&quot;pcmk_monitor_action&quot; value=&quot;metadata&quot;/&gt;<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_status_action&quot; name=&quot;pcmk_status_action&quot; value=&quot;metadata&quot;/&gt;<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_reboot_action&quot; name=&quot;pcmk_reboot_action&quot; value=&quot;off&quot;/&gt;<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_host_list&quot; name=&quot;pcmk_host_list&quot; value=&quot;bisalp01-HB bisalp02-HB&quot;/&gt;<br>        &lt;/instance_attributes&gt;<br>        &lt;operations&gt;<br>          &lt;op id=&quot;kdump_stonith-monitor-interval-60s&quot; interval=&quot;60s&quot; name=&quot;monitor&quot;/&gt;<br>        &lt;/operations&gt;<br>      &lt;/primitive&gt;<br>      &lt;primitive class=&quot;stonith&quot; id=&quot;ipmilan_stonith1&quot; type=&quot;fence_ipmilan&quot;&gt;<br>        &lt;instance_attributes id=&quot;ipmilan_stonith1-instance_attributes&quot;&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-pcmk_host_check&quot; name=&quot;pcmk_host_check&quot; value=&quot;static-list&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-pcmk_host_list&quot; name=&quot;pcmk_host_list&quot; value=&quot;bisalp01-HB&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-ipaddr&quot; name=&quot;ipaddr&quot; value=&quot;42.1.240.135&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-login&quot; name=&quot;login&quot; value=&quot;redhat&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-passwd&quot; name=&quot;passwd&quot; value=&quot;****&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-lanplus&quot; name=&quot;lanplus&quot; value=&quot;on&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-auth&quot; name=&quot;auth&quot; value=&quot;password&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith1-instance_attributes-delay&quot; name=&quot;delay&quot; value=&quot;15&quot;/&gt;<br>        &lt;/instance_attributes&gt;<br>        &lt;operations&gt;<br>          &lt;op id=&quot;ipmilan_stonith1-monitor-interval-60s&quot; interval=&quot;60s&quot; name=&quot;monitor&quot;/&gt;<br>        &lt;/operations&gt;<br>      &lt;/primitive&gt;<br>      &lt;primitive class=&quot;stonith&quot; id=&quot;ipmilan_stonith2&quot; type=&quot;fence_ipmilan&quot;&gt;<br>        &lt;instance_attributes id=&quot;ipmilan_stonith2-instance_attributes&quot;&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-pcmk_host_check&quot; name=&quot;pcmk_host_check&quot; value=&quot;static-list&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-pcmk_host_list&quot; name=&quot;pcmk_host_list&quot; value=&quot;bisalp02-HB&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-ipaddr&quot; name=&quot;ipaddr&quot; value=&quot;42.1.240.136&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-login&quot; name=&quot;login&quot; value=&quot;redhat&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-passwd&quot; name=&quot;passwd&quot; value=&quot;****&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-lanplus&quot; name=&quot;lanplus&quot; value=&quot;on&quot;/&gt;<br>          &lt;nvpair id=&quot;ipmilan_stonith2-instance_attributes-auth&quot; name=&quot;auth&quot; value=&quot;password&quot;/&gt;<br>        &lt;/instance_attributes&gt;<br>        &lt;operations&gt;<br>          &lt;op id=&quot;ipmilan_stonith2-monitor-interval-60s&quot; interval=&quot;60s&quot; name=&quot;monitor&quot;/&gt;<br>        &lt;/operations&gt;<br>      &lt;/primitive&gt;<br>    [....]<br>    &lt;constraints&gt;<br>      [....]<br>      &lt;rsc_location id=&quot;const_ipmilan1&quot; node=&quot;bisalp01-HB&quot; rsc=&quot;ipmilan_stonith1&quot; score=&quot;-INFINITY&quot;/&gt;<br>      &lt;rsc_location id=&quot;const_ipmilan2&quot; node=&quot;bisalp02-HB&quot; rsc=&quot;ipmilan_stonith2&quot; score=&quot;-INFINITY&quot;/&gt;<br>    &lt;/constraints&gt;<br>    [....]<br>    &lt;fencing-topology&gt;<br>      &lt;fencing-level devices=&quot;kdump_stonith&quot; id=&quot;fl-bisalp01-HB-1&quot; index=&quot;1&quot; target=&quot;bisalp01-HB&quot;/&gt;<br>      &lt;fencing-level devices=&quot;kdump_stonith&quot; id=&quot;fl-bisalp02-HB-1&quot; index=&quot;1&quot; target=&quot;bisalp02-HB&quot;/&gt;<br>      &lt;fencing-level devices=&quot;ipmilan_stonith1&quot; id=&quot;fl-bisalp01-HB-2&quot; index=&quot;2&quot; target=&quot;bisalp01-HB&quot;/&gt;<br>      &lt;fencing-level devices=&quot;ipmilan_stonith2&quot; id=&quot;fl-bisalp02-HB-2&quot; index=&quot;2&quot; target=&quot;bisalp02-HB&quot;/&gt;<br>    &lt;/fencing-topology&gt;<br><br>-------------------------------------------------------------------------------------------------<br>fence_kdump special configuration<br>-------------------------------------------------------------------------------------------------<br><br>They appear to have fence_kdump properly configured:<br>$ grep -ie &quot;pcmk_monitor_action&quot;  sos_commands/cluster/crm_report/bisalp01/cib.xml<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_monitor_action&quot; name=&quot;pcmk_monitor_action&quot; value=&quot;metadata&quot;/&gt;<br>$ grep -ie &quot;pcmk_status_action&quot;  sos_commands/cluster/crm_report/bisalp01/cib.xml<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_status_action&quot; name=&quot;pcmk_status_action&quot; value=&quot;metadata&quot;/&gt;<br>$ grep -ie &quot;pcmk_reboot_action&quot;  sos_commands/cluster/crm_report/bisalp01/cib.xml<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_reboot_action&quot; name=&quot;pcmk_reboot_action&quot; value=&quot;off&quot;/&gt;<br>$ grep -ie &quot;host_list&quot;  sos_commands/cluster/crm_report/bisalp01/cib.xml | grep kdump<br>          &lt;nvpair id=&quot;kdump_stonith-instance_attributes-pcmk_host_list&quot; name=&quot;pcmk_host_list&quot; value=&quot;bisalp01-HB bisalp02-HB&quot;/&gt;<br><br>- A fence_kdump stonith device fails to start and fencing fails in RHEL 6 or 7 pacemaker clusters<br>  https://access.redhat.com/solutions/875883<br><br><br>-------------------------------------------------------------------------------------------------<br>Logs<br>-------------------------------------------------------------------------------------------------<br><br>We can see it did work at one point:<br>Feb 20 20:42:48 bisalp01 stonith-ng[57147]:  notice: Operation 'reboot' [61894] (call 2 from crmd.57151) for host 'bisalp02-HB' with device 'kdump_stonith' returned: 0 (OK)<br>Feb 20 20:42:48 bisalp01 stonith-ng[57147]:  notice: Call to kdump_stonith for bisalp02-HB on behalf of crmd.57151@bisalp01-HB: OK (0)<br><br><br>Here are a couple failures that I noticed<br>--------<br>Event 1:<br>--------<br><br>This appears to be normal as the network interfaces were taking down and there would<br>no way for fence_kdump to know if kdump was in progress on node 2. <br><br>  Feb 20 23:24:39 bisalp01 kernel: ixgbe 0000:04:00.0 p7p1: NIC Link is Down<br>  Feb 20 23:24:40 bisalp01 kernel: bond2: link status definitely down for interface p7p1, disabling it<br>  Feb 20 23:24:41 bisalp01 kernel: ixgbe 0000:03:00.0 p6p1: NIC Link is Down<br>  Feb 20 23:24:41 bisalp01 kernel: bond2: link status definitely down for interface p6p1, disabling it<br>  Feb 20 23:24:41 bisalp01 kernel: bond2: now running without any active interface!<br>  [....]<br>  Feb 20 23:24:48 bisalp01 corosync[23982]: [TOTEM ] A processor failed, forming new configuration.<br>  [....]<br>  Feb 20 23:24:50 bisalp01 stonith-ng[24003]:  notice: kdump_stonith can fence (reboot) bisalp02-HB: static-list<br>  Feb 20 23:25:50 bisalp01 stonith-ng[24003]:   error: Operation 'reboot' [74179] (call 2 from crmd.24007) for host 'bisalp02-HB' <br>                  with device 'kdump_stonith' returned: -62 (Timer expired)<br>  Feb 20 23:25:50 bisalp01 stonith-ng[24003]:  notice: Call to kdump_stonith for bisalp02-HB on behalf of crmd.24007@bisalp01-HB:<br>                  Timer expired (-62)<br><br>--------<br>Event 2:<br>--------<br><br>A token was lost. The member was evicted. The first fencing agent fence_kdump did not successfully work, but the second level agent did successfully fence the cluster node.<br><br>  Feb 20 19:47:33 bisalp02 corosync[106267]: [TOTEM ] A new membership (192.168.25.92:116) was formed. Members left: 1<br>  Feb 20 19:47:33 bisalp02 corosync[106267]: [TOTEM ] Failed to receive the leave message. failed: 1<br>  Feb 20 19:47:33 bisalp02 attrd[106286]:  notice: crm_update_peer_proc: Node bisalp01-HB[1] - state is now lost (was member)<br>  Feb 20 19:47:33 bisalp02 attrd[106286]:  notice: Removing all bisalp01-HB attributes for attrd_peer_change_cb<br>  Feb 20 19:47:33 bisalp02 attrd[106286]:  notice: Removing bisalp01-HB/1 from the membership list<br>  Feb 20 19:47:33 bisalp02 attrd[106286]:  notice: Purged 1 peers with id=1 and/or uname=bisalp01-HB from the membership cache<br>  Feb 20 19:47:33 bisalp02 corosync[106267]: [QUORUM] Members[1]: 2<br>  [....]<br>  Feb 20 19:47:35 bisalp02 crmd[106288]:  notice: Initiating action 34: start kdump_stonith_start_0 on bisalp02-HB (local)<br>  Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Client crmd.106288.33f2ee80 wants to fence (reboot) 'bisalp01-HB' with device '(any)'<br>  Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Initiating remote operation reboot for bisalp01-HB: <br>                  f9bad540-7598-4f13-ac59-584b37a1b53c (0)<br>  Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: kdump_stonith can fence (reboot) bisalp01-HB: static-list<br>  Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: ipmilan_stonith1 can fence (reboot) bisalp01-HB: static-list<br>  Feb 20 19:47:35 bisalp02 stonith-ng[106284]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' <br>                  action instead<br>  Feb 20 19:47:35 bisalp02 fence_kdump[108819]: failed to get node 'bisalp01-HB'<br>  Feb 20 19:47:36 bisalp02 fence_kdump[108820]: failed to get node 'bisalp01-HB'<br>  Feb 20 19:47:36 bisalp02 stonith-ng[106284]:   error: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with<br>                  device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>  Feb 20 19:47:36 bisalp02 stonith-ng[106284]: warning: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>  Feb 20 19:47:36 bisalp02 stonith-ng[106284]:  notice: Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB:<br>                  Generic   Pacemaker error (-201)<br><br>  We eventually received the ack from bisalp01 which verifies we using kdump kernel on bisalp01.<br>  Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'<br><br>  Then we attempt to fence with agent 2 which works.<br>  Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Operation 'reboot' [108847] (call 2 from crmd.106288) for host 'bisalp01-HB'  <br>                  with device 'ipmilan_stonith1' returned: 0 (OK)<br>  Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Call to ipmilan_stonith1 for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: OK (0)<br>  Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Operation reboot of bisalp01-HB by bisalp02-HB for crmd.106288@bisalp02-HB.f9bad540: OK<br><br><br>  The actually error in this case I believe is:<br>    Feb 20 19:47:36 bisalp02 fence_kdump[108820]: failed to get node 'bisalp01-HB'<br>  As noted in this entry:<br>      Feb 20 19:47:36 bisalp02 stonith-ng[106284]: warning: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br><br>  This occurs in two different places in code:<br>   fence-agents-4.0.11/fence/agents/kdump/fence_kdump_send.c#223 # When send is done by kdump kernel.<br>   fence-agents-4.0.11/fence/agents/kdump/fence_kdump.c#518      # When the fence_dump binary is ran which is where this message<br>                                                                 # is coming from.<br>  The fence_dump was apparently still running as it processed this message which would have yielded a fencing success <br>  if it had not had the failures before <br>       Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'<br><br><br>  The default stonith-timeout is 60s. It does not appear that we timed-out. <br><br><br>  • We never received the fence_kdump saying it is waiting, but we do in previous test.<br>     # grep fence_kdump sos_commands/cluster/crm_report/bisalp02/cluster-log.txt<br>     Feb 20 14:23:07 bisalp02 fence_kdump[111001]: waiting for message from '192.168.25.91'<br>     Feb 20 14:23:11 bisalp02 fence_kdump[111001]: received valid message from '192.168.25.91'<br>     Feb 20 18:35:21 bisalp02 fence_kdump[30072]: waiting for message from '192.168.25.91'<br>     Feb 20 18:35:33 bisalp02 fence_kdump[30392]: failed to get node 'bisalp01-HB'<br>     Feb 20 18:35:42 bisalp02 fence_kdump[30522]: failed to get node 'bisalp01-HB'<br>     Feb 20 18:36:07 bisalp02 fence_kdump[31033]: failed to get node 'bisalp01-HB'<br>     Feb 20 18:36:24 bisalp02 fence_kdump[31402]: failed to get node 'bisalp01-HB'<br>     Feb 20 18:36:29 bisalp02 fence_kdump[31473]: failed to get node 'bisalp02-HB'<br>     Feb 20 18:51:22 bisalp02 fence_kdump[49020]: failed to get node 'bisalp01-HB'<br><br>     Feb 20 19:47:35 bisalp02 stonith-ng[106284]: warning: Agent 'fence_kdump' does not advertise <br>                     support for 'reboot', performing 'off' action instead<br>     Feb 20 19:47:35 bisalp02 fence_kdump[108819]: failed to get node 'bisalp01-HB'<br>     Feb 20 19:47:36 bisalp02 fence_kdump[108820]: failed to get node 'bisalp01-HB'<br>     Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'<br>  <br> • We get that message twice, so fence_kdump had to have failed twice cause `1` is returned when that message occurs. <br>   In addition we get ack on receiving a message after the stonith-ng error messages so fence_kdump had to be running<br>   if we got those.  <br><br>  That part I am still trying to figure out. <br>  - Why wasn't the other errors logged or maybe in different log file?<br>  - Why in last test we did not get &quot;waiting&quot; message but did get &quot;received&quot; message?<br><br>-------------------------------------------------------------------------------------------------<br><br>1) How are they testing this? <br>   <br>   Taking a network interface down is not a valid test as noted here if that is what they are doing:<br>   - What is the proper way to simulate a network failure on a RHEL Cluster?<br>     https://access.redhat.com/solutions/79523<br>   <br>   In the logs I see that all network interfaces went down.<br>   Feb 20 23:24:39 bisalp01 kernel: ixgbe 0000:04:00.0 p7p1: NIC Link is Down<br>   Feb 20 23:24:40 bisalp01 kernel: bond2: link status definitely down for interface p7p1, disabling it<br>   Feb 20 23:24:41 bisalp01 kernel: ixgbe 0000:03:00.0 p6p1: NIC Link is Down<br>   Feb 20 23:24:41 bisalp01 kernel: bond2: link status definitely down for interface p6p1, disabling it<br>   Feb 20 23:24:41 bisalp01 kernel: bond2: now running without any active interface!<br>   [....]<br>   Feb 20 23:24:48 bisalp01 corosync[23982]: [TOTEM ] A processor failed, forming new configuration.<br><br>   What are there reproducer steps to cause the error for fence_kdump to occur?<br><br><br>2) Can you verify which clusters can crash into a kdump kernel and then be fenced?<br><br>3) Based on logs it appears at some both were working. Has at any point fence_kdump <br>   successfully worked on all nodes?<br><br>4) Can you tarball your /var/log directory up on both nodes and attach to ticket?<br><br><publishedDate>2016-03-09T19:27:26Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GdnCHIAZ"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-03-09T05:15:45Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-03-09T05:15:45Z</b><br><br>안녕하세요.<br>먼저 혼선이 있을 수 있는 부분에대해서,<br>충분한 사전 설명 드리지 못한 점에대해 양해 부탁 드립니다.<br><br>정확하진 않지만, <br>제가 보았을 때 같은 패턴으로 동일한 이슈를 겪고있다고 판단하였기에 <br>bisalp02와 eccalp02 시스템 중에서 빠르게 확인해 볼 수 있는 설정이나 정보를 혼합하여 전달 드린 것 같습니다.<br>가능한 동일한 시스템에서 정보를 수집해보도록 하겠으며,<br>다시 한번 양해 부탁 드립니다.<br><br><br>또한 문의주신 내용과 로그가 일치하지 않았다라고 하셨는데, <br>로그에서 어떤 부분의 로그가 일치하지 않는다고 생각하시는지 알려주시면<br>현장에서 확인 가능한 범위내 시스템 구성의 차이점을 찾아보고, 전달드릴 수 있는 내용이 있는지 확인 해보겠습니다.<br><br><br>저는 하기의 로그 패턴에서, bisalp02와 eccclp02 시스템이 동일한 원인에 기인한  이슈를 겪고 있다고 추정하고 있습니다.<br><br><br>#################### 전달 드린 bisalp02 로그 ####################<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]: error: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) <br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]: warning: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ] # &lt;--------------------------------------------- WATCH THIS <br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]: notice: Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: Generic Pacemaker error (-201) <br><br><br>#################### TAM께서 분석 해주신 bisalp02 로그의 일부 ####################<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng: error: log_operation: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) <br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng: warning: log_operation: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng: notice: process_remote_stonith_exec: Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: Generic Pacemaker error (-201)<br><br>#################### 최초 케이스에 등록한 eccclp02 로그의 일부 ####################<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:   error: Operation 'reboot' [2051] (call 2 from crmd.90546) for host 'eccclp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]: warning: kdump_stonith:2051 [ [error]: failed to get node 'eccclp01-HB' ]<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:  notice: Call to kdump_stonith for eccclp01-HB on behalf of crmd.90546@eccclp02-HB: Generic Pacemaker error (-201)<br>======================<br>다시 요청하신, 해당 시스템의 요청하신 클러스터 정보 입니다.<br><br>1. # pcs status <br><br>Cluster Name: bisa_cluster<br> Last updated: Tue Mar  9 13:00:18 2016  Last change: Fri Feb 26 16:53:10 2016 by root via cibadmin on bisalp01-HB<br> Stack: corosync<br> Current DC: bisalp02-HB (version 1.1.13-10.el7-44eb2dd) - partition with quorum<br> 2 nodes and 8 resources configured<br><br> Online: [ bisalp01-HB bisalp02-HB ]<br><br><br>Full list of resources: <br><br>Clone Set: rsc_ping-clone [rsc_ping] <br>  Started: [ bisalp01-HB bisalp02-HB ] <br>Resource Group: grp1 <br>  rsc_vip1 (ocf::heartbeat:IPaddr2): Started bisalp01-HB <br>  rsc_ascs1 (ocf::sfmi:MySAP): (target-role:Stopped) Stopped <br>kdump_stonith (stonith:fence_kdump): Started bisalp02-HB <br>ipmilan_stonith1 (stonith:fence_ipmilan): Started bisalp02-HB <br>ipmilan_stonith2 (stonith:fence_ipmilan): Started bisalp01-HB <br>Resource Group: grp2 <br>  rsc_vip2 (ocf::heartbeat:IPaddr2): Started bisalp02-HB <br><br>PCSD Status:<br>  bisalp01-HB: Online<br>  bisalp02-HB: Online<br><br><br>Daemon Status: <br>  corosync: active/disabled <br>  pacemaker: active/disabled <br>  pcsd: active/enabled<br>======================<br>2. # pcs stonith show<br> Cluster Name: bisa_cluster <br><br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;bisalp01-HB bisalp02-HB&quot; pcmk_off_timeout=90<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=bisalp01-HB ipaddr=42.1.240.135 login=redhat passwd=bisalp01!1 lanplus=on auth=password delay=15<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=bisalp02-HB ipaddr=42.1.240.136 login=redhat passwd=bisalp02!1 lanplus=on auth=password<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br> Node: bisalp01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: bisalp02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br><br><br> <br><br>지원에 감사 드립니다.<br><br><publishedDate>2016-03-09T05:15:45Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GdilvIAB"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-03-09T00:34:43Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-03-09T00:34:43Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>해당 케이스의 문의를 처음 eccclp0x를 가지고 문의주시고 sosreport는 bisalp0x 에대한 자료입니다.<br><br>문의주신 내용과 로그가 일치하지 않았는데 또 다시 eccclp0x의 데이터를 주시면 해당 이슈를 추적하기가 어렵습니다.<br><br><br>문의주시는 내용과 데이터를 일치하여 주시기 바랍니다. 현재 bisalp0x의 로그를 확인한 바, 마지막에 요청드린 내용을 해당 시스템에서 수집하여 주시기 바랍니다.<br>======================<br>감사합니다.<br><br><publishedDate>2016-03-09T00:34:43Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GdVckIAF"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-03-08T05:47:15Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-03-08T05:47:15Z</b><br><br>안녕하세요.<br><br>요청하신 정보 회신 드립니다.<br><br><br>1. # pcs status<br><br>Cluster name: eccc_cluster<br>Last updated: Fri Feb 26 16:59:37 2016<br><br>Last change: Fri Feb 26 16:54:31 2016 by root via crm_resource on eccclp01-HB<br>Stack: corosync<br>Current DC: eccclp01-HB (version 1.1.13-10.el7-44eb2dd) - partition with quorum<br>2 nodes and 8 resources configured<br><br>Online: [ eccclp01-HB eccclp02-HB ]<br><br>Full list of resources:<br><br> Clone Set: rsc_ping-clone [rsc_ping]<br>     Started: [ eccclp01-HB eccclp02-HB ]<br> Resource Group: grp1<br>     rsc_vip1<br>(ocf::heartbeat:IPaddr2):<br>Started eccclp01-HB<br>     rsc_ascs1<br>(ocf::sfmi:MySAP):<br>(target-role:Stopped) Stopped<br> kdump_stonith<br>(stonith:fence_kdump):<br>Started eccclp02-HB<br> ipmilan_stonith1<br>(stonith:fence_ipmilan):<br>Started eccclp02-HB<br> ipmilan_stonith2<br>(stonith:fence_ipmilan):<br>Started eccclp01-HB<br> Resource Group: grp2<br>     rsc_vip2<br>(ocf::heartbeat:IPaddr2):<br>Started eccclp02-HB<br><br>PCSD Status:<br>  eccclp01-HB: Online<br>  eccclp02-HB: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br>======================<br>2. # pcs stonith show<br>Cluster Name: eccc_cluster<br><br>Stonith Devices: <br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;eccclp01-HB eccclp02-HB&quot; pcmk_off_timeout=90 <br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp01-HB ipaddr=42.1.240.114 login=redhat passwd=eccclp01!1 lanplus=on auth=password delay=15 <br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp02-HB ipaddr=42.1.240.115 login=redhat passwd=eccclp02!1 lanplus=on auth=password <br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels: <br><br> Node: eccclp01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: eccclp02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>======================<br>(HAN, JINKOO에 회신)<br>&gt; 안녕하세요,<br>&gt; Red Hat 한진구 입니다.<br>&gt; <br>&gt; 현재 보내주신 sosreport는 cluster가 실행이 되지 않는 것인지 관련 설정 파일을 볼 수가 없는 상태입니다.<br>&gt; <br>&gt; 아래 정보를 요청드립니다.<br>&gt; <br>&gt; 1. # pcs status<br>&gt; 2. # pcs stonith show  (fence 설정 정보)<br>&gt; <br>&gt; <br>&gt; 감사합니다.<br><br><publishedDate>2016-03-08T05:47:15Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GciGcIAJ"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-03-04T08:52:03Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-03-04T08:52:03Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>현재 보내주신 sosreport는 cluster가 실행이 되지 않는 것인지 관련 설정 파일을 볼 수가 없는 상태입니다.<br><br>아래 정보를 요청드립니다.<br><br>1. # pcs status<br>2. # pcs stonith show  (fence 설정 정보)<br><br><br>감사합니다.<br><br><publishedDate>2016-03-04T08:52:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GcH8yIAF"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-03-03T05:36:02Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-03-03T05:36:02Z</b><br><br>추가로 Message를 남겨 드립니다.<br><br>1. 더욱 주의깊게 보아야 할 것은, &quot;WATCH THIS&quot;로 표시해 두었습니다.<br><br>2. node1 인 bisalp01의 fence IP가 192.168.25.91이며, fence ack 메세지를 받았다는 로그가 node2인 bisalp02에도 남아있습니다.<br>Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'<br><br>3. fence ack messages를 받았음에도 불구하고, Power Fencing으로 넘어가는 것 또한 이상한 점 입니다.<br><br><br># cat /var/log/messages<br><br>Feb 20 19:47:35 bisalp02 crmd[106288]:  notice: Executing reboot fencing operation (40) on bisalp01-HB (timeout=60000)<br>Feb 20 19:47:35 bisalp02 crmd[106288]:  notice: Initiating action 34: start kdump_stonith_start_0 on bisalp02-HB (local)<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Client crmd.106288.33f2ee80 wants to fence (reboot) 'bisalp01-HB' with device '(any)'<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Initiating remote operation reboot for bisalp01-HB: f9bad540-7598-4f13-ac59-584b37a1b53c (0)<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: kdump_stonith can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: ipmilan_stonith1 can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 19:47:35 bisalp02 fence_kdump[108819]: failed to get node 'bisalp01-HB'   # &lt;------------------------------------------------------------------------------------------ WATCH THIS<br>Feb 20 19:47:36 bisalp02 fence_kdump[108820]: failed to get node 'bisalp01-HB'  # &lt;------------------------------------------------------------------------------------------ WATCH THIS<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]:   error: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]: warning: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ] # &lt;--------------------------------------------- WATCH THIS<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]:  notice: Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: Generic Pacemaker error (-201)<br>Feb 20 19:47:37 bisalp02 crmd[106288]:  notice: Operation kdump_stonith_start_0: ok (node=bisalp02-HB, call=63, rc=0, cib-update=94, confirmed=true)<br>Feb 20 19:47:37 bisalp02 crmd[106288]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on bisalp02-HB (local)<br>Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'  # &lt;----------------------------------------------------------------------------- WATCH THIS<br>Feb 20 19:48:01 bisalp02 systemd: Started Session 2343 of user root.<br>======================<br><br>(Kim, Seung-Pil에 회신)<br>&gt; 안녕하세요.<br>&gt; 회신 감사합니다.<br>&gt; <br>&gt; <br>&gt; 1. 분석하여 주신 메세지에서도, 본 이슈의 
제목과 같은 메세지인 , <br>&gt;  [ [error]: failed to get node 'bisalp01-HB' ] 와 host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) 가 나타남을 보실 수 있습니다.<br>&gt; <br>&gt; # cat /var/log/cluster/corosync.log<br>&gt; ...생략...<br>&gt; Feb 20 19:47:36 [106284] bisalp02 stonith-ng: error: log_operation: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) <br>&gt; Feb 20 19:47:36 [106284] bisalp02 stonith-ng: warning: log_operation: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>&gt; ...생략...<br>&gt; <br>&gt; <br>&gt; <br>&gt; 2. 본 이슈의 
제목과 같은 메세지가,  node2의 /var/log/messages에 기록되어 있습니다.<br>&gt; <br>&gt; # cat /var/log/messages<br>&gt; ...생략...<br>&gt; Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Initiating remote operation reboot for bisalp01-HB: f9bad540-7598-4f13-ac59-584b37a1b53c (0)<br>&gt; Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: kdump_stonith can fence (reboot) bisalp01-HB: static-list<br>&gt; Feb 20 19:47:35 bisalp0<br><br><publishedDate>2016-03-03T05:36:02Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GcH4NIAV"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-03-03T05:26:41Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-03-03T05:26:41Z</b><br><br>안녕하세요.<br>회신 감사합니다.<br><br><br>1. 분석하여 주신 메세지에서도, 본 이슈의 
제목과 같은 메세지인 , <br> [ [error]: failed to get node 'bisalp01-HB' ] 와 host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) 가 나타남을 보실 수 있습니다.<br><br># cat /var/log/cluster/corosync.log<br>...생략...<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng: error: log_operation: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error) <br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng: warning: log_operation: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>...생략...<br>======================<br>2. 본 이슈의 
제목과 같은 메세지가,  node2의 /var/log/messages에 기록되어 있습니다.<br><br># cat /var/log/messages<br>...생략...<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: Initiating remote operation reboot for bisalp01-HB: f9bad540-7598-4f13-ac59-584b37a1b53c (0)<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: kdump_stonith can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]:  notice: ipmilan_stonith1 can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 bisalp02 stonith-ng[106284]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>###################################################################################################################<br>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;<br>Feb 20 19:47:35 bisalp02 fence_kdump[108819]: failed to get node 'bisalp01-HB'<br>Feb 20 19:47:36 bisalp02 fence_kdump[108820]: failed to get node 'bisalp01-HB'<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]:   error: Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]: warning: kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>Feb 20 19:47:36 bisalp02 stonith-ng[106284]:  notice: Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: Generic Pacemaker error (-201)<br>&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;<br>###################################################################################################################<br>Feb 20 19:47:37 bisalp02 crmd[106288]:  notice: Operation kdump_stonith_start_0: ok (node=bisalp02-HB, call=63, rc=0, cib-update=94, confirmed=true)<br>Feb 20 19:47:37 bisalp02 crmd[106288]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on bisalp02-HB (local)<br>Feb 20 19:47:48 bisalp02 fence_kdump[30072]: received valid message from '192.168.25.91'<br>Feb 20 19:48:01 bisalp02 systemd: Started Session 2343 of user root.<br>Feb 20 19:48:01 bisalp02 systemd: Starting Session 2343 of user root.<br>Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Operation 'reboot' [108847] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'ipmilan_stonith1' returned: 0 (OK)<br>Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Call to ipmilan_stonith1 for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: OK (0)<br>Feb 20 19:48:05 bisalp02 stonith-ng[106284]:  notice: Operation reboot of bisalp01-HB by bisalp02-HB for crmd.106288@bisalp02-HB.f9bad540: OK<br>Feb 20 19:48:05 bisalp02 crmd[106288]:  notice: Stonith operation 2/40:20:0:79dd239b-eebd-4114-a0ce-2fcaf957e374: OK (0)<br>Feb 20 19:48:05 bisalp02 crmd[106288]:  notice: Peer bisalp01-HB was terminated (reboot) by bisalp02-HB for bisalp02-HB: OK (ref=f9bad540-7598-4f13-ac59-584b37a1b53c) by client crmd.106288<br>...생략...<br>======================<br>3. 상기 /var/log/messages 파일은 본 케이스의 코멘트에 전달해드린, #01588943 케이스에 업로드 되어있는 sosreport-bisalp02-20160220-var_log.tar.bz2 에 있습니다.<br> - 본 이슈와 동일한 증상을 겪는 시스템 bisalp01 bisalp02의 sosreport를 대체하여 전달 드렸습니다. <br>   ( 케이스 https://access.redhat.com/support/cases/#/case/01588943 에 등록되어 있습니다. )<br><br>- 해당 케이스에 등록된 시스템 로그 /var/log/messages 를 확인하여 보면 아래와 같은 내용이 있습니다.<br>   파일명 : sosreport-bisalp02-20160220-var_log.tar.bz2 (12.2 MB) <br>   https://api.access.redhat.com/rs/cases/01588943/attachments/8644bcdc-767f-4c34-af19-3f5fdfd86886<br>   SHA-256: 498e950f94105c836ef34415ec3447a2beadc59f78936532c1924cb93dadf2cb<br><br><br>확인 부탁 드립니다.<br>고맙습니다.<br><br>(HAN, JINKOO에 회신)<br>&gt; 안녕하세요,<br>&gt; Red Hat 한진구 입니다.<br>&gt; <br>&gt; 우선 보내주신 로그의 데이터가 초기 케이스 이슈와 시간차가 있는 것으로 보입니다.<br>&gt; 그리고 또한 노드가 kdump가 동작하고 안한 것에 대한 노드의 이름도 정확하게 일치하지 않는 것으로 보임에 따라 우선 로그상의 내용을 살펴본 것을 업데이트 드립니다.<br>&gt; <br>&gt; Node2에서 fencing을 처리하고 있으나, 실제 노드 1은 cluster에 연결이 되지 않은 상태인것으로 보입니다.<br>&gt; <br>&gt; * Note2 - bisalp02<br>&gt; Feb 20 19:47:35 [106288] bisalp02       crmd:   notice: te_fence_node:  Executing reboot fencing operation (40) on bisalp01-HB (timeout=60000)<br>&gt; Feb 20 19:47:35 [106288] bisalp02       crmd:   notice: te_rsc_command: Initiating action 34: start kdump_stonith_start_0 on bisalp02-HB (local)<br>&gt; Feb 20 19:47:35 [106288] bisalp02       crmd:     info: do_lrm_rsc_op:  Performing key=34:20:0:79dd239b-eebd-4114-a0ce-2fcaf957e374 op=kdump_stonith_start_0<br>&gt; Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: handle_request: Client crmd.106288.33f2ee80 wants to fence (reboot) 'bisalp01-HB' with device '(any)'<br>&gt; Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: initiate_remote_stonith_op:     Initiating remote operation reboot for bisalp01-HB: f9b<br><br><publishedDate>2016-03-03T05:26:41Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GbmZ6IAJ"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-03-02T10:15:18Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-03-02T10:15:18Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>우선 보내주신 로그의 데이터가 초기 케이스 이슈와 시간차가 있는 것으로 보입니다.<br>그리고 또한 노드가 kdump가 동작하고 안한 것에 대한 노드의 이름도 정확하게 일치하지 않는 것으로 보임에 따라 우선 로그상의 내용을 살펴본 것을 업데이트 드립니다.<br><br>Node2에서 fencing을 처리하고 있으나, 실제 노드 1은 cluster에 연결이 되지 않은 상태인것으로 보입니다.<br><br>* Note2 - bisalp02<br>Feb 20 19:47:35 [106288] bisalp02       crmd:   notice: te_fence_node:  Executing reboot fencing operation (40) on bisalp01-HB (timeout=60000)<br>Feb 20 19:47:35 [106288] bisalp02       crmd:   notice: te_rsc_command: Initiating action 34: start kdump_stonith_start_0 on bisalp02-HB (local)<br>Feb 20 19:47:35 [106288] bisalp02       crmd:     info: do_lrm_rsc_op:  Performing key=34:20:0:79dd239b-eebd-4114-a0ce-2fcaf957e374 op=kdump_stonith_start_0<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: handle_request: Client crmd.106288.33f2ee80 wants to fence (reboot) 'bisalp01-HB' with device '(any)'<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: initiate_remote_stonith_op:     Initiating remote operation reboot for bisalp01-HB: f9bad540-7598-4f13-ac59-584b37a1b53c (0)<br>Feb 20 19:47:35 [106285] bisalp02       lrmd:     info: log_execute:    executing - rsc:kdump_stonith action:start call_id:63<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: can_fence_host_with_device:     kdump_stonith can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:   notice: can_fence_host_with_device:     ipmilan_stonith1 can fence (reboot) bisalp01-HB: static-list<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:     info: process_remote_stonith_query:   Query result 1 of 1 from bisalp02-HB for bisalp01-HB/reboot (2 devices) f9bad540-7598-4f13-ac59-584b37a1b53c<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:     info: call_remote_stonith:    Total remote op timeout set to 120 for fencing of node bisalp01-HB for crmd.106288.f9bad540<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:     info: call_remote_stonith:    Requesting that bisalp02-HB perform op reboot bisalp01-HB with kdump_stonith for crmd.106288 (72s)<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:  warning: stonith_device_execute: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 19:47:35 [106284] bisalp02 stonith-ng:     info: internal_stonith_action_execute:        Attempt 2 to execute fence_kdump (off). remaining timeout is 60<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:     info: update_remaining_timeout:       Attempted to execute agent fence_kdump (off) the maximum number of times (2) allowed<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:    error: log_operation:  Operation 'reboot' [108820] (call 2 from crmd.106288) for host 'bisalp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:  warning: log_operation:  kdump_stonith:108820 [ [error]: failed to get node 'bisalp01-HB' ]<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:     info: make_args:      Substituting action 'metadata' for requested operation 'monitor'<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:   notice: process_remote_stonith_exec:    Call to kdump_stonith for bisalp01-HB on behalf of crmd.106288@bisalp02-HB: Generic Pacemaker error (-201)<br>Feb 20 19:47:36 [106284] bisalp02 stonith-ng:     info: call_remote_stonith:    Requesting that bisalp02-HB perform op reboot bisalp01-HB with ipmilan_stonith1 for crmd.106288 (72s)<br>Feb 20 19:47:36 [106285] bisalp02       lrmd:     info: log_finished:   finished - rsc:kdump_stonith action:start call_id:63  exit-code:0 exec-time:1011ms queue-time:0ms<br>Feb 20 19:47:37 [106288] bisalp02       crmd:   notice: process_lrm_event:      Operation kdump_stonith_start_0: ok (node=bisalp02-HB, call=63, rc=0, cib-update=94, confirmed=true)<br><br><br>즉, fencing을 시도하였으나, 실제로 node 1이 cluster에 join되지 않은 상태였고 kdump는 여러차례 실패를 하다가 결국 다음 fence device은 ipmi를 통해서 시스템이 리부팅 된것으로 추정됩니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-03-02T10:15:18Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GazCWIAZ"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-29T02:09:20Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-29T02:09:20Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>현재 저희 전문 엔지니어와 함께 분석중에 있으며, 업데이트 내용이 있으면 바로 안내드리도록 하겠습니다.<br>======================<br>감사합니다.<br><br><publishedDate>2016-02-29T02:09:20Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GabagIAB"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-26T08:25:46Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-26T08:25:45Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>자세한 내용 감사합니다.<br><br>해당 내용을 가지고 다시 저희 엔지니어들과 확인하어 답변 드리도록 하겠습니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-02-26T08:25:45Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GaaU3IAJ"><br>======================<br><b>생성계정 : Kim, Seung-Pil</b><br><b>생성날짜 : 2016-02-26T06:02:58Z</b><br><b>마지막 답변자 : Kim, Seung-Pil</b><br><b>마지막 수정 일자 : 2016-02-26T06:02:58Z</b><br><br>안녕하세요.<br>질문의 회신 드립니다.<br><br>1) 테스트 시나리오<br>======================<br>eccclp01( bisalp01 )<br>======================<br>eccclp02( bisalp02 )<br>1-1)<br><br>pcs cluster start<br>======================<br><br>pcs cluster start<br>1-2)<br><br>pcs status<br>======================<br><br><br>pcs status<br>1-3)<br><br>echo c &gt; /proc/sysrq-trigger<br><br>pcs status<br>1-4)<br><br>CRASH KERNEL MODE<br><br><br>pcs status &amp;&amp; tail -f /var/log/messages<br>1-5)<br><br>ls /CRASH/127-*<br><br><br>2) 두 노드의 sosreport<br>     - 본 이슈와 동일한 증상을 겪는 시스템 bisalp01 bisalp02의 sosreport를 대체하여 전달 드립니다.<br>      ( 케이스 https://access.redhat.com/support/cases/#/case/01588943 에 등록되어 있습니다. )<br><br>       파일명 : sosreport-bisalp02.20160220-20160220210605.tar.xz<br>       설    명 : sosreport - Feb 20 20시 경, bisalp02 에서 MONITORING pcs status  AND /var/log/messages 발생<br><br>       파일명 : sosreport-bisalp01.20160220-20160223173434.tar.xz (10.7 MB) <br>       설    명 : sosreport - Feb 20 20시 경, bisalp01 에  CRASH발생<br><br>       sosreport-bisalp01-20160220202845.tar.xz (9.4 MB) <br>       설    명 : sosreport - Feb 20 20시 경, bisalp01 에  CRASH 발생<br><br><br>고맙습니다.<br><br>(HAN, JINKOO에 회신)<br>&gt; 안녕하세요,<br>&gt; Red Hat 한진구 입니다.<br>&gt; <br>&gt; 로그를 분석한 결과, 추가적인 문의 및 데이터 요청이 있어서 업데이트 드립니다.<br>&gt; <br>&gt; 1. 테스트를 절차에 대한 자세한 추가적인 설명<br>&gt; 2. 두 노드의 sosreport를 첨부하여 주시기 바랍니다. 구성정보를 text로 보내주긴 하였으나 좀 더 정확하게 구성정보를 확인이 필요하여 추가적으로 요청드립니다.<br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; 감사합니다.<br><br><publishedDate>2016-02-26T06:02:58Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000GaZLpIAN"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-26T02:19:00Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-26T02:19:00Z</b><br><br>안녕하세요,<br>Red Hat 한진구 입니다.<br><br>로그를 분석한 결과, 추가적인 문의 및 데이터 요청이 있어서 업데이트 드립니다.<br><br>1. 테스트를 절차에 대한 자세한 추가적인 설명<br>2. 두 노드의 sosreport를 첨부하여 주시기 바랍니다. 구성정보를 text로 보내주긴 하였으나 좀 더 정확하게 구성정보를 확인이 필요하여 추가적으로 요청드립니다.<br>======================<br><br>감사합니다.<br><br><publishedDate>2016-02-26T02:19:00Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000GaM4yIAF"><br>======================<br><b>생성계정 : HAN, JINKOO</b><br><b>생성날짜 : 2016-02-25T02:08:53Z</b><br><b>마지막 답변자 : HAN, JINKOO</b><br><b>마지막 수정 일자 : 2016-02-25T02:08:53Z</b><br><br>안녕하세요,<br>Red Hat Technical Account Manager 한진구 입니다.<br><br>우선 문의주신 내용에 대한 부분을 검토 후, 업데으트 드리도록 하겠습니다.<br>======================<br>감사합니다.<br><br><publishedDate>2016-02-25T02:08:53Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br># Failed Info AND Messages<br>##################################################<br>요약 :<br>Cluster node's STONITH is configured  fence_kdump and fence_ipmilan. Cluseter node is not fenced by fence_kdump,  But It is fenced by power.<br>1) 클러스터에 설정된 STONITH에는 LEVEL1의 fence_kdump 와 LEVEL2의 fence_ipmilan 이 있음<br>2) node2는 정상적으로 fence_kdump 가 수행되어 vmcore가 생성됨<br>3) node1은 fence_kdump 가 설정되어 있음에도, fence_ipmilan이 수행되어 power fencing 되고 vmcore가 생성되지 않음.<br><br><br>###### node1 was fenced LOG<br><br>Feb 20 19:46:18 eccclp01 root: ############################## TEST - HA03 [ START ] ##############################<br>Feb 20 19:46:18 eccclp01 systemd-logind: Removed session 2133.<br>Feb 20 19:47:01 eccclp01 systemd: Started Session 2134 of user root.<br>Feb 20 19:47:01 eccclp01 systemd: Starting Session 2134 of user root.<br>Feb 20 20:02:46 eccclp01 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;7.4<br><br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br>Feb 20 19:46:18 eccclp02 root: ############################## TEST - HA03 [ START ] ##############################<br>Feb 20 19:46:18 eccclp02 systemd-logind: Removed session 2025.<br>Feb 20 19:47:01 eccclp02 systemd: Started Session 2026 of user root.<br>Feb 20 19:47:01 eccclp02 systemd: Starting Session 2026 of user root.<br>Feb 20 19:47:57 eccclp02 corosync[87147]: [TOTEM ] A processor failed, forming new configuration.<br>Feb 20 19:47:58 eccclp02 systemd-logind: New session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd: Started Session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd: Starting Session 2027 of user root.<br>Feb 20 19:47:58 eccclp02 systemd-logind: Removed session 2027.<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [TOTEM ] A new membership (192.168.25.22:620) was formed. Members left: 1<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [TOTEM ] Failed to receive the leave message. failed: 1<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [QUORUM] Members[1]: 2<br>Feb 20 19:47:58 eccclp02 corosync[87147]: [MAIN  ] Completed service synchronization, ready to provide service.<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Removing all eccclp01-HB attributes for attrd_peer_change_cb<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: crm_update_peer_proc: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Lost attribute writer eccclp01-HB<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 attrd[90541]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: Removing eccclp01-HB/1 from the membership list<br>Feb 20 19:47:58 eccclp02 pacemakerd[90379]:  notice: crm_reap_unseen_nodes: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 cib[90531]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: crm_reap_unseen_nodes: Node eccclp01-HB[1] - state is now lost (was member)<br>Feb 20 19:47:58 eccclp02 crmd[90546]: warning: Our DC node (eccclp01-HB) left the cluster<br>Feb 20 19:47:58 eccclp02 stonith-ng[90534]:  notice: Purged 1 peers with id=1 and/or uname=eccclp01-HB from the membership cache<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: State transition S_NOT_DC -&gt; S_ELECTION [ input=I_ELECTION cause=C_FSA_INTERNAL origin=reap_dead_nodes ]<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Feb 20 19:47:58 eccclp02 crmd[90546]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Feb 20 19:47:58 eccclp02 crmd[90546]:  notice: Notifications disabled<br>Feb 20 19:48:00 eccclp02 pengine[90544]: warning: Node eccclp01-HB will be fenced because the node is no longer part of the cluster<br>Feb 20 19:48:00 eccclp02 pengine[90544]: warning: Node eccclp01-HB is unclean<br>Feb 20 19:48:00 eccclp02 crmd[90546]:  notice: Executing reboot fencing operation (40) on eccclp01-HB (timeout=60000)<br>Feb 20 19:48:00 eccclp02 crmd[90546]:  notice: Initiating action 34: start kdump_stonith_start_0 on eccclp02-HB (local)<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: Client crmd.90546.e5cea6dd wants to fence (reboot) 'eccclp01-HB' with device '(any)'<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: Initiating remote operation reboot for eccclp01-HB: f1429d36-4ba7-4c28-8564-c51a8ff279c9 (0)<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: kdump_stonith can fence (reboot) eccclp01-HB: static-list<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]:  notice: ipmilan_stonith1 can fence (reboot) eccclp01-HB: static-list<br>Feb 20 19:48:00 eccclp02 stonith-ng[90534]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 19:48:00 eccclp02 fence_kdump[2049]: failed to get node 'eccclp01-HB'<br>Feb 20 19:48:01 eccclp02 systemd: Started Session 2028 of user root.<br>Feb 20 19:48:01 eccclp02 systemd: Starting Session 2028 of user root.<br>Feb 20 19:48:01 eccclp02 fence_kdump[2051]: failed to get node 'eccclp01-HB'<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:   error: Operation 'reboot' [2051] (call 2 from crmd.90546) for host 'eccclp01-HB' with device 'kdump_stonith' returned: -201 (Generic Pacemaker error)<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]: warning: kdump_stonith:2051 [ [error]: failed to get node 'eccclp01-HB' ]<br>Feb 20 19:48:01 eccclp02 stonith-ng[90534]:  notice: Call to kdump_stonith for eccclp01-HB on behalf of crmd.90546@eccclp02-HB: Generic Pacemaker error (-201)<br>Feb 20 19:48:02 eccclp02 crmd[90546]:  notice: Operation kdump_stonith_start_0: ok (node=eccclp02-HB, call=63, rc=0, cib-update=64, confirmed=true)<br>Feb 20 19:48:02 eccclp02 crmd[90546]:  notice: Initiating action 35: monitor kdump_stonith_monitor_60000 on eccclp02-HB (local)<br>Feb 20 19:48:13 eccclp02 fence_kdump[101793]: received valid message from '192.168.25.21'<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Operation 'reboot' [2064] (call 2 from crmd.90546) for host 'eccclp01-HB' with device 'ipmilan_stonith1' returned: 0 (OK)<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Call to ipmilan_stonith1 for eccclp01-HB on behalf of crmd.90546@eccclp02-HB: OK (0)<br>Feb 20 19:48:29 eccclp02 stonith-ng[90534]:  notice: Operation reboot of eccclp01-HB by eccclp02-HB for crmd.90546@eccclp02-HB.f1429d36: OK<br>Feb 20 19:48:29 eccclp02 crmd[90546]:  notice: Stonith operation 2/40:0:0:a16507a7-716b-4b7b-b1e3-0f0fd8f6958c: OK (0)<br>Feb 20 19:48:29 eccclp02 crmd[90546]:  notice: Peer eccclp01-HB was terminated (reboot) by eccclp02-HB for eccclp02-HB: OK (ref=f1429d36-4ba7-4c28-8564-c51a8ff279c9) by client crmd.90546<br><br><br>###### node2 was fenced LOG<br>Feb 20 20:42:55 eccclp01 corosync[17526]: [TOTEM ] A processor failed, forming new configuration.<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [TOTEM ] A new membership (192.168.25.21:628) was formed. Members left: 2<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [TOTEM ] Failed to receive the leave message. failed: 2<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Removing all eccclp02-HB attributes for attrd_peer_change_cb<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: Our peer on the DC (eccclp02-HB) is dead<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Lost attribute writer eccclp02-HB<br>Feb 20 20:42:56 eccclp01 cib[17545]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [QUORUM] Members[1]: 1<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 attrd[17548]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 corosync[17526]: [MAIN  ] Completed service synchronization, ready to provide service.<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: crm_update_peer_proc: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: State transition S_NOT_DC -&gt; S_ELECTION [ input=I_ELECTION cause=C_CRMD_STATUS_CALLBACK origin=peer_update_callback ]<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: crm_reap_unseen_nodes: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: State transition S_ELECTION -&gt; S_INTEGRATION [ input=I_ELECTION_DC cause=C_TIMER_POPPED origin=election_timeout_popped ]<br>Feb 20 20:42:56 eccclp01 pacemakerd[17541]:  notice: crm_reap_unseen_nodes: Node eccclp02-HB[2] - state is now lost (was member)<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: Removing eccclp02-HB/2 from the membership list<br>Feb 20 20:42:56 eccclp01 stonith-ng[17546]:  notice: Purged 1 peers with id=2 and/or uname=eccclp02-HB from the membership cache<br>Feb 20 20:42:56 eccclp01 crmd[17550]: warning: FSA: Input I_ELECTION_DC from do_election_check() received in state S_INTEGRATION<br>Feb 20 20:42:56 eccclp01 crmd[17550]:  notice: Notifications disabled<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Node eccclp02-HB will be fenced because the node is no longer part of the cluster<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Node eccclp02-HB is unclean<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_ping:1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_ping:1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_vip1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_dummy1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_vip2_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action rsc_dummy2_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action kdump_stonith_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action ipmilan_stonith1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Action ipmilan_stonith1_stop_0 on eccclp02-HB is unrunnable (offline)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Scheduling Node eccclp02-HB for STONITH<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Stop    rsc_ping:1#011(eccclp02-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_vip1#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_dummy1#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_vip2#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    rsc_dummy2#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Move    kdump_stonith#011(Started eccclp02-HB -&gt; eccclp01-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]:  notice: Stop    ipmilan_stonith1#011(eccclp02-HB)<br>Feb 20 20:42:58 eccclp01 pengine[17549]: warning: Calculated Transition 0: /var/lib/pacemaker/pengine/pe-warn-10.bz2<br>Feb 20 20:42:58 eccclp01 crmd[17550]:  notice: Executing reboot fencing operation (40) on eccclp02-HB (timeout=60000)<br>Feb 20 20:42:58 eccclp01 crmd[17550]:  notice: Initiating action 34: start kdump_stonith_start_0 on eccclp01-HB (local)<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: Client crmd.17550.73aec11b wants to fence (reboot) 'eccclp02-HB' with device '(any)'<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: Initiating remote operation reboot for eccclp02-HB: 626f57a5-0bf4-4e12-872a-05b6493bde48 (0)<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: kdump_stonith can fence (reboot) eccclp02-HB: static-list<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]:  notice: ipmilan_stonith2 can fence (reboot) eccclp02-HB: static-list<br>Feb 20 20:42:58 eccclp01 stonith-ng[17546]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Feb 20 20:42:58 eccclp01 fence_kdump[22178]: waiting for message from '192.168.25.22'<br>Feb 20 20:43:01 eccclp01 systemd: Started Session 72 of user root.<br>Feb 20 20:43:01 eccclp01 systemd: Starting Session 72 of user root.<br>Feb 20 20:43:11 eccclp01 fence_kdump[22178]: received valid message from '192.168.25.22'<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Operation 'reboot' [22178] (call 2 from crmd.17550) for host 'eccclp02-HB' with device 'kdump_stonith' returned: 0 (OK)<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Call to kdump_stonith for eccclp02-HB on behalf of crmd.17550@eccclp01-HB: OK (0)<br>Feb 20 20:43:11 eccclp01 stonith-ng[17546]:  notice: Operation reboot of eccclp02-HB by eccclp01-HB for crmd.17550@eccclp01-HB.626f57a5: OK<br>Feb 20 20:43:11 eccclp01 crmd[17550]:  notice: Stonith operation 2/40:0:0:10ed0f8e-e7da-48b1-b142-9ead4591f55f: OK (0)<br>Feb 20 20:43:11 eccclp01 crmd[17550]:  notice: Peer eccclp02-HB was terminated (reboot) by eccclp01-HB for eccclp01-HB: OK (ref=626f57a5-0bf4-4e12-872a-05b6493bde48) by client crmd.17550<br><br>--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br><br>Feb 20 20:42:07 eccclp02 systemd: Starting Session 2124 of user root.<br>Feb 20 20:42:07 eccclp02 systemd-logind: Removed session 2124.<br>Feb 20 20:47:42 eccclp02 rsyslogd: [origin software=&quot;rsyslogd&quot; swVersion=&quot;7.4.7&quot; x-pid=&quot;3181&quot; x-info=&quot;http://www.rsyslog.com&quot;] start<br>Feb 20 20:47:35 eccclp02 journal: Runtime journal is using 8.0M (max allowed 4.0G, trying to leave 4.0G free of 62.6G available → current limit 4.0G).<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpuset<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpu<br>Feb 20 20:47:35 eccclp02 kernel: Initializing cgroup subsys cpuacct<br>Feb 20 20:47:35 eccclp02 kernel: Linux version 3.10.0-327.el7.x86_64 (mockbuild@x86-034.build.eng.bos.redhat.com) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Oct 29 17:29:29 EDT 2015<br>Feb 20 20:47:35 eccclp02 kernel: Command line: BOOT_IMAGE=/vmlinuz-3.10.0-327.el7.x86_64 root=/dev/mapper/vg00-root_lv ro crashkernel=512M rd.lvm.lv=vg00/root_lv rd.lvm.lv=vg00/swap_lv nomodeset biosdevname=1 rhgb quiet LANG=en_US.UTF-8 nmi_watchdog=0 transparent_hugepage=never elevator=deadline rdloaddriver=megaraid_sas rdloaddriver=lpfc vga=794<br>======================<br><br>======================<br><br>[root@eccclp01 ]# cat pcs_config <br>Cluster Name: eccc_cluster<br>Corosync Nodes:<br> eccclp01-HB eccclp02-HB <br>Pacemaker Nodes:<br> eccclp01-HB eccclp02-HB <br><br>Resources: <br> Clone: rsc_ping-clone<br>  Resource: rsc_ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: multiplier=10 host_list=42.1.231.1 <br>   Operations: start interval=0s timeout=60 (rsc_ping-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_ping-stop-interval-0s)<br>               monitor interval=10 timeout=60 (rsc_ping-monitor-interval-10)<br> Group: grp1<br>  Resource: rsc_vip1 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.1.231.23 <br>   Operations: start interval=0s timeout=20s (rsc_vip1-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip1-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip1-monitor-interval-10s)<br>  Resource: rsc_dummy1 (class=ocf provider=heartbeat type=Dummy)<br>   Operations: start interval=0s timeout=20 (rsc_dummy1-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_dummy1-stop-interval-0s)<br>               monitor interval=10 timeout=20 (rsc_dummy1-monitor-interval-10)<br> Group: grp2<br>  Resource: rsc_vip2 (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=42.1.231.24 <br>   Operations: start interval=0s timeout=20s (rsc_vip2-start-interval-0s)<br>               stop interval=0s timeout=20s (rsc_vip2-stop-interval-0s)<br>               monitor interval=10s timeout=20s (rsc_vip2-monitor-interval-10s)<br>  Resource: rsc_dummy2 (class=ocf provider=heartbeat type=Dummy)<br>   Operations: start interval=0s timeout=20 (rsc_dummy2-start-interval-0s)<br>               stop interval=0s timeout=20 (rsc_dummy2-stop-interval-0s)<br>               monitor interval=10 timeout=20 (rsc_dummy2-monitor-interval-10)<br><br>Stonith Devices: <br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;eccclp01-HB eccclp02-HB&quot; <br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp01-HB ipaddr=42.1.240.114 login=redhat passwd=eccclp01!1 lanplus=on auth=password delay=15 <br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_check=static-list pcmk_host_list=eccclp02-HB ipaddr=42.1.240.115 login=redhat passwd=eccclp02!1 lanplus=on auth=password <br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels: <br><br> Node: eccclp01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: eccclp02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: grp1<br>    Enabled on: eccclp01-HB (score:10) (id:location-grp1-eccclp01-HB-10)<br>  Resource: grp2<br>    Enabled on: eccclp02-HB (score:10) (id:location-grp2-eccclp02-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: eccclp01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: eccclp02-HB (score:-INFINITY) (id:const_ipmilan2)<br>  Resource: rsc_dummy1<br>    Enabled on: eccclp01-HB (score:10) (id:location-rsc_dummy1-eccclp01-HB-10)<br>  Resource: rsc_dummy2<br>    Enabled on: eccclp02-HB (score:10) (id:location-rsc_dummy2-eccclp02-HB-10)<br>  Resource: rsc_vip1<br>    Enabled on: eccclp01-HB (score:10) (id:location-rsc_vip1-eccclp01-HB-10)<br>    Constraint: location-rsc_vip1<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip1-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip1-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip1-rule-expr-1)<br>  Resource: rsc_vip2<br>    Enabled on: eccclp02-HB (score:10) (id:location-rsc_vip2-eccclp02-HB-10)<br>    Constraint: location-rsc_vip2<br>      Rule: score=-INFINITY boolean-op=or  (id:location-rsc_vip2-rule)<br>        Expression: pingd lt 1  (id:location-rsc_vip2-rule-expr)<br>        Expression: not_defined pingd  (id:location-rsc_vip2-rule-expr-1)<br>Ordering Constraints:<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: eccc_cluster<br> dc-version: 1.1.13-10.el7-44eb2dd<br> have-watchdog: false<br> maintenance-mode: false<br> no-quorum-policy: stop<br> stonith-enabled: true<br>======================<br>[root@eccclp01 ]# cat /etc/kdump.conf <br>path /CRASH<br>core_collector makedumpfile -c -d 31 -F<br>fence_kdump_nodes 192.168.25.22<br><br><br>[root@eccclp02 ]# cat /etc/kdump.conf <br>path /CRASH<br>core_collector makedumpfile -c -d 31 -F<br>fence_kdump_nodes 192.168.25.21<br><br><br>[root@eccclp01 ]# cat /etc/hosts <br><br># 2016.01.18 - Red Hat for HA ( HeartBeat )<br>192.168.25.21<br>eccclp01-HB<br>192.168.25.22<br>eccclp02-HB<br><br># 2016.01.18 - Red Hat for HA ( Service )<br>42.1.231.21<br>eccclp01<br>42.1.231.21<br>eccclp02</issue><environment>HA on RHEL7</environment><periodicityOfIssue>##################################################<br># Re-produced<br>##################################################<br>It is appeared when node1 is fenced by node2 through kdump.</periodicityOfIssue><cep>false</cep><folderName>ERP-SAP-ASCS_ERS</folderName></case>