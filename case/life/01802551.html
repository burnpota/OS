======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2017-03-02T08:43:43Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2017-04-24T10:01:26Z</b><br><b>id : 500A000000WVJLLIA5</b><br>======================<br><br><b><font size=15>
제목  : Multipath devices of three nodes used as oracle asm are not shown equally [구성문의]fdisk, label정보값
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요.<br>SLDIDADL01, SLDIDADL02, SLDIDADL03을 현재 share disk를 이용하여<br>oracle asm으로 구성 후 사용중입니다.<br><br>Q1)SLDIDADL01 에 multipath -ll DG_BACKUP4를 보면<br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-21 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdn  8:208  active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br>보이고 개별 블럭디바이스를 보면<br>Disk /dev/sdn: 107.4 GB, 107374510080 bytes<br>53 heads, 54 sectors/track, 73275 cylinders<br>Units = cylinders of 2862 * 512 = 1465344 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0xe1d8815e<br><br>   Device Boot      Start         End      Blocks   Id  System<br>/dev/sdn1               1       73276   104856896   83  Linux<br><br><br>Disk /dev/sdab: 107.4 GB, 107374510080 bytes<br>53 heads, 54 sectors/track, 73275 cylinders<br>Units = cylinders of 2862 * 512 = 1465344 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0xe1d8815e<br><br>    Device Boot      Start         End      Blocks   Id  System<br>/dev/sdab1               1       73276   104856896   83  Linux<br><br>되어있습니다.<br><br>그런데 <br>SLDIDADL02 을보면 <br><br>G_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-32 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdy  65:128 active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br><br>Disk /dev/sdy: 107.4 GB, 107374510080 bytes<br>255 heads, 63 sectors/track, 13054 cylinders<br>Units = cylinders of 16065 * 512 = 8225280 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0x00000000<br><br>Disk /dev/sdab: 107.4 GB, 107374510080 bytes<br>255 heads, 63 sectors/track, 13054 cylinders<br>Units = cylinders of 16065 * 512 = 8225280 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0x00000000<br><br>파티션이 보이지 않습니다.<br><br>그리고 <br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-32 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdy  65:128 active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br><br>Disk /dev/sdy: 107.4 GB, 107374510080 bytes<br>255 heads, 63 sectors/track, 13054 cylinders<br>Units = cylinders of 16065 * 512 = 8225280 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0x00000000<br><br>Disk /dev/sdab: 107.4 GB, 107374510080 bytes<br>255 heads, 63 sectors/track, 13054 cylinders<br>Units = cylinders of 16065 * 512 = 8225280 bytes<br>Sector size (logical/physical): 512 bytes / 512 bytes<br>I/O size (minimum/optimal): 512 bytes / 512 bytes<br>Disk identifier: 0x00000000<br><br>보면 1번과 실린더를 비교하였을때 서로 다르게 보이는 문제가 있습니다.<br><br>위의 사항에 대해서 확인 부탁드리며<br>현재<br>oracleasm에서 SLDIDADL01은 정상적인 실행이 되지 않고 있으며<br>SLDIDADL02, SLDIDADL03은 정상적으로 실행중에 있습니다.<br><br>3개의 노드의 디스크 정보를 동일하게 맞출수 있는 방법을 알려주시기 바랍니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.6</b><br><b>타입  : Configuration issue</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 4 (Low)</b><br>======================<br><comment id="a0aA000000JCmChIAL"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-04-07T05:09:49Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-04-07T05:09:48Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>네, 그렇습니다. 멀티패스로 업데이트 하고 난 이후에는 이전의 세가지 예제중 하나를 사용하셔도 무리가 없을것입니다.<br><br>감사합니다.<br><br><publishedDate>2017-04-07T05:09:48Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JCm8uIAD"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-04-07T05:00:05Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-04-07T05:00:05Z</b><br><br>답변감사합니다. <br>그럼 kpartx, partprobe사이에서 <br>권장의 개념이 없이 아무것이나 되는 것을 사용하면 된다는 말씀이신가요?<br><br><publishedDate>2017-04-07T05:00:05Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000JCm2hIAD"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-04-07T04:42:39Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-04-07T04:42:38Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>device-mapper-multipath-0.4.9-93.el6 이상버전의 multipath 환경이라면 아래의 3가지 모두가 정상동작 할것입니다.<br><br>$ kpartx -pp -av /dev/mapper/mpathdp<br>$ partprobe /dev/mapper/mpathdp<br>$ kpartx -av /dev/dm-8<br><br><br>감사합니다.<br><br><publishedDate>2017-04-07T04:42:38Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JClzOIAT"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-04-07T04:35:51Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-04-07T04:35:51Z</b><br><br>답변감사합니다.<br>그렇다면 device-mapper-multipath-0.4.9-93.el6 이상버전은<br>멀티패스 환경에서는<br>kpartx 를 권장하시는 건가요??<br><br><publishedDate>2017-04-07T04:35:51Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000JCVCSIA5"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-04-06T04:25:21Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-04-06T04:25:21Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>직면하신 이슈는 참고문서[1]에서 보고된 multipath 의 버그로 예상됩니다. 원인으로는 이미 존재하고 있는 디바이스의 이름이 새로운 디바이<br>스의 이름과 매치가 될때 kpartx 가 새로운것으로 이미 존재하는 파티션 디바이스를 덮어써서 발생을 하는것이고, 결과로서 이름 충돌이 있으면<br>가리키고 있는 디바이스를 kpartx 가 갑자기 변경을 시켜서 파티션이 p1 형태로 보여지지 않은것으로 알려져 있습니다.<br><br>이 문제를 해소하기 위해서는 최소한 device-mapper-multipath-0.4.9-93.el6 이상의 버전으로 업데이트 하시고,<br>kpartx 에 의해서 사용된 파티션 구분자를 강제지정하고, partprobe 를 사용하거나 udev 를 trigger 할 dm 디바이스를 지정하셔서 문제<br>가 해소되는지 확인해보시고, 저희 Red Hat 의 지원이 필요하시다면 다시 연락주십시오.<br><br>예제)<br><br>[root@host]# kpartx -pp -av /dev/mapper/mpathdp<br>[root@host]# partprobe /dev/mapper/mpathdp<br>[root@host]# kpartx -av /dev/dm-8<br><br><br>[ 참고문서 ]<br><br>[1] The kpartx command fails to add parition mapping to multipath devices when using partition delimiter 'p'<br>- https://access.redhat.com/solutions/1519723<br><br>감사합니다.<br><br><publishedDate>2017-04-06T04:25:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JCDQaIAP"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-04-05T03:30:17Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-04-05T03:30:17Z</b><br><br>답변감사합니다.<br>제가 궁금한 부분은 위 아래의 케이스를 보시면<br>partprobe는 global scan을 하기 때문에<br>multipath  장치는 kpartx 로<br>단일 block device partx 로 하는 것을 권장 한다고 하셨는데요<br><br>작업 당일<br>1번 노드에서 fdisk -cu 파티션을 한 후에 <br>1번, 2번, 3번 각각의 노드에서 kpartx -a /dev/mapper/[멀티패싱장치]를 하였는데<br>[멀티패싱장치]p1이라는 파티션 정보가 메모리(fdisk -l)을 하였을때는 보이나<br>/dev/mapper/ 아래의  [멀티패싱장치]p1이라는 파일이 생성되지 않았습니다.<br><br>결국 partprobe를 사용하여<br>파티션스캔을 진행하였습니다.<br><br>1. block device 를 DATA1_STG로 multipathing(1,2,3 노드)<br>2. fdisk -cu /dev/mapper/DATA1_STG 파티션 (1노드)<br>3. kpartx -a /dev/mapper/DATA1_STG(1,2,3노드)<br>방법으로 해야 하는건지<br><br>1. 각 block device 를 DATA1_STG로 multipathing(1,2,3노드)<br>2. fdisk -cu /dev/mapper/DATA1_STG 파티션(1노드)<br>3. partprobe /dev/mapper/DATA1_STG(1,2,3노드)<br>4. kpartx -a /dev/mapper/DATA1_STG(1,2,3노드)<br> 방법으로 해야 하는건지<br><br>왜<br>kpartx 만 하였을때 정상적인 DATA1_STGp1이 생성되지 않는 것인지 확인 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2017-04-05T03:30:17Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000JBuLLIA1"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-04-04T02:26:02Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-04-04T08:28:12Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>일반적으로 Oracle ASMLib 을 사용할때 device mapper shared disk 의 파티셔닝이 필요하고. 각 device mapper 볼륨이<br>만약 ( db1,db2,fra... ) 등이라면 파티셔닝이 된 이후에는 /dev/mapper/db1p1 의 형식으로 생성이 되고 이는 ../dm-11<br>로 링크가 되게 됩니다.<br><br>이렇게 생성이 된 파티션은 device mapper 디스크로 파티션을 매핑하기 위해서 각 클러스터 노드에서 kpartx -a 명령을 실행해야<br>합니다. 만약, kpartx -a 로 파티션이 더해지지 않으면 시스템을 리부팅해야 할 필요가 있습니다.<br><br>좀 더 자세한 내용과 절차는 Oracle RAC 을 위한 Best Practice 를 보시길 권장드립니다.<br><br>[ 참고문서 ]<br><br>[1] 3.4.2 Partitioning Device Mapper Shared Disks<br>- https://access.redhat.com/sites/default/files/attachments/deploying-oracle-11gr2-rac-on-rhel6_1.1.pdf<br><br>감사합니다.<br><br><publishedDate>2017-04-04T02:26:02Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000JBtygIAD"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-04-04T01:18:33Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-04-04T01:18:32Z</b><br><br>안녕하세요.<br>일요일에 해당 서버(통합인증DB)의 작업을 진행하였습니다.<br><br>작업의 결과는 정상적으로 진행이 되었는데요.<br>이상한건 말씀하신데로 kpartx를 사용해서 <br>파티션에 대한 정보를 커널로 로드를 하였는제 /device/mapper/아래에 정상적으로  file가 생성되지 않았습니다.<br>작업 순서에 대해서는 아래를 참고 부탁드립니다.<br><br><br>&lt;아래&gt;--<br>1. 스토리지 LUN 멀티패스 작업 (1호기,2호기,3호기)<br>2. fdisk -cu [멀티패싱 장치] 파티션(1호기)<br>3. kpartx -a [멀티패싱 장치] (1호기,2호기,3호기)<br>4. oracleasm createdisks [파티션된멀티패싱장치] 하였는데 해당 파일이 /dev/mapper/ 아래에 없다고나옴 (1호기,2호기,3호기)<br>5. partprobe [멀티패싱장치] 하니 /dev/mapper/xxxp1 이라는 파일이 생성됨 (1호기,2호기,3호기)<br>6. kpartx -a [멀티패싱장치] (1호기,2호기,3호기)<br>7. oracleasm작업 진행<br><br>우선 위와 같이 진행을 하였고요.<br><br>삼성측에서 <br>partprobe를 꼭 하여야하는 부분인지<br>kprartx만 하면되는 부분인지에 대한 <br>요청이 왔습니다.<br><br><br>답변 부탁드립니다.<br><br><publishedDate>2017-04-04T01:18:32Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J4HNsIAN"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-20T02:05:31Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-20T02:05:31Z</b><br><br>안녕하세요.<br><br>Red Hat Global Support Service를 이용해주셔서 감사합니다.<br><br>먼저, 케이스를 직접 종료해주셔서 감사드리며 요청하신것과 같이 본 케이스는 처리완료하도록 하겠습니다.<br>만약 본 케이스와 관련하여 추가 질문이 있으시다면 언제든지 재 오픈 하실 수 있습니다.<br><br>케이스가 처리 완료되면 고객 설문조사 메일이 발송됩니다. 고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해, 소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-20T02:05:31Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J4HHaIAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-20T01:48:58Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-20T01:48:58Z</b><br><br>안녕하세요.<br>해당 이슈에 대한 답변을 정리하여삼성측에 전달하였고<br>추가 요청이 현재까지 없는 상황입니다.<br><br>그간<br>고생 하셨습니다.<br><br>케이스는 클로징 하겠습니다.<br><br>혹시 추가 요청이 오게되면 재요청을 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-20T01:48:58Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J3Ze3IAF"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-03-16T02:47:03Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-03-16T02:47:03Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br>partx는 블락디바이스들에 대해서<br>kpartx는 멀티패싱된 장치에 대해서 <br><br>-a : add를 하겠다는 의미인가요?<br>아니면 scan 을 하겠다는 의미인가요?<br><br>답변:<br>-a 옵션은 디스크를 읽어서 모든 파티션을 추가하는 작용을 합니다. 이는 실제로 fdisk 명령어 처럼<br>파티션을 추가하는것이 아니라 디스크에서 파티션의 정보를 읽어와서 커널에 알리게 됩니다.<br>따라서 -a 옵션을 사용하시면 됩니다. <br><br>감사합니다.<br><br><publishedDate>2017-03-16T02:47:03Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J3MBgIAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-15T09:07:37Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-15T09:07:37Z</b><br><br>답변감사합니다.<br>제가 좀 혼동이 되서 회신드리는데요.<br><br>말씀하진<br>partx -a /dev/&lt;sd_device&gt; 와<br>kpartx -a /dev/mapper/&lt;mpathdevice&gt;에서<br><br>partx는 블락디바이스들에 대해서<br>kpartx는 멀티패싱된 장치에 대해서 <br><br>-a : add를 하겠다는 의미인가요?<br>아니면 scan 을 하겠다는 의미인가요?<br><br><br>예를들어<br>1호기 에서 fdisk 로 파티션후에<br>2호기에서 partx -a /dev/&lt;sd_device&gt;를하면 scan을 하는건가요? 아니면 add하겠다는 건가요?<br>그리고 멀티패싱한 장치에 대해서는 kpartx -a /dev/mapper/&lt;mpathdevice&gt;를하면 add를 하겠다는 건가요? scan 을 하겠다는의미인가요?<br><br><publishedDate>2017-03-15T09:07:37Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J3LNKIA3"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-03-15T08:00:22Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-03-15T08:00:22Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br>정리하신 부분에는 문제가 없는것으로 보입니다.<br>추가적으로 partprobe 명령어를 클로벌로 실행하는것을 추천하지 않기 때문에 주의사항으로 <br>&quot;partprobe 명령어는 글로벌로 사용하지 않는것을 권장하며 파티션 정보가 변경된 디바이스에서만<br>실행 혹은 'partx -a /dev/&lt;sd_device&gt;' 명령어를 사용하시것을 권장 드립니다.&quot; <br>를 추가해 주시면 좋을것으로 보입니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-15T08:00:22Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J3Ju7IAF"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-15T04:54:00Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-15T04:54:00Z</b><br><br>답변감사합니다.<br>현재의 이슈에 대해 답변 주신 부분을 정리하여<br>삼성측에 아래와 같이 정리하여 보내고자 합니다.<br><br>답변과 다른 부분이 있다면 회신부탁드립니다.<br><br>--&lt;아래&gt;<br>문제발생 : PRODUCT 와 DR의 볼륨싱크시 헤더 정보가 다르게 들어가 oracleasm에 볼륨이 정상적으로 보이지 않는 현상<br>대상서버 : RPODUCT(SLPIDADL01, SLPIDADL02, SLPIDADL03 ) DR(SLDIDADL01, SLDIDADL02, SLDIDADL03)<br>문제볼륨 : DG_BACKUP4, DG_BACKUP5, DG_BACKUP6<br>작업내용 : <br>1. 최초 1호기에서 oracleasm작업시 fdisk 로 파티셔닝 후 partprobe를 진행(1호기,2호기,3호기)<br>2. 동일한 볼륨을 fdisk  -cu를 이용하여 재파티션 요청으로  재파티셔닝후 2호기 3호기에서 partprobe를 실행하지 않음<br>3. oracleasm 을 구성<br><br>원        인 :<br>위와 같은 1호기에서만 파티션후 2호기 3호기에서 정상적으로 인식을 시켜주지 않아서 <br>커널에서 인식한 파티션 정보가 각 장비마다 다르게 되어서 서비스가 정상적으로 진행이 되지 않은 것으로 판단됩니다.<br>만약 동일한 작업을 진행하게 된다면 파티션닝 후에 각각의 노드에 파티션을 rescan을 하시기 바랍니다.<br>만약 멀티패스 환경이라면 글로벌 partprobe를 실행하는 것을 권장하지 않고<br>kpartx -a /dev/mapper/&lt;mpathdevice&gt; 권장드립니다.<br><br><publishedDate>2017-03-15T04:54:00Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J32fXIAR"><br>======================<br><b>생성계정 : Zheng, Meiyan</b><br><b>생성날짜 : 2017-03-14T07:13:06Z</b><br><b>마지막 답변자 : Zheng, Meiyan</b><br><b>마지막 수정 일자 : 2017-03-14T07:13:05Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용 해 주셔서 감사합니다. <br><br><br>문종영 부장님 자리를 비운 관계로 대신 코멘트 남겨드립니다. <br><br>현재 장애가 발생 한 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6볼륨은 <br>product, DR 서버에서 모두 제거 하신 관계로 더이상 원인 분석을 해 <br>드리기에 어려움이 있어 보입니다. <br><br>이번 이슈는 1호기에서만 파티션 생성 작업을 하여 다른 서버에서 인식 <br>하게 못한 상황에서 서비스를 올려 문제가 생긴 것으로 보입니다. <br><br>만약 이런 이슈가 재현이 되면 파티션 정보가 동기화 되지 못한 서버에서 <br>kpartx 또한 multipath 리로딩을 하시고 만약 계속 파티션 정보 동기화 <br>못한 경우 앞 코멘트에서 안내 해 드렸듯 모든 서버에서 dd 정보를 수집 <br>하여 보내 주실 경우 원인 분석에 큰 도움이 될수 있을 것 같습니다. <br><br>감사합니다. <br><br>Red Hat GSS<br>정미연 드림<br><br><publishedDate>2017-03-14T07:13:05Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J2wDtIAJ"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-14T02:52:02Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-14T02:52:02Z</b><br><br>답변감사합니다.<br><br>현재 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6볼륨은 product, DR 서버에서 모두 제거 하였습니다.<br>제거 후에 서비스는 모두 정상 처리 되었습니다.<br>======================<br>1. DG_BACKUP4, 5, 6에 대한 DD결과는 보내드릴수가 없습니다.<br><br>2. 정확한 히스토리를 다시 말씀드릴 필요가 있습니다.<br><br>3개의 노드에서 최초 asm을 구성시<br>DG_BACKUP4,5,6에서 파티션 fdisk 로 한 후 <br>partprobe를 치고<br>오라클에서 fdisk -cu 옵션을 이용해서 파티션을 해달라는 요청이 있어서<br>기존 파티션 정보를 삭제후<br>fdisk -cu 로 파티션을 한  후에 <br>다시 partprobe를 진행하고 <br>oracleasm볼륨에 추가 작업을 하였습니다.<br><br>위의 모든 작업은 1호기에서만 진행을 하였습니다.<br>2호기 3호기에서는 작업을 진행하지 않았습니다.<br><br>위와 같은작업을 진행후에<br>DR 볼륨은 멀티패스 작업후 파티션을 하지 않은 상태에서<br>스토리지 to 스토리지 카피를 진행하였는데<br><br>DG_BACKUP4, 5, 6에 대한 정보가 product와 DR 헤더정보가 서로 달라서 DR서비스가 정상으로 start되지 않았습니다.<br>======================<br>멀티패스 파티션은 <br>각각의 블락디바이스들을 파티션하지 않았고 멀티패싱된 디바이스를 파티션하였습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-14T02:52:02Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J2haUIAR"><br>======================<br><b>생성계정 : Huang, Ying</b><br><b>생성날짜 : 2017-03-13T06:24:30Z</b><br><b>마지막 답변자 : Huang, Ying</b><br><b>마지막 수정 일자 : 2017-03-13T06:24:30Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>문부장이 자리를 비운 관계로 대신 업데이트 드립니다.<br>백업 엔지니어의 답변에 의하면 SLPIDADL01와 SLD* 시스템의 dd 차이점을 확인해보는것이 원인 분석에 도움이 됩니다.<br>따라서 아래와 같은 명령어로 정보 수집후 수집된 데이터를 케이스에 업로드 부탁 드립니다.<br><br>SLPIDADL01:<br>Cmd:  dd if=/dev/mapper/DG_BACKUP4 of=/tmp/SLPIDADL01_DG_BACKUP4.out<br><br>SLDIDADL01:<br>Cmd:  dd if=/dev/mapper/DG_BACKUP4 of=/tmp/SLDIDADL01_DG_BACKUP4.out<br><br>Then provide SLPIDADL01_DG_BACKUP4.out and SLDIDADL01_DG_BACKUP4.out<br><br>* I believe we already have a dd from SLDIDADL01, but I included it anyways to make sure our data is fresh.<br><br>추가적으로 글로벌 partprobe 실행하시는것을 권장하지 않으며 멀티패스를 사용하는 환경에서는 kpartx 사용하시는것을 권장 드립니다.<br><br>또한 멀티패스에 파티션을 생성시 어느 디바이스에 생성하셨는지요(sd* 아니면 멀티패스 디바이스)? 혹시 최근에 sd* 디바이스 파티션 정보를 생성하는 작업이 있었는지 확인 부탁 드립니다.<br><br><br>감사합니다.<br><br><publishedDate>2017-03-13T06:24:30Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J2LQvIAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-10T01:21:49Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-10T01:21:49Z</b><br><br>답변감사합니다.<br>base device 에 대한  헤더값이 같아야 한다고 하셨는데.<br>base device 에 대한 헤더값도 마찬가지로 product 과 dr의 정보가 다릅니다.<br><br>복제 후  kpartx -a /dev/mapper/&lt;mpathdevice&gt;' 또는 이후에<br>'partx -a /dev/&lt;sd_device&gt;' 로 확인은 진행하지 않고 <br>partprobe를 진행하였지만<br><br>동일한 증상이 발생하였습니다.<br><br>다른 관점에서 분석가능한 부분이 있는지<br>필요한 정보가 있으면 요청해주시기 부탁드립니다.<br><br><publishedDate>2017-03-10T01:21:49Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J24x0IAB"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-09T01:40:23Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-09T01:40:22Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>추가로 문의주신 내용에 대해서 저희 전문그룹에서 온 답변을 아래와 같이 안내드립니다.<br><br>- 아  래<br><br>Q1) 현재 해당 볼륨을 살리는 것이 아닌 제거를 하기로 하였기 때문에 추가 케이스 진행은 하지 않을 것입니다만<br>    아래와 같이 헤더의 정보가 다르게 들어가는 원인에 대해서는 파악을 진행해야 할것으로 보입니다.<br><br>A1) 파티션상에서 hexdump tool 을 실행중이서 파티션이 변화한것은 가능한 일입니다. partition 에 대해서 hexdump 를<br>    사용하는것보다 base device 에 대해서 이를 실행해야 할것입니다.<br>   <br>    이 방법은 lable 의 offset 상에서 더 낳은 정보를 줄것입니다.<br><br>    sosreport 에서 dm 디바이스를 위한 partition table 은 같습니다만, sd 디바이스는 그렇지 않습니다. 이들은 두개의<br>    다른 디바이스 ( 다른 wwid ) 이고, 공유되지 않습니다.<br><br>DG_BACKUP4 (360060e8006d5e6000000d5e60000600f)<br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f)<br><br>SLPIDADL01:<br>===========<br><br>DG_BACKUP4 (360060e8006d5e6000000d5e60000600f) dm-32 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdy  65:128 active ready running<br>  `- 15:0:0:11 sdab 65:176 active ready running<br><br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-32: 107GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End    Size   Type     File system  Flags<br> 1      1049kB  107GB  107GB  primary<br><br>[john@dhcp145-120 SLPIDADL01-2017030517361488702999]$ egrep 'sdy|sdab' proc/partitions <br>  65      128  104857920 sdy<br>  65      176  104857920 sdab<br><br>[john@dhcp145-120 SLPIDADL01-2017030517361488702999]$ cat sos_commands/filesys/parted_-s_.dev.sdy_print <br>Error: /dev/sdy: unrecognised disk label<br>[john@dhcp145-120 SLPIDADL01-2017030517361488702999]$ cat sos_commands/filesys/parted_-s_.dev.sdab_print <br>Error: /dev/sdab: unrecognised disk label<br><br>SLDIDADL01:<br>===========<br><br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-21 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdn  8:208  active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br><br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-21: 107GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End    Size   Type     File system  Flags<br> 1      1049kB  107GB  107GB  primary<br><br>[john@dhcp145-120 SLDIDADL01-2017030215291488436146]$ cat sos_commands/filesys/parted_-s_.dev.sdn_print <br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdn: 107GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End    Size   Type     File system  Flags<br> 1      1049kB  107GB  107GB  primary<br><br>[john@dhcp145-120 SLDIDADL01-2017030215291488436146]$ cat sos_commands/filesys/parted_-s_.dev.sdab_print <br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdab: 107GB<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br><br>Number  Start   End    Size   Type     File system  Flags<br> 1      1049kB  107GB  107GB  primary<br><br><br>위에서 보실 수 있듯이, 이들 시스템의 커널중 하나는 관련있는 디바이스상에서 파티션 변화를 인지하고, 다른 하나는 그러하지 않았습니다.<br>이는 리부팅하면 다시 보일 수 있습니다. 다른점은 파티션에 명령을 실행할때 다른 위치로부터 읽을 수 있는 tool 이 원인입니다.<br>이것이 파티션들이 어디에 있어야 하는지 정확히 아는지 중요한 이유입니다. 그래서, 이를 좀 더 자세히 이해할 수 있도록 오라클로 문의를<br>장드린것입니다. 한 파티션 테이블이 빠졌거나, 소멸되었고, 파티션 변화들이 남겨지지 않으므로서 저희는 어떻게 이것을 변경해야할지 알수<br>가 없습니다. <br><br>만약, LUN 들이 파티션 테이블을 포함한 DR 로 복제가 되었다면, 사용하신 'kpartx -a /dev/mapper/&lt;mpathdevice&gt;' 또는 이후에<br>'partx -a /dev/&lt;sd_device&gt;' 로 확인하는것은 가치가 있을것입니다. 이는 커널로 알맞게 파티션들을 더할것입니다.<br>======================<br>Q2) 해당 볼륨에 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6에 대해서 재구성이 아닌 시스템 상에서 제거를 하는 것으로 결과가 나왔습니다.<br>    DR은현재 서비스 중에 아니기 때문에 관계가 없을듯 합니다만, 운용계에서 운용중으로 작업을 진행하고자합니다 <br><br>    아래의 절차로 진행하게되면 서비스에 영향이 없는지 확인 부탁드립니다.<br><br>1. oracleasm deletedisks 로 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 제거<br>2. multipath.conf에서 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 alias 주석처리및 삭제<br>3. multipath -r<br>4. echo 1 &gt; /sys/block/DG_BACKUP4&amp;DG_BACKUP5&amp;DG_BACKUP6/device/delete &amp; echo 1 &gt; /sys/block/sdn/device/delete &amp; echo 1 &gt; /sys/block/sdab/device/delete<br>5. 스토리지 상에서 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 LUN제거<br><br><br>A2) 저는 위의 단계중 하나를 변경했습니다. production 시스템상에서 &quot;multipath -r&quot; 을 실행하는것은 저희가 이슈의 원인으로<br>    본것으로서 좋은 아이디어가 아닙니다. 대신, 'multipath -f' 로 개별 multipath 매핑을 단지 제거해야만 할것입니다.<br><br>3. multipath -r<br><br>위의 3번 단계는 아래로 변경되어야 합니다.<br><br>$ multipath -f DG_BACKUP4 <br>$ multipath -f DG_BACKUP5 <br>$ multipath -f DG_BACKUP6<br><br><br>감사합니다.<br><br><publishedDate>2017-03-09T01:40:22Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1q96IAB"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-08T06:11:05Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-08T06:11:05Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>자세한 설명과 함께 피드백 주셔서 감사합니다. 요청해주신 내용은 저희 스토리지 전문그룹의 엔지니어에게 전달하여 가능한 빠른시간내에<br>답변을 드릴수 있도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-08T06:11:05Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1prvIAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-08T05:41:11Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-08T05:41:10Z</b><br><br>안녕하세요.<br><br>해당 볼륨에 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6에 대해서<br>재구성이 아닌 시스템 상에서 제거를 하는 것으로 결과가 나왔습니다.<br><br>DR은현재 서비스 중에 아니기 때문에 관계가 없을듯 합니다만,<br><br>운용계에서 운용중으로 작업을 진행하고자합니다 <br><br>아래의 절차로 진행하게되면 서비스에 영향이 없는지 확인 부탁드립니다.<br><br><br>1.  oracleasm deletedisks 로 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 제거<br>2. multipath.conf에서 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 alias 주석처리및 삭제<br>3. multipath -r<br>4. echo 1 &gt; /sys/block/DG_BACKUP4&amp;DG_BACKUP5&amp;DG_BACKUP6/device/delete &amp; echo 1 &gt; /sys/block/sdn/device/delete &amp; echo 1 &gt; /sys/block/sdab/device/delete<br>5. 스토리지 상에서 DG_BACKUP4, DG_BACKUP5, DG_BACKUP6 LUN제거<br><br>현재 해당 볼륨을 살리는 것이 아닌 제거를 하기로 하였기 때문에 <br>추가 케이스 진행은 하지 않을 것입니다만<br><br>아래와 같이 헤더의 정보가 다르게 들어가는 원인에 대해서는 <br>파악을 진행해야 할것으로 보입니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-08T05:41:10Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1or4IAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-08T02:42:04Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-08T02:42:04Z</b><br><br>안녕하세요.<br>답변감사합니다.<br><br>DG_BACKUP4 볼륨의 라벨의 위치가 다르게 되어있어서 <br>정확한 파티셔닝이 되어 있어야 한다고 하셨는데.<br><br>DR쪽의 경우는 파티션을 하지 않습니다.<br><br>DR의 볼륨의 경우<br><br>스토리지 할당 &gt; 멀티패싱 &gt; 운용계볼륨 과 DR볼륨 스토리지 대 스토리지 싱크 하는 방식입니다.<br><br>이렇게 진행을 하게되면<br>DR볼륨은 자동으로 파티션이 생성됩니다.<br><br>일단 운용과 DR 볼륨 싱크 후의<br>헤더 정보를 첨부합니다.<br><br>--------&lt;운용 / DG_BACKUP4&gt;<br><br>root@SLPIDADL01 /root # dd if=/dev/mapper/DG_BACKUP4p1 bs=4096 count=1 | hexdump -C | head -1000<br>1+0 records in<br>1+0 records out<br>4096 bytes (4.1 kB) copied, 4.566e-05 s, 89.7 MB/s<br>00000000  01 82 01 01 00 00 00 00  03 00 00 80 f8 f2 3e 5a  |..............&gt;Z|<br>00000010  a4 f8 bf 02 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000020  4f 52 43 4c 44 49 53 4b  42 41 43 4b 55 50 34 00  |ORCLDISKBACKUP4.|<br>00000030  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000040  00 00 10 0c 03 00 01 03  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000060  00 00 00 00 00 00 00 00  44 47 5f 42 41 43 4b 55  |........DG_BACKU|<br>00000070  50 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |P...............|<br>00000080  00 00 00 00 00 00 00 00  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00000090  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>000000c0  00 00 00 00 00 00 00 00  b3 33 f8 01 00 50 05 77  |.........3...P.w|<br>000000d0  b3 33 f8 01 00 64 05 77  00 02 00 10 00 00 40 00  |.3...d.w......@.|<br>000000e0  80 ee 06 00 ff 63 00 00  03 00 00 00 01 00 00 00  |.....c..........|<br>000000f0  02 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000100  00 00 10 0c 6a 11 f8 01  00 2c 4c 90 00 00 00 00  |....j....,L.....|<br>00000110  00 00 00 00 00 00 00 00  00 00 00 00 01 00 00 00  |................|<br>00000120  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>00001000<br><br><br>--------&lt;DR / DG_BACKUP4&gt;<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # dd if=/dev/mapper/DG_BACKUP4p1 bs=4096 count=1 | hexdump -C | head -1000<br>1+0 records in<br>1+0 records out<br>4096 bytes (4.1 kB) copied, 0.00895231 s, 458 kB/s<br>00000000  00 00 00 00 ff ff 8f 00  00 00 00 00 ff ff 8f 00  |................|<br>*<br>00000c40  00 00 00 00 ff ff 8f 00  00 00 00 00 00 00 00 00  |................|<br>00000c50  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>00000e00  01 82 03 02 f9 00 00 00  03 00 00 80 3b ca 99 80  |............;...|<br>00000e10  a4 f8 bf 02 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00000e20  40 b0 01 00 00 00 00 00  08 00 08 00 0c 00 0c 00  |@...............|<br>00000e30  10 00 10 00 14 00 14 00  18 00 18 00 1c 00 1c 00  |................|<br>00000e40  20 00 20 00 00 00 00 00  00 00 00 00 ff ff 8f 00  | . .............|<br>00000e50  00 00 00 00 ff ff 8f 00  00 00 00 00 ff ff 8f 00  |................|<br>*<br>00001000<br><br><br>보시면 아시겠지만<br>현재 정상적으로 구동중이 운용의 볼륨의 헤더는 <br>00000000 번지부터 시작하는데 <br><br>싱크후<br>DR의 경우는<br>00000000 아무것도 존재하지 않고<br>해당 라벨의 위치가 DR에서는 <br>/dev/mapper/DG_BACKUP4p1 아닌 /dev/mapper/DG_BACKUP4 부터 라벨이 시작이 됩니다.<br><br>어떠한 원인으로 이러한 문제가 발생하는지 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-08T02:42:04Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1oMiIAJ"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-08T01:39:14Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-08T01:39:14Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>저희 전문그룹으로부터 온 답변을 아래와 같이 안내드립니다.<br><br>- 아  래<br><br>올려주신 내용은 파티션을 제거하기 이전의 버전인것으로 보입니다. 그래서, 저희는 파티션을 제거한 multipath 에 대해<br>서 걱정할 필요가 없습니다. 저는 data dump 를 보았는데, 아래와 같이 두개의 다른 메타데이터 영역이 있었습니다.<br><br>00000000  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>000001b0  00 00 00 00 00 00 00 00  5e 81 d8 e1 00 00 00 20  |........^...... |<br>000001c0  21 00 83 34 b6 fe 00 08  00 00 80 fa 7f 0c 00 00  |!..4............|<br>000001d0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>000001f0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 55 aa  |..............U.|<br>00000200  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>*<br>00007e00  01 82 01 01 00 00 00 00  03 00 00 80 f8 f2 3e 5a  |..............&gt;Z|<br>00007e10  a4 f8 bf 02 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00007e20  4f 52 43 4c 44 49 53 4b  42 41 43 4b 55 50 34 00  |ORCLDISKBACKUP4.|   &lt;========<br>00007e30  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00007e40  00 00 10 0c 03 00 01 03  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00007e50  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00007e60  00 00 00 00 00 00 00 00  44 47 5f 42 41 43 4b 55  |........DG_BACKU|<br>00007e70  50 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |P...............|<br>00007e80  00 00 00 00 00 00 00 00  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00007e90  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>&lt;skip...&gt;<br>00805e00  01 82 01 01 fe 03 00 00  03 00 00 80 06 f1 3e 5a  |..............&gt;Z|<br>00805e10  a4 f8 bf 02 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00805e20  4f 52 43 4c 44 49 53 4b  42 41 43 4b 55 50 34 00  |ORCLDISKBACKUP4.|   &lt;========<br>00805e30  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00805e40  00 00 10 0c 03 00 01 03  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00805e50  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br>00805e60  00 00 00 00 00 00 00 00  44 47 5f 42 41 43 4b 55  |........DG_BACKU|<br>00805e70  50 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |P...............|<br>00805e80  00 00 00 00 00 00 00 00  42 41 43 4b 55 50 34 00  |........BACKUP4.|<br>00805e90  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|<br><br>offset 00007e20 와 00805e20 에서 ORCLDISK label 이 있는데, 이로부터 저희는 예상된 파티션<br>지역을 계산할 수 있습니다.<br><br>0x00007e20 hex = 32288 decimal / 512 byte sector size = sector 63 <br>0x00805e20 hex = 8412704 decimal / 512 byte sector size = sector 16431<br><br>그래서, 디스크 상에서 oracle label 에 따라 sector 63 또는 16431 중에 하나로 시작하는 파티션<br>이 있어야 해서, 오라클과 함께 확인할 필요가 있습니다.<br><br>지금은 dm 디바이스를 위해서 커널내에 매핑된 파티션이 되어 있는거 같습니다.<br><br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # parted /dev/dm-15 u s p<br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-15: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br><br><br>하지만, 이는 2048s 에 있는데, 잘못된 위치입니다.<br><br>oracle 디스크를 복구하기 위한 파티션이 정확히 어디인지 찾고, DG_BACKUP4 상에서 나타난 어떤 파티션<br>을 제거하고, oracle 당 올바른 섹터에 파티션을 재생성하기 위해서 오라클로 연락하실 필요가 있습니다.<br>( 저는 오직 가능한 위치만 언급할 수 있고, 최종적인 판단을 할 수 없습니다. )<br><br>그래서, 노드들을 리부팅 하거나 변화들을 적용하기 위해서 &quot;partx -a /dev/&lt;devices&gt;&quot; 로 파티션 테<br>이블들을 rescan 하십시오.<br><br>이슈는 올바르지 않는 파티셔닝인것으로 보이며, 올바른 파티션이 되어야 하기 위해서 오라클로 요청하실 필요가<br>있습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-08T01:39:14Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1o7SIAR"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-08T01:06:55Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-08T01:06:55Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>현재 저희 스토리지 전문그룹으로부터 온 답변을 정리중에 있으니 곧 업데이트를 남기도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-08T01:06:55Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1U8XIAV"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-07T00:47:41Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-07T00:47:40Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>현재 본 케이스를 벡엔드에서 지원을 해주고 있는 전문그룹의 엔지니어가 미국시간대에 근무하여서 금일내에 답변이 나가기는<br>어려울듯 합니다. 분석 도움을 요청해서 가능한 빠른시일내에 관련된 내용을 안내드릴테니 조금만 더 기다려주시길 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-07T00:47:40Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1EZGIA3"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-06T07:29:24Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-06T07:29:24Z</b><br><br>답변감사합니다. 내용 정정해드립니다. <br>5. 케이스를 오픈하기 위해서 운용계에서  sosreport를 실행하였는데 실행 후 운용계의 DG_BACKUP4에서 다르게 보인던 헤더 섹터 실린더가 갑자기 동일하게 맞춰지면서 운용계의 DG_BACKUP4의 볼륨이 보이지 않음<br><br>에서 DG_BACKUP4의 볼륨이 안보이는 것이 아니라 파티션이였던 정보가 보이지 않는 상황입니다.<br><br>아래 명령어중<br><br># grep -B1 'partx -d' /lib/udev/rules.d/40-multipath.rules <br>명령어만 치면 되는건지요???<br><br>아래 정보중 <br>/lib/udev/rules.d/40-multipath.rules  의 모든 내용을 가져왔는데<br>해당정보와 다른건가요??<br><br><publishedDate>2017-03-06T07:29:24Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1ETmIAN"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-06T07:18:23Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-06T07:18:23Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>1) 고객님의 시스템상에서 partx -d 을 실행하면 어떤 결과가 나타나나요?<br><br>위의 문장에 잘못 오역이 된거 같아서 다시 안내드립니다.<br><br>이 질문의 의도는 현재 고객님의 시스템내에서 아래의 규칙과 같이 partx -d 명령이 아래의<br>udev 규칙내에 존재하는지 여쭈어보는것이었습니다. 그래서 이에 대한 답변을 주시면 될거 같습니다.<br><br>[root@dell-per510-2 ~]# grep -B1 'partx -d' /lib/udev/rules.d/40-multipath.rules <br>ENV{DM_MULTIPATH_DEVICE_PATH}==&quot;1&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br><br>RUN+=&quot;/sbin/partx -d --nr 1-1024 $env{DEVNAME}&quot;<br><br>그래서, partx -d 의 옵션에 대한 문의는 위의 규칙과 같이 디바이스 명이 들어가야 된다는 답변으로 대신하겠습니다.<br>그리고, 본 케이스는 말씀하신것과 같이 새로운 sosreport 와 분석 데이터를 올려주시는대로 계속해서 지원하도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-06T07:18:23Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000J1EOXIA3"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-06T07:07:37Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-06T07:07:37Z</b><br><br>안녕하세요.<br>요청하신 자료와 운용계와 DR sosreport 보내드립니다.<br><br>아래를 참고 부탁드립니다.<br><br>운용계 : SLPIDADL01, SLPIDADL02, SLPIDADL01<br>DR : SLDIDADL01, SLDIDADL02,   SLDIDADL01(sosreport가 되지않아서 실행안함)<br><br>주말을 포함 현재 까지 진행상황입니다.<br><br>1. DR운용계 리부팅 진행 -&gt;진행후 서로 틀렸던 섹터 헤더 실린더가 동일하게 맞춰졌으나 갑가지 oracleasm listdisks를 하니 DG_BACKUP4, DB_BACKUP5, DG_BACKUP6 이 안보이고 blkid를 하였을때 각각의 볼륨에 할당되었던 LABEL이 사라지고 TYPE=&quot;oracleasm&quot; 도 사라짐, oracleasm정상 가동 불가한 상황 발생<br><br>2. 그리고 운용계 공유볼륨과 DR  공유볼륨을 스토리지대스토리지로 싱크를 진행-&gt;스토리지에서는 정상싱크로 확인되었으나 여전히 oracleasm listdisks상에서는 DB_BACKUP4, DB_BACKUP5, DG_BACKUP6이 보이지않고 할당된 LABEL, TYPE도 보이지 않음<br>리눅스 상에서 헤더정보를 dd옵션을을 이용해서 확인해보니  운용계의 DG_BACKUP4의 헤더값과 DR DG_BACKUP4의 헤더의 위치가 서로 다르게 보임<br><br>3. dd if=/dev/zero ... 를이용하여 DR의 DG_BACKUP4의 헤더를 모두 삭제후 다시 운용계와 싱크를 하였는데 헤더가 여전히 서로 다르게 들어감<br><br>4. DR DB_BACKUP4 를 fdisk를 이용하여 파티션을 날린후 fdisk -cu 를 이용하여 파티셔닝 후 다시 운용계와 싱크를 하였으나 헤더가 여전히 다르게 들어감<br><br>5. 케이스를 오픈하기 위해서 운용계에서  sosreport를 실행하였는데 실행 후 운용계의 DG_BACKUP4에서 다르게 보인던 헤더 섹터 실린더가 갑자기 동일하게 맞춰지면서 운용계의 DG_BACKUP4의 볼륨이 보이지 않음<br><br>이상입니다. 글로 설명해야해서 다소 부족한 면이 있습니다.<br>연락을 주시면 담당어드민 혹은 담당 엔지니어와 통화가 가능하니 참고부탁드리며,<br>요청하신 내용과 sosreport를 보내드리겠습니다.<br><br><br>-아래<br>&lt;&lt;SLDIDADL01&gt;&gt;<br><br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-21 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdn  8:208  active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/dm-21<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdn<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdab<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdn u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdn: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdab u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdab: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br>root@SLDIDADL01 &lt;ICT Suwon Center&gt;:/root # parted /dev/dm-21 u s p<br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-21: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br><br>vi 40-multipath.rules<br># multipath wants the devmaps presented as meaninglful device names<br># so name them after their devmap name<br>SUBSYSTEM!=&quot;block&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{MPATH_SBIN_PATH}=&quot;/sbin&quot;<br>TEST!=&quot;$env{MPATH_SBIN_PATH}/multipath&quot;, ENV{MPATH_SBIN_PATH}=&quot;/usr/sbin&quot;<br>ACTION==&quot;add&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}!=&quot;1&quot;, \<br>        PROGRAM==&quot;$env{MPATH_SBIN_PATH}/multipath -c $tempnode&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}=&quot;1&quot;<br>ENV{DM_MULTIPATH_DEVICE_PATH}==&quot;1&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        RUN+=&quot;/sbin/partx -d --nr 1-1024 $env{DEVNAME}&quot;<br>RUN+=&quot;socket:/org/kernel/dm/multipath_event&quot;<br>KERNEL!=&quot;dm-*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}==&quot;mpath-?*|part[0-9]*-mpath-?*&quot;, OPTIONS+=&quot;link_priority=10&quot;<br>ACTION!=&quot;change&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}!=&quot;mpath-?*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_SUSPENDED}==&quot;1&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTION}==&quot;PATH_FAILED&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTIVATION}==&quot;1&quot;, RUN+=&quot;$env{MPATH_SBIN_PATH}/kpartx -a -p p $tempnode&quot;<br>LABEL=&quot;end_mpath&quot;<br><br><br>-------------------------------------------------------------------------<br>&lt;&lt;SLDIDADL02&gt;&gt;<br><br><br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-15 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 2:0:0:11 sdn  8:208  active ready running<br>  `- 3:0:0:11 sdab 65:176 active ready running<br> <br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/dm-15<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdn<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdab<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdn u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdn: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End  Size  Type  File system  Flags<br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdab u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdab: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End  Size  Type  File system  Flags<br>root@SLDIDADL02 &lt;ICT Suwon Center&gt;:/root # parted /dev/dm-15 u s p<br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-15: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br> <br># multipath wants the devmaps presented as meaninglful device names<br># so name them after their devmap name<br>SUBSYSTEM!=&quot;block&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{MPATH_SBIN_PATH}=&quot;/sbin&quot;<br>TEST!=&quot;$env{MPATH_SBIN_PATH}/multipath&quot;, ENV{MPATH_SBIN_PATH}=&quot;/usr/sbin&quot;<br>ACTION==&quot;add&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}!=&quot;1&quot;, \<br>        PROGRAM==&quot;$env{MPATH_SBIN_PATH}/multipath -c $tempnode&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}=&quot;1&quot;<br>ENV{DM_MULTIPATH_DEVICE_PATH}==&quot;1&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        RUN+=&quot;/sbin/partx -d --nr 1-1024 $env{DEVNAME}&quot;<br>RUN+=&quot;socket:/org/kernel/dm/multipath_event&quot;<br>KERNEL!=&quot;dm-*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}==&quot;mpath-?*|part[0-9]*-mpath-?*&quot;, OPTIONS+=&quot;link_priority=10&quot;<br>ACTION!=&quot;change&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}!=&quot;mpath-?*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_SUSPENDED}==&quot;1&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTION}==&quot;PATH_FAILED&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTIVATION}==&quot;1&quot;, RUN+=&quot;$env{MPATH_SBIN_PATH}/kpartx -a -p p $tempnode&quot;<br>LABEL=&quot;end_mpath&quot;<br> <br> <br>-------------------------------------------------------------------------<br><br>&lt;&lt;SLDIDADL03&gt;&gt;<br><br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-15 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 12:0:0:11 sdn  8:208  active ready running<br>  `- 13:0:0:11 sdab 65:176 active ready running<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/dm-15<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdn<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # scsi_id --whitelisted /dev/sdab<br>360060e8007c61f000030c61f0000600f<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdn u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdn: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End  Size  Type  File system  Flags<br> <br>DG_BACKUP4 (360060e8007c61f000030c61f0000600f) dm-15 HITACHI,OPEN-V<br>size=100G features='1 queue_if_no_path' hwhandler='0' wp=rw<br>`-+- policy='round-robin 0' prio=1 status=active<br>  |- 2:0:0:11 sdn  8:208  active ready running<br>  `- 3:0:0:11 sdab 65:176 active ready running<br> <br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # parted /dev/sdab u s p<br>Model: HITACHI OPEN-V (scsi)<br>Disk /dev/sdab: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End  Size  Type  File system  Flags<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root # parted /dev/dm-15 u s p<br>Model: Linux device-mapper (multipath) (dm)<br>Disk /dev/dm-15: 209715840s<br>Sector size (logical/physical): 512B/512B<br>Partition Table: msdos<br>Number  Start  End         Size        Type     File system  Flags<br> 1      2048s  209715839s  209713792s  primary<br>root@SLDIDADL03 &lt;ICT Suwon Center&gt;:/root #<br># multipath wants the devmaps presented as meaninglful device names<br># so name them after their devmap name<br>SUBSYSTEM!=&quot;block&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{MPATH_SBIN_PATH}=&quot;/sbin&quot;<br>TEST!=&quot;$env{MPATH_SBIN_PATH}/multipath&quot;, ENV{MPATH_SBIN_PATH}=&quot;/usr/sbin&quot;<br>ACTION==&quot;add&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}!=&quot;1&quot;, \<br>        PROGRAM==&quot;$env{MPATH_SBIN_PATH}/multipath -c $tempnode&quot;, \<br>        ENV{DM_MULTIPATH_DEVICE_PATH}=&quot;1&quot;<br>ENV{DM_MULTIPATH_DEVICE_PATH}==&quot;1&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br>        RUN+=&quot;/sbin/partx -d --nr 1-1024 $env{DEVNAME}&quot;<br>RUN+=&quot;socket:/org/kernel/dm/multipath_event&quot;<br>KERNEL!=&quot;dm-*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}==&quot;mpath-?*|part[0-9]*-mpath-?*&quot;, OPTIONS+=&quot;link_priority=10&quot;<br>ACTION!=&quot;change&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_UUID}!=&quot;mpath-?*&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_SUSPENDED}==&quot;1&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTION}==&quot;PATH_FAILED&quot;, GOTO=&quot;end_mpath&quot;<br>ENV{DM_ACTIVATION}==&quot;1&quot;, RUN+=&quot;$env{MPATH_SBIN_PATH}/kpartx -a -p p $tempnode&quot;<br>LABEL=&quot;end_mpath&quot;<br>~<br><br><publishedDate>2017-03-06T07:07:37Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1CO5IAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-06T01:46:51Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-06T01:46:51Z</b><br><br>혹시 <br>partx -d 는 뒤에 추가 옵션이 없나요?<br><br><publishedDate>2017-03-06T01:46:51Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1CJyIAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-06T01:40:07Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-06T01:40:07Z</b><br><br>답변 감사합니다.<br>리부팅을 진행하지 말라고 하셨는데<br>주말에 해당 문제에 대한 진행을 하기로 일정이 되어있어서 리부팅을 진행하였습니다.<br><br>자세한 내용은 최신 sosreport를 다시 갱신해 드리겠습니다.<br>리부팅후 변동 사항에 대해서 간단히 말씀드리면<br><br>SLDIDADL01 , SLDIDADL02, SLDIDADL03 모두 동일하게 실린더와 헤더를 및 보이지 않았던 파티션도 3개의 노드가 동일하게 맞춰졌습니다.<br>문제는<br><br>운용계의 공유볼륨을 DR의 볼륨을 sync하였는데<br>운용계의 헤더와 DR의 헤더의 정보가 다르게 보입니다.<br><br>sync방식은 서버를 거치지 않고 스토리지 to 스토리지로  sync를 하는 방식입니다.<br><br>자세한 정보는  운영계와 DR의 sosreport와 dd dump한 헤더정보를 보내드리겠습니다.<br><br><publishedDate>2017-03-06T01:40:07Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000J1CD4IAN"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-06T01:19:00Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-06T01:19:00Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>올려주신 문의내용을 저희 전문그룹으로 요청해서 답변을 받아서 아래와 같이 안내드립니다.<br><br>- 아  래<br><br>하부 multipath 디바이스는 파티션들을 가지지 않아야 합니다. 예제를 보여드리자면 아래와 같습니다.<br><br>mpatha (360a98000433468506535477549413844) dm-2 NETAPP,LUN<br>size=10G features='4 queue_if_no_path pg_init_retries 50 retain_attached_hw_handle' hwhandler='0' wp=rw<br>|-+- policy='service-time 0' prio=50 status=active<br>| |- 2:0:0:0 sdb 8:16   active ready running<br>| |- 4:0:0:0 sdf 8:80   active ready running<br>| |- 3:0:1:0 sde 8:64   active ready running<br>| `- 5:0:0:0 sdh 8:112  active ready running<br>`-+- policy='service-time 0' prio=10 status=enabled<br>  |- 2:0:1:0 sdc 8:32   active ready running<br>  |- 3:0:0:0 sdd 8:48   active ready running<br>  |- 4:0:1:0 sdg 8:96   active ready running<br>  `- 5:0:1:0 sdi 8:128  active ready running<br><br>모든 파티션 매핑은 udev 에 의해서 제거가 되었고,<br><br>[root@dell-per510-2 ~]# egrep 'sdb|sdf|sde|sdh|sdc|sdd|sdg|sdi' /proc/partitions <br>   8       16   10485760 sdb<br>   8       80   10485760 sdf<br>   8       64   10485760 sde<br>   8      112   10485760 sdh<br>   8       32   10485760 sdc<br>   8       48   10485760 sdd<br>   8       96   10485760 sdg<br>   8      128   10485760 sdi<br><br><br>이 규칙들은 /lib/udev/rules.d/ 에서 찾을 수 있습니다.<br><br>[root@dell-per510-2 ~]# grep -B1 'partx -d' /lib/udev/rules.d/40-multipath.rules <br>ENV{DM_MULTIPATH_DEVICE_PATH}==&quot;1&quot;, ENV{DEVTYPE}!=&quot;partition&quot;, \<br><br>RUN+=&quot;/sbin/partx -d --nr 1-1024 $env{DEVNAME}&quot;<br><br><br>하지만, 고객님의 시스템상에서는 거의 모든 파티션들이 여기에 있습니다.<br><br>   8       48   10486080 sdd<br>   8       49   10485056 sdd1<br>   8       80   10486080 sdf<br>   8       81   10485056 sdf1<br>   8       32  188743680 sdc<br>   8       33  188742656 sdc1<br>   8       96  104857920 sdg<br>   8       97  104856896 sdg1<br>   8      128  104857920 sdi<br>   8      129  104856896 sdi1<br><br>여기서 아래의 질문을 드립니다.<br><br>1) 고객님의 시스템상에서 partx -d 을 실행하면 어떤 결과가 나타나나요?<br>2) partprobe 또는 partx 와 같은 명령을 파티션을 probe 하기 위해서 실행중인가요?<br>3) SLDIDADL01 상에서 정상적으로 oracle 이 실행중이지 않았다고 말씀을 하셨는데, 정확히 어떤 오라클의 장애이신가요?<br>4) 이 문제의 원인분석을 위해서 아래 명령어의 출력물을 케이스에 업로드 부탁드리며, 시스템들을 리부팅을 하지 않기를 요청드립니다.<br>   <br><br>SLDIDADL01 에서:<br><br>$ scsi_id --whitelisted /dev/dm-21<br>$ scsi_id --whitelisted /dev/sdn<br>$ scsi_id --whitelisted /dev/sdab<br>$ parted /dev/sdab u s p<br>$ parted /dev/dm-21 u s p<br>$ dd if=/dev/mapper/DG_BACKUP4 of=/tmp/DG_BACKUP4_SLDIDADL01.out bs=1M count=10<br><br>SLDIDADL02 에서:<br><br>$ scsi_id --whitelisted /dev/dm-32<br>$ scsi_id --whitelisted /dev/sdy<br>$ scsi_id --whitelisted /dev/sdab<br>$ parted /dev/sdab u s p<br>$ parted /dev/dm-32 u s p<br>$ dd if=/dev/mapper/DG_BACKUP4 of=/tmp/DG_BACKUP4_SLDIDADL02.out bs=1M count=10<br><br>제가 보기에는 파티션이 제거가 되었거나 덮어쓰여졌고, 모든 모드들이 아직까지 그것을 인지하지 않은거 같습니다. 그래서, 리부팅을 하지 않는것이 중요합니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-06T01:19:00Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Iwg1SIAR"><br>======================<br><b>생성계정 : Moon, Jack, Jong Young</b><br><b>생성날짜 : 2017-03-03T07:56:32Z</b><br><b>마지막 답변자 : Moon, Jack, Jong Young</b><br><b>마지막 수정 일자 : 2017-03-03T07:56:32Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>현재 올려주신 문의내용을 자세히 살펴보고 있는중이며 확인이 되는대로 업데이트를 남기도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-03-03T07:56:32Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000IwfHKIAZ"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-03-03T06:32:19Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-03-03T06:32:19Z</b><br><br>안녕하세요.<br>해당 문제에 대해서 공유 볼륨들에 대해서 각3개의 노드에서 동일하게 보이게 작업예정입니다.<br>작업전에 해당 원인에 대해서 파악이 필요한 상황이라서 어려우시겠지만 빠른 답변 부탁드립니다.<br>감사합니다.<br><br><publishedDate>2017-03-03T06:32:19Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>