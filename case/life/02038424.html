======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2018-02-20T02:28:16Z</b><br><b>마지막 답변자 : Workflow Integration</b><br><b>마지막 수정 일자 : 2018-03-14T01:16:17Z</b><br><b>id : 500A000000ZqXeWIAV</b><br>======================<br><br><b><font size=15>
제목  : [생명] 네트워크 지연에 따른 분석요청
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하십니까 타임게이트 오선우입니다<br><br>지난번 비슷한 이슈로 케이스를 오픈한적이 있었었습니다<br><br>기술 문의 02008112<br>[생명]네트워크 I/O 증가로 인한 서비스 지연<br><br>위 케이스와 관련하여 세션이 19일 17시부터 17시10분 사이에 급격하게 증가하여 <br>세션이 끈기는 증상이 발생하였습니다<br><br>해당시점에 OS상에 특이사항이 있는지 현재 커널파라미터등 문제는 없는지 확인이 필요한 상황입니다<br><br>해당 서버 2식에 대한 sosreport, sar데이타, message로그 파일 파일 첨부드립니다<br><br>지난번 해당이슈로 해당 파라미터를 변경하여도 증상이 호전되지 않았습니다 <br>net.core.netdev-max-backlog 1000 -&gt; 8192<br>net,ipv4,tcp-max-syn-backlog 2048 -&gt; 8192<br>net.core.somaxconn=128-&gt; 8192<br><br>여러 벤더사가 해당 이슈를 가지고 분석을 하고 있지만 특별한 특이점을 발견할수 없는 상황입니다<br>19일 17시부터 17시10분 사이에 특이점이 있는지 분석 부탁드리며 권고해주실 말씀있으시면 부탁드립니다<br><br>감사합니다<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.6</b><br><b>타입  : Other</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>SLPOAMAL01,SLPOAMAL02</hostname><enhancedSLA>false</enhancedSLA><contactIsPartner>false</contactIsPartner><tags><tag><name>networking</name></tag></tags><br><comment id="a0aA000000Kt5nHIAR"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-03-14T01:16:16Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-03-14T01:16:16Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>해당 케이스에 대하여 직접 close 해주셔서 감사합니다.<br><br>케이스가 처리 완료되면 고객 설문조사 메일이 발송됩니다.<br>고객님께서 남기신 의견은 보다 나은 서비스를 위해 지속적으로 반영될 것입니다.<br>향후 기술 지원 서비스의 품질 향상을 위해,<br>소중한 시간을 내어 주시면 대단히 감사드리겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-03-14T01:16:16Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Kt5cqIAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-03-14T00:52:27Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-03-14T00:52:27Z</b><br><br>답변감사합니다.<br>해당이슈에 대해서 어플리케이션단에서 진행하기로 하였습니다.<br>감사합니다.<br><br><publishedDate>2018-03-14T00:52:27Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LsXyxIAF"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-03-05T01:37:02Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-03-05T01:37:02Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>&gt; https://access.redhat.com/solutions/647953<br><br>일반적으로 해당 문서의 내용이 도움이 되실 것으로 보입니다.<br>다만, 해당 문서에도 다음과 같이 나와 있습니다.<br><br>If the application does not set its own socket buffer sizes, the system default size is used.<br><br>즉, 적용 후, 개선 여부는 사용 환경 의존적입니다. <br>또한, 구체적인 튜닝 값 제공 및 검증은 저희 기술지원 범위가 아닙니다.<br>적절한 값은 테스트를 통해 도출하실 필요가 있습니다.<br><br>감사합니다.<br><br><publishedDate>2018-03-05T01:37:02Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LsXuqIAF"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-03-05T01:29:08Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-03-05T01:29:08Z</b><br><br>답변감사합니다.<br>진행내용 알려드립니다.<br>현재 <br>1. <br>chkconifg irqbalance on<br>service irpbalance start<br><br>2. <br>sysctl.conf 에 <br>net.core.netdev_budget=1000<br><br>3. <br>/etc/security/limits.conf 와 /etc/security/limits.d/90-nproc.conf<br>중복설정변경 및 kernel.pid_max 32768 (디폴트) -&gt; 2배 정도 변경<br><br>모두 센터 내부 사정상 진행하지 않았습니다.<br>현재 <br>해당 서버에서 네트워크 모니터링 중인데<br>아래와 같은 로그가 발생중입니다.<br>root@SLPOAMAL01 /sysadmin/netstat/log # grep collapse 180228*[5,0]_netstat.lst<br>1802281640_netstat.lst:    20195967 packets collapsed in receive queue due to low socket buffer<br>1802281645_netstat.lst:    20197477 packets collapsed in receive queue due to low socket buffer<br>1802281650_netstat.lst:    20197939 packets collapsed in receive queue due to low socket buffer<br>1802281655_netstat.lst:    20199433 packets collapsed in receive queue due to low socket buffer<br>1802281700_netstat.lst:    20199433 packets collapsed in receive queue due to low socket buffer<br>1802281705_netstat.lst:    20200965 packets collapsed in receive queue due to low socket buffer<br>1802281710_netstat.lst:    20200965 packets collapsed in receive queue due to low socket buffer<br>1802281715_netstat.lst:    20202470 packets collapsed in receive queue due to low socket buffer<br>1802281720_netstat.lst:    20203153 packets collapsed in receive queue due to low socket buffer<br>1802281725_netstat.lst:    20204708 packets collapsed in receive queue due to low socket buffer<br>1802281730_netstat.lst:    20204708 packets collapsed in receive queue due to low socket buffer<br>root@SLPOAMAL01 /sysadmin/netstat/log # grep prune 180228*[5,0]_netstat.lst<br>1802281640_netstat.lst:    122610 packets pruned from receive queue because of socket buffer overrun<br>1802281645_netstat.lst:    122614 packets pruned from receive queue because of socket buffer overrun<br>1802281650_netstat.lst:    122616 packets pruned from receive queue because of socket buffer overrun<br>1802281655_netstat.lst:    122620 packets pruned from receive queue because of socket buffer overrun<br>1802281700_netstat.lst:    122620 packets pruned from receive queue because of socket buffer overrun<br>1802281705_netstat.lst:    122624 packets pruned from receive queue because of socket buffer overrun<br>1802281710_netstat.lst:    122624 packets pruned from receive queue because of socket buffer overrun<br>1802281715_netstat.lst:    122628 packets pruned from receive queue because of socket buffer overrun<br>1802281720_netstat.lst:    122635 packets pruned from receive queue because of socket buffer overrun<br>1802281725_netstat.lst:    122639 packets pruned from receive queue because of socket buffer overrun<br>1802281730_netstat.lst:    122639 packets pruned from receive queue because of socket buffer overrun <br><br>해당 내용 관련에서 <br>https://access.redhat.com/solutions/647953<br>문서를 보면 파라메터 변경을 권고하는데요.<br><br>ERP프로젝트 할때 사용한 아래값을 검토중입니다.<br><br>net.core.rmem_default = 262144<br>net.core.rmem_max = 16777216 <br>net.core.wmem_default = 262144<br>net.core.wmem_max = 16777216<br>net.ipv4.tcp_rmem = 4096 262144 16777216<br>net.ipv4.tcp_wmem = 4096 262144 16777216 <br><br>해당 파라메터가 위의 내용을 해소 할 수있는 방법일지 <br>혹시 다른 값이 있다면 권고 부탁드립니다.<br><br><publishedDate>2018-03-05T01:29:08Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LqGrKIAV"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-02-22T01:17:59Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-02-22T04:53:35Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>문의하신 결과에 대하여 안내를 드리면 다음과 같습니다.<br><br>&gt; 2월 20일자 sar 보내드립니다. 17시-17시10분 사이를 중점적으로 추가 확인 부탁드립니다 <br><br>sar 를 보면 17시-17시10분 사이에 SLPOAMAL01 에서 5:01:01 경 다수의 tcp-tw 소켓이 확인됩니다.<br>tcp-tw 소켓의 경우, 해당 서버의 어플리케이션에서 세션을 종료하기 위해 상대 노드에게 fin 시그널을 송신하여서<br>세션을 끊었을 때 발생할 수 있습니다.<br>이는 일반적으로 정상적인 tcp 세션 플로우라 문제라고 말씀드리기는 어렵습니다.<br>또한, 어플리케이션에서 어떤 필요에 의해 세션을 끊었는지는 OS 차원에서 말씀드리기도 어렵습니다.<br><br>SLPOAMAL01.sar20<br><br>12:00:01 AM    totsck    tcpsck    udpsck    rawsck   ip-frag    tcp-tw<br>...<br>05:00:01 PM      5198      4936        56         0         0       238<br>05:01:01 PM      2344      2110        45         0         0      2557<br>05:02:01 PM      2762      2528        45         0         0       710<br>05:03:01 PM      2795      2546        56         0         0       322<br>05:04:01 PM      3382      3127        55         0         0       223<br>05:05:01 PM      3378      3123        55         0         0       229<br>05:06:01 PM      4154      3898        56         0         0       236<br>05:07:01 PM      5219      4958        61         0         0       230<br>05:08:01 PM      5525      5268        57         0         0       236<br>05:09:02 PM      5456      5198        56         0         0       239<br>05:10:01 PM      5529      5269        58         0         0       241<br><br>SLPOAMAL02.sar20<br><br>05:00:01 PM      4053      3839        55         0         0       218<br>05:01:01 PM      4072      3851        55         0         0       208<br>05:02:01 PM      4064      3842        55         0         0       214<br>05:03:01 PM      4086      3863        55         0         0       215<br>05:04:01 PM      4154      3986        55         0         0       199<br>05:05:01 PM      4165      3912        55         0         0       215<br>05:06:01 PM      3900      3683        55         0         0       217<br>05:07:01 PM      4341      4126        55         0         0       214<br>05:08:01 PM      4340      4124        56         0         0       219<br>05:09:01 PM      4358      4142        56         0         0       227<br>05:10:01 PM      4327      4107        57         0         0       220<br> <br><br>&gt; 두 경우 모두 리부팅을 진행하면 될까요? 적용 후에 확인하는 방법에 대해서도 가이드 주시면 감사하겠습니다<br><br>특별히 리부팅을 요구하는 내용은 확인되지 않습니다.<br>확인은 예를 들어 다음과 같이 하시면 됩니다.<br>===<br># cat proc/interrupts  | grep eth<br> 57: 4230685204          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v0-Rx<br> 58: 4279299795          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v1-Rx<br> 59: 3857546452          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v2-Rx<br> 60: 4122853266          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v3-Rx<br> 61:  433730340          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v4-Rx<br> 62:   74188282          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v5-Rx<br> 63:  283995945          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v6-Rx<br> 64: 4096489069          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v7-Rx<br> 65:          0          0          0          0          0          0          0          0          0          0          0          0   PCI-MSI-edge      eth1:v8-event<br><br>===&gt; 현재 1번째 cpu 에 NIC 의 모든 큐의 인터럽트가 몰려 있습니다.<br>     분산시키면 각 큐별 다른 cpu 로 인터럽트가 분산될 수 있습니다.<br>===<br><br>추가적으로 확인결과 해당 시스템에서 몇가지 수정할 부분이 있어 남겨드립니다.<br><br>weblogic 유저와 관련하여 설정된 ulimit 설정에 대한 확인이 필요해보입니다.<br><br>/etc/security/limits.conf 와 /etc/security/limits.d/90-nproc.conf<br>에 weblogic 유저의 nproc 및 nofile 이 중복설정되어 있습니다.<br><br>nproc 의 경우 90-nproc.conf 에만 설정해주시고,<br>nofile 등 그밖의 값의 경우는 limits.conf 에 설정하시기 바랍니다.<br>물론 적절한 값은 weblogic 설정 팀에 확인하시기 바랍니다.<br><br>Setting nproc in /etc/security/limits.conf has no effect in Red Hat Enterprise Linux. <br>- https://access.redhat.com/solutions/146233<br><br>또한, nproc 값을 늘리신 경우, 다음 값도 사용하시는 환경 및 필요에 따라 늘리실 필요가 있을 수 있습니다.<br><br>예)<br>kernel.pid_max 32768 (디폴트) -&gt; 2배 정도<br><br>sysctl 관련 설정은 반영을 위해 재부팅을 요구하지는 않습니다.<br><br>감사합니다.<br><br><publishedDate>2018-02-22T01:17:58Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000Lq241IAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-02-21T10:16:00Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-02-21T10:16:00Z</b><br><br>답변감사합니다<br><br>고객에게 내용 전달하고 특별히 진전사항이 없어서 권고해주신 irqbalance,net.core.netdev_budget  값 적용을 추후 진행해보기로 했습니다<br><br>2월 20일자 sar 보내드립니다. 17시-17시10분 사이를 중점적으로 추가 확인 부탁드립니다 <br><br>chkconifg irqbalance on<br>service irpbalance start<br><br> sysctl.conf 에 <br>net.core.netdev_budget=1000<br>추가후에<br><br>위와 같이 적용을 예정하고 있습니다<br>두 경우 모두 리부팅을 진행하면 될까요? 적용 후에 확인하는 방법에 대해서도 가이드 주시면 감사하겠습니다<br><br>감사합니다<br><br><publishedDate>2018-02-21T10:16:00Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LpgSSIAZ"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-02-20T06:20:14Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-02-20T06:52:00Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>상황은 잘 알겠습니다.<br>현재로서는 제약된 정보만으로 안내드릴 수 밖에 없는 점 양해바랍니다.<br><br>일단 양 서버상에서 패킷 드랍 및 에러가 확인되지 않습니다.<br>즉, 패킷 자체는 정상적으로 어플리케이션 레이어에 전달된 것으로 보입니다.<br>메시지 로그는 특별한 내용이 없습니다.<br><br>===<br>## SLPOAMAL01<br><br>NETDEV<br>  Interface  RxMiBytes  RxPackets   RxErrs  RxDrop  RxFifo  RxComp  RxFrame  RxMultCast<br>  =========  =========  =========   ======  ======  ======  ======  =======  ==========<br>  eth1       17825372   42273970 k  0       0       0       0       0        9752758 (0%)<br>  - - - - - - - - - - - - - - - - -<br>  Interface  TxMiBytes  TxPackets   TxErrs  TxDrop  TxFifo  TxComp  TxColls  TxCarrier <br>  =========  =========  =========   ======  ======  ======  ======  =======  ==========<br>  eth1       25708068   42766736 k  0       0       0       0       0        0 <br><br>## SLPOAMAL02<br><br>NETDEV<br>  Interface  RxMiBytes  RxPackets   RxErrs  RxDrop  RxFifo  RxComp  RxFrame  RxMultCast<br>  =========  =========  =========   ======  ======  ======  ======  =======  ==========<br>  eth2       17370651   39712445 k  0       0       0       0       0        9748709 (0%)<br>  - - - - - - - - - - - - - - - - -<br>  Interface  TxMiBytes  TxPackets   TxErrs  TxDrop  TxFifo  TxComp  TxColls  TxCarrier <br>  =========  =========  =========   ======  ======  ======  ======  =======  ==========<br>  eth2       23371126   41073630 k  0       0       0       0       0        0 <br>===<br><br>인터럽트 분산 측면을 보면,<br>첫번째 cpu 만 사용되고 있습니다.<br>===<br>## SLPOAMAL01<br><br>   57: ▊........... PCI-MSI-edge eth1:v0-Rx<br>   58: ▊........... PCI-MSI-edge eth1:v1-Rx<br>   59: ▊........... PCI-MSI-edge eth1:v2-Rx<br>   60: ▊........... PCI-MSI-edge eth1:v3-Rx<br>   61: ▊........... PCI-MSI-edge eth1:v4-Rx<br>   62: ▊........... PCI-MSI-edge eth1:v5-Rx<br>   63: ▊........... PCI-MSI-edge eth1:v6-Rx<br>   64: ▊........... PCI-MSI-edge eth1:v7-Rx<br>   65: ............ PCI-MSI-edge eth1:v8-event<br><br>## SLPOAMAL02<br><br>   57: ▊........... PCI-MSI-edge eth2:v0-Rx<br>   58: ▊........... PCI-MSI-edge eth2:v1-Rx<br>   59: ▊........... PCI-MSI-edge eth2:v2-Rx<br>   60: ▊........... PCI-MSI-edge eth2:v3-Rx<br>   61: ▊........... PCI-MSI-edge eth2:v4-Rx<br>   62: ▊........... PCI-MSI-edge eth2:v5-Rx<br>   63: ▊........... PCI-MSI-edge eth2:v6-Rx<br>   64: ▊........... PCI-MSI-edge eth2:v7-Rx<br>   65: ............ PCI-MSI-edge eth2:v8-event<br>===<br><br>일반적으로 효율적인 인터럽트 분산을 위해 irqbalance 활성화를 권장드리고 있습니다.<br>정지시킨 특별한 이유가 있으신가요? 확인 바랍니다.<br><br>irqbalance     <br>0:off <br>1:off <br>2:off <br>3:off <br>4:off <br>5:off <br>6:off<br><br>네트워크 파라메터 측면에서 보면 일부 값을 늘리신 것으로 보이는데요,<br>현재 10G NIC 를 쓰시는 환경이므로 다음 값의 경우 1000 으로 올리셔도 무방해 보입니다.<br><br>net.core.netdev_budget = 300 -&gt; 1000<br><br>관련 KCS:<br>What are the recommendations for 10 Gigabit network adapter tuning parameters? <br>- https://access.redhat.com/solutions/127143<br><br>추가로 질문입니다만,<br><br>그런데 두 서버(SLPOAMAL01 , SLPOAMAL02)의 관계는 뭔가요?<br>두 서버다 동일한 이슈가 발생했나요?<br><br>그리고 다음 파라메터 들을 늘리셨는데, somaxconn 의 경우 어플리케이션 측면에서 제한이 있으면<br>어플리케이션 측(예, apache, WAS 등) 설정이 우선이 됩니다.<br>어플리케이션 설정 부분을 확인해보셨나요?<br><br>&gt; 지난번 해당이슈로 해당 파라미터를 변경하여도 증상이 호전되지 않았습니다 <br>&gt; net.core.netdev_max_backlog = 1000 -&gt; 8192<br>&gt; net.ipv4.tcp_max_syn_backlog = 2048 -&gt; 8192<br>&gt; net.core.somaxconn = 128-&gt; 8192<br><br>감사합니다.<br><br><publishedDate>2018-02-20T06:20:14Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LpftcIAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-02-20T05:15:39Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-02-20T05:15:39Z</b><br><br>차장님 빠른 답변 감사합니다<br><br>우선 현재 고객측은 시스템운영쪽에 있고 해당 내용은 어플리케이션쪽에서 세션이 끊기는 증상이 있습니다<br><br>고객에 팀이 나뉘어져 있고 파트가 나뉘어져 있다보니 자세한 네트워크 토플로지 제공이 어렵거나 시간이 많이 소모되는 상황인것 같습니다<br><br>현재 해당 OS vmware 위에 게스트OS로 운영중이며 해당 시점에 OS기준에서 sosreport,sar,message log를 가지고 특이점이 있는지 확인 부탁드립니다<br><br>감사합니다<br><br><publishedDate>2018-02-20T05:15:39Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000LpfZcIAJ"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-02-20T04:34:25Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-02-20T04:34:24Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>정확한 상황 파악을 위해 어떤 세션이 끊겼는지 확인하신 방법 등 보다 구체적인 예로 관련정보를 연계해주시기 바랍니다.<br>또한, 네트워크 토폴로지 등 사용하시는 네트워크 구성 및 흐름에 대한 참고할 수 있는 자료가 있으면<br>올려주시기 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2018-02-20T04:34:24Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LpfYeIAJ"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-02-20T04:30:14Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-02-20T04:30:13Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>저는 신재욱 차장이라고 하며 앞으로 해당 케이스를 담당하게 되었습니다.<br><br>현재 케이스 내용을 살펴보는 중이며, 관련하여 업데이트 드리도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-02-20T04:30:13Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>