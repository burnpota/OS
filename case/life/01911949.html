======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2017-08-16T01:21:55Z</b><br><b>마지막 답변자 : HyunYick Park</b><br><b>마지막 수정 일자 : 2017-09-15T10:18:33Z</b><br><b>id : 500A000000Y6aoGIAR</b><br>======================<br><br><b><font size=15>
제목  : pacemaker 환경에서 리소스 관리에 대한 문의
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하십니까 타임게이트 오선우입니다<br>2017년 8월 14일 오전 10경부터 해당 시스템에대한 가용성 테스트를 진행하였습니다<br>해당 과정을 진행하면서  발생한 이슈들에 대해서 몇가지 문의사항이 있어 케이스 오픈합니다<br>아래 리소스들 중에서 중간에 LV리소스에 대해서 umount을 진행하였을경우 <br>해당 리소스 이후에 리소스들이 모두  재시작 되는것을 확인하였습니다<br>정상적인 경우라고 알고 있는데요<br><br>1. 이 처럼 해당 리소스 이후에 리소들이 재시작되는것이 정상적인 경우인가요?<br><br>2. 만약 그렇다면 해당 리소스만을 umount를 실행하여도 해당 기능이 동작하지 않도록 구성하는 방법이 무엇이 있을지 확인 부탁드립니다<br>참고적으로 현제는 on-fail=ignore로 바꿔놓은 상태입니다<br><br>3. 해당 서버는 DB서버로 디스크에 대한 부분은 pacemaker에 등록되어 실행시 디스크 환경이 만들어지면 수동으로 서비스(DB 등)올려주는 과정을 실행하고 있습니다 <br>데이터의 안정성등을 고려하여 최대한 클러스터 종료시 안정적으로 서비스 내리는 과정을 추가하고 싶은데요  최대한 정상적이고 안정적으로 내릴수 있는 방법이 있는지 검토 부탁드립니다<br><br>4. 관련 파일들 업로드 합니다 참고 부탁드립니다<br><br>감사합니다<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>타입  : Other</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>SLPIBADL01,SLPIBADL02</hostname><br><br><br><comment id="a0aA000000KH3ZmIAL"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-30T01:24:43Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-30T01:24:43Z</b><br><br>답변 감사합니다.<br>확인 후 말씀드리겠습니다.<br><br><publishedDate>2017-08-30T01:24:43Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000FxOyzIAF"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-29T05:19:23Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-29T05:45:05Z</b><br><br>안녕하세요,<br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>먼저 pacemaker에서 resource group의 컨셉을 말씀드리겠습니다.<br>resource group에 속해있는 resource 들은 동시간에 한번 순서대로 시작되어야 하고 역순으로 정지되어야 합니다.<br>resource group은 resource 개별의 동작이 아닌 group으로 인한 하나의 동작을 위해 만들어 졌습니다.<br>다른 resouce와 독립적으로 운영하고 싶으시다면 resource를 resource group에서 제거하시는게 가장 명확한 방법입니다.<br> <br>위의 내용에 기반하여 resource cleanup 명령어 입력시에 특정한 하나의 resource만 명시하였더라도<br>하나의 resource에만 영향을 주는것이 아닌 resource group에 속한 모든 resource에 영향을 주게 됩니다.<br>이는 cleanup 명령어 실행시 group에 속한 모든 resource의 상태를 확인하고, 정상적인 상태가 아닌 경우,<br>재시작 / 정지 / 시작 작업이 발생할 수 있습니다.<br>또한 의존성이 있는 resource가 정상적이지 않은 경우에는 manage 여부와도 관계 없이 하위 resource는 영향을 받게 됩니다.<br> <br>결과적으로  resource group에 존재하고, 상대적으로 영향도가 적은 resource의 failed action을 제거하시려면<br>서비스 시간을 피해서 resource cleanup을 사용하시는 것이 안전합니다.<br> <br>감사합니다.<br><br><publishedDate>2017-08-29T05:19:23Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KSm1mIAD"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-28T15:48:58Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-28T15:48:58Z</b><br><br>constraint order를 삭제 후 테스트 했습니다.<br><br>말씀주셨듯이 여러가지 테스트를 해보았으나 똑같은 내용입니다.<br>제가 달리 질문 드리겠습니다.<br>아래 제가 테스트한 내용을 보시면 <br><br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Forcing the status of all resources to be redetected<br>Aug 29 00:02:03 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm1 [ NOT RUNNING ]<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC1_script_monitor_0: ok (node=node1-hb, call=145, rc=0, cib-update=134, confirmed=true)<br>Aug 29 00:02:03 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm2 [ NOT RUNNING ]<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag1_monitor_0: ok (node=node1-hb, call=137, rc=0, cib-update=135, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC2_script_monitor_0: ok (node=node1-hb, call=161, rc=0, cib-update=136, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC1_vip_monitor_0: ok (node=node1-hb, call=141, rc=0, cib-update=137, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC2_vip_monitor_0: ok (node=node1-hb, call=157, rc=0, cib-update=138, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag2_monitor_0: ok (node=node1-hb, call=153, rc=0, cib-update=139, confirmed=true)<br>Aug 29 00:02:04 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP1VG_monitor_0: ok (node=node1-hb, call=133, rc=0, cib-update=140, confirmed=true)<br>Aug 29 00:02:04 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP2VG_monitor_0: ok (node=node1-hb, call=149, rc=0, cib-update=141, confirmed=true)<br>Aug 29 00:02:10 NODE1 crmd[4007]:  notice: Operation kdump_stonith_monitor_0: not running (node=node1-hb, call=176, rc=7, cib-update=142, confirmed=true)<br>Aug 29 00:02:16 NODE1 crmd[4007]:  notice: Operation ping_monitor_0: ok (node=node1-hb, call=172, rc=0, cib-update=149, confirmed=true)<br><br>위 내용이 리소스를 올리고 내렸다(stop start)는 말씀인지요? 제가 보기에는 모니터 상태만 체크한것 같은데요...<br><br>맨 아래 테스트에서는 해당 리소스가 정상으로 되지 않은상태에서 리소스를 클린업 했을때<br>리소스를 stop  시키고 start 를 진행하는것을 볼수 있습니다.<br>엄연히 로그가 틀린것으로 보입니다. 이점 확인 부탁드리고<br><br>정상적인 상태에서 리소스 클린업을 했을때 모든 리소스가 stop &amp; start 하지 않는것으로 알고 있습니다.<br>또한 번외로 정상상태로 만들어 놓은후 unmanage 상태로 하지 않은 상태에서 resource cleanup를 하여도 해당 로그는 똑같음을 <br>확인하였습니다.<br><br>=================================태스트 내용======================================================<br>&lt;&lt;before type command &quot;pcs constraint order remove LV_APP_con_ctmag1&quot; &gt;&gt;<br><br>root@NODE1 /root # pcs constraint show --full<br>Location Constraints:<br>  Resource: LV_APP_con_ctmag2<br>    Enabled on: node01-hb (score:10) (id:location-LV_APP_con_ctmag2-node01-hb-10)<br>  Resource: RESC1_group<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_group-node01-hb-10)<br>    Enabled on: node1-hb (score:INFINITY) (role: Started) (id:cli-prefer-RESC1_group)<br>  Resource: RESC1_script<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_script-node01-hb-10)<br>  Resource: RESC1_vip<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_vip-node01-hb-10)<br>    Constraint: location-RESC1_vip<br>      Rule: score=-INFINITY boolean-op=or  (id:location-RESC1_vip-rule)<br>        Expression: pingd lt 1  (id:location-RESC1_vip-rule-expr)<br>        Expression: not_defined pingd  (id:location-RESC1_vip-rule-expr-1)<br>  Resource: RESC2_script<br>    Enabled on: node01-hb (score:10) (id:location-RESC2_script-node01-hb-10)<br>  Resource: RESC2_vip<br>    Enabled on: node01-hb (score:10) (id:location-RESC2_vip-node01-hb-10)<br>    Constraint: location-RESC2_vip<br>      Rule: score=-INFINITY boolean-op=or  (id:location-RESC2_vip-rule)<br>        Expression: pingd lt 1  (id:location-RESC2_vip-rule-expr)<br>        Expression: not_defined pingd  (id:location-RESC2_vip-rule-expr-1)<br>  Resource: VG_SRPCTAAP1VG<br>    Enabled on: node01-hb (score:10) (id:location-VG_SRPCTAAP1VG-node01-hb-10)<br>  Resource: VG_SRPCTAAP2VG<br>    Enabled on: node01-hb (score:10) (id:location-VG_SRPCTAAP2VG-node01-hb-10)<br>Ordering Constraints:<br>  start VG_SRPCTAAP1VG then start LV_APP_con_ctmag1 (kind:Mandatory) (id:order-VG_SRPCTAAP1VG-LV_APP_con_ctmag1-mandatory)<br>  start LV_APP_con_ctmag1 then start RESC1_vip (kind:Mandatory) (id:order-LV_APP_con_ctmag1-RESC1_vip-mandatory)<br>  start RESC1_vip then start RESC1_script (kind:Mandatory) (id:order-RESC1_vip-RESC1_script-mandatory)<br>  start RESC1_script then start VG_SRPCTAAP2VG (kind:Mandatory) (id:order-RESC1_script-VG_SRPCTAAP2VG-mandatory)<br>  start VG_SRPCTAAP2VG then start LV_APP_con_ctmag2 (kind:Mandatory) (id:order-VG_SRPCTAAP2VG-LV_APP_con_ctmag2-mandatory)<br>  start LV_APP_con_ctmag2 then start RESC2_vip (kind:Mandatory) (id:order-LV_APP_con_ctmag2-RESC2_vip-mandatory)<br>  start RESC2_vip then start RESC2_script (kind:Mandatory) (id:order-RESC2_vip-RESC2_script-mandatory)<br>Colocation Constraints:<br><br>&lt;&lt;after type command &quot;pcs constraint order remove LV_APP_con_ctmag1&quot; &gt;&gt;<br><br>root@NODE1 /root # pcs constraint show --full<br>Location Constraints:<br>  Resource: LV_APP_con_ctmag2<br>    Enabled on: node01-hb (score:10) (id:location-LV_APP_con_ctmag2-node01-hb-10)<br>  Resource: RESC1_group<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_group-node01-hb-10)<br>  Resource: RESC1_script<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_script-node01-hb-10)<br>  Resource: RESC1_vip<br>    Enabled on: node01-hb (score:10) (id:location-RESC1_vip-node01-hb-10)<br>    Constraint: location-RESC1_vip<br>      Rule: score=-INFINITY boolean-op=or  (id:location-RESC1_vip-rule)<br>        Expression: pingd lt 1  (id:location-RESC1_vip-rule-expr)<br>        Expression: not_defined pingd  (id:location-RESC1_vip-rule-expr-1)<br>  Resource: RESC2_script<br>    Enabled on: node01-hb (score:10) (id:location-RESC2_script-node01-hb-10)<br>  Resource: RESC2_vip<br>    Enabled on: node01-hb (score:10) (id:location-RESC2_vip-node01-hb-10)<br>    Constraint: location-RESC2_vip<br>      Rule: score=-INFINITY boolean-op=or  (id:location-RESC2_vip-rule)<br>        Expression: pingd lt 1  (id:location-RESC2_vip-rule-expr)<br>        Expression: not_defined pingd  (id:location-RESC2_vip-rule-expr-1)<br>  Resource: VG_SRPCTAAP1VG<br>    Enabled on: node01-hb (score:10) (id:location-VG_SRPCTAAP1VG-node01-hb-10)<br>  Resource: VG_SRPCTAAP2VG<br>    Enabled on: node01-hb (score:10) (id:location-VG_SRPCTAAP2VG-node01-hb-10)<br>Ordering Constraints:<br>  start RESC1_vip then start RESC1_script (kind:Mandatory) (id:order-RESC1_vip-RESC1_script-mandatory)<br>  start RESC1_script then start VG_SRPCTAAP2VG (kind:Mandatory) (id:order-RESC1_script-VG_SRPCTAAP2VG-mandatory)<br>  start VG_SRPCTAAP2VG then start LV_APP_con_ctmag2 (kind:Mandatory) (id:order-VG_SRPCTAAP2VG-LV_APP_con_ctmag2-mandatory)<br>  start LV_APP_con_ctmag2 then start RESC2_vip (kind:Mandatory) (id:order-LV_APP_con_ctmag2-RESC2_vip-mandatory)<br>  start RESC2_vip then start RESC2_script (kind:Mandatory) (id:order-RESC2_vip-RESC2_script-mandatory)<br>Colocation Constraints:<br>======================<br><br>=====================================================================================================================================<br><br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  435M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1   89M  4.8M   84M   6% /APP/controlm/ctmag1<br>root@NODE1 /root # umount /APP/controlm/ctmag1<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  435M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br><br>&lt;&lt;&lt;pcs status&gt;&gt;&gt;<br><br>Every 0.5s: pcs status                                                        Mon Aug 28 23:56:18 2017<br><br>Cluster name: testhalvm<br>Last updated: Mon Aug 28 23:56:18 2017          Last change: Mon Aug 28 23:51:14 2017 by hacluster via<br> crmd on node2-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (failure ignored)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>Failed Actions:<br>* LV_APP_con_ctmag1_monitor_5000 on node1-hb 'not running' (7): call=108, status=complete, exitreason=<br>'none',<br>    last-rc-change='Mon Aug 28 23:56:13 2017', queued=0ms, exec=0ms<br><br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br><br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;<br>Aug 28 23:56:12 NODE1 kernel: XFS (dm-3): Unmounting Filesystem<br><br><br>=======================================================================================================<br>2. 해당 문제 볼륨 마운트<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  435M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>root@NODE1 /root # mount /dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 /APP/controlm/ctmag1<br>root@NODE1 /root # d f-h<br>-bash: d: command not found<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  437M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1   89M  4.8M   84M   6% /APP/controlm/ctmag1<br><br>&lt;&lt;&lt;pcs status&gt;&gt;&gt;<br>Every 0.5s: pcs status                                                        Mon Aug 28 23:58:51 2017<br><br>Cluster name: testhalvm<br>Last updated: Mon Aug 28 23:58:51 2017          Last change: Mon Aug 28 23:51:14 2017 by hacluster via<br> crmd on node2-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (failure ignored)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>Failed Actions:<br>* LV_APP_con_ctmag1_monitor_5000 on node1-hb 'not running' (7): call=108, status=complete, exitreason=<br>'none',<br>    last-rc-change='Mon Aug 28 23:56:13 2017', queued=0ms, exec=0ms<br><br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;<br>Aug 28 23:57:58 NODE1 kernel: XFS (dm-3): Mounting V4 Filesystem<br>Aug 28 23:57:59 NODE1 kernel: XFS (dm-3): Ending clean mount<br>Aug 28 23:58:01 NODE1 CROND[26917]: (pcp) CMD ( /usr/libexec/pcp/bin/pmie_check -C)<br><br><br>=======================================================================================================<br>3. unmanage 상태 진입<br><br>root@NODE1 /root # pcs resource unmanage LV_APP_con_ctmag1<br><br><br>&lt;&lt;&lt;pcs status&gt;&gt;&gt;<br><br>Every 0.5s: pcs status                                                        Tue Aug 29 00:00:43 2017<br><br>Cluster name: testhalvm<br>Last updated: Tue Aug 29 00:00:43 2017          Last change: Mon Aug 28 23:59:43 2017 by root via crm_<br>resource on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (unmanaged)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br>======================<br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;<br><br>Aug 28 23:56:12 NODE1 kernel: XFS (dm-3): Unmounting Filesystem<br>Aug 28 23:57:58 NODE1 kernel: XFS (dm-3): Mounting V4 Filesystem<br>Aug 28 23:57:59 NODE1 kernel: XFS (dm-3): Ending clean mount<br>Aug 28 23:58:01 NODE1 CROND[26917]: (pcp) CMD ( /usr/libexec/pcp/bin/pmie_check -C)<br>Aug 28 23:59:42 NODE1 stonith-ng[4003]:  notice: [cib_diff_notify] Patch aborted: Application of an update diff failed (-206)<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit dmraid-activation.service, ignoring: Unit dmraid-activation.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit alsa-restore.service, ignoring: Unit alsa-restore.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit alsa-state.service, ignoring: Unit alsa-state.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit avahi-daemon.socket, ignoring: Unit avahi-daemon.socket is masked.<br>Aug 29 00:01:01 NODE1 CROND[32717]: (root) CMD (run-parts /etc/cron.hourly)<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32717]: starting 0anacron<br>Aug 29 00:01:01 NODE1 anacron[32727]: Anacron started on 2017-08-29<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32729]: finished 0anacron<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32717]: starting 0yum-hourly.cron<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32735]: finished 0yum-hourly.cron<br>Aug 29 00:01:01 NODE1 anacron[32727]: Normal exit (0 jobs run)<br><br><br>=======================================================================================================<br>4. pcs resource cleanup 진행<br><br>&lt;&lt;&lt;pcs status&gt;&gt;&gt;<br><br>Every 0.5s: pcs status                                                        Tue Aug 29 00:12:08 2017<br><br>Cluster name: testhalvm<br>Last updated: Tue Aug 29 00:12:08 2017          Last change: Tue Aug 29 00:11:48 2017 by root via ciba<br>dmin on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (unmanaged)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br>======================<br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;<br>Aug 28 23:56:12 NODE1 kernel: XFS (dm-3): Unmounting Filesystem<br>Aug 28 23:57:58 NODE1 kernel: XFS (dm-3): Mounting V4 Filesystem<br>Aug 28 23:57:59 NODE1 kernel: XFS (dm-3): Ending clean mount<br>Aug 28 23:58:01 NODE1 CROND[26917]: (pcp) CMD ( /usr/libexec/pcp/bin/pmie_check -C)<br>Aug 28 23:59:42 NODE1 stonith-ng[4003]:  notice: [cib_diff_notify] Patch aborted: Application of an update diff failed (-206)<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit dmraid-activation.service, ignoring: Unit dmraid-activation.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit alsa-restore.service, ignoring: Unit alsa-restore.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit alsa-state.service, ignoring: Unit alsa-state.service is masked.<br>Aug 29 00:00:03 NODE1 systemd: Cannot add dependency job for unit avahi-daemon.socket, ignoring: Unit avahi-daemon.socket is masked.<br>Aug 29 00:01:01 NODE1 CROND[32717]: (root) CMD (run-parts /etc/cron.hourly)<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32717]: starting 0anacron<br>Aug 29 00:01:01 NODE1 anacron[32727]: Anacron started on 2017-08-29<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32729]: finished 0anacron<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32717]: starting 0yum-hourly.cron<br>Aug 29 00:01:01 NODE1 run-parts(/etc/cron.hourly)[32735]: finished 0yum-hourly.cron<br>Aug 29 00:01:01 NODE1 anacron[32727]: Normal exit (0 jobs run)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Forcing the status of all resources to be redetected<br>Aug 29 00:02:03 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm1 [ NOT RUNNING ]<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC1_script_monitor_0: ok (node=node1-hb, call=145, rc=0, cib-update=134, confirmed=true)<br>Aug 29 00:02:03 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm2 [ NOT RUNNING ]<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag1_monitor_0: ok (node=node1-hb, call=137, rc=0, cib-update=135, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC2_script_monitor_0: ok (node=node1-hb, call=161, rc=0, cib-update=136, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC1_vip_monitor_0: ok (node=node1-hb, call=141, rc=0, cib-update=137, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation RESC2_vip_monitor_0: ok (node=node1-hb, call=157, rc=0, cib-update=138, confirmed=true)<br>Aug 29 00:02:03 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag2_monitor_0: ok (node=node1-hb, call=153, rc=0, cib-update=139, confirmed=true)<br>Aug 29 00:02:04 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP1VG_monitor_0: ok (node=node1-hb, call=133, rc=0, cib-update=140, confirmed=true)<br>Aug 29 00:02:04 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP2VG_monitor_0: ok (node=node1-hb, call=149, rc=0, cib-update=141, confirmed=true)<br>Aug 29 00:02:10 NODE1 crmd[4007]:  notice: Operation kdump_stonith_monitor_0: not running (node=node1-hb, call=176, rc=7, cib-update=142, confirmed=true)<br>Aug 29 00:02:16 NODE1 crmd[4007]:  notice: Operation ping_monitor_0: ok (node=node1-hb, call=172, rc=0, cib-update=149, confirmed=true)<br>======================<br><br>=======================================================================================================<br>cf.<br>&lt;정상상태로 복구 하지 않은상태(언마운트상태)에서 resource cleanup&gt;<br><br><br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.1M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  440M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1   89M  4.8M   84M   6% /APP/controlm/ctmag1<br>root@NODE1 /root # umount /APP/controlm/ctmag1<br>root@NODE1 /root # pcs resource cleanup<br>Waiting for 1 replies from the CRMd. OK<br>======================<br>Aug 29 00:16:07 NODE1 kernel: XFS (dm-3): Unmounting Filesystem<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Forcing the status of all resources to be redetected<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag1_monitor_0: not running (node=node1-hb, call=854, rc=7, cib-update=507, confirmed=true)<br>Aug 29 00:16:19 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm1 [ NOT RUNNING ]<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation RESC1_script_monitor_0: ok (node=node1-hb, call=862, rc=0, cib-update=508, confirmed=true)<br>Aug 29 00:16:19 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm2 [ NOT RUNNING ]<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation RESC2_script_monitor_0: ok (node=node1-hb, call=878, rc=0, cib-update=509, confirmed=true)<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation RESC1_vip_monitor_0: ok (node=node1-hb, call=858, rc=0, cib-update=510, confirmed=true)<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation RESC2_vip_monitor_0: ok (node=node1-hb, call=874, rc=0, cib-update=511, confirmed=true)<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag2_monitor_0: ok (node=node1-hb, call=870, rc=0, cib-update=512, confirmed=true)<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP1VG_monitor_0: ok (node=node1-hb, call=850, rc=0, cib-update=513, confirmed=true)<br>Aug 29 00:16:19 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP2VG_monitor_0: ok (node=node1-hb, call=866, rc=0, cib-update=514, confirmed=true)<br>Aug 29 00:16:26 NODE1 crmd[4007]:  notice: Operation ping_monitor_0: ok (node=node1-hb, call=883, rc=0, cib-update=515, confirmed=true)<br>Aug 29 00:16:26 NODE1 crmd[4007]:  notice: Operation kdump_stonith_monitor_0: not running (node=node1-hb, call=889, rc=7, cib-update=516, confirmed=true)<br>Aug 29 00:16:26 NODE1 logger: EXCUTE : su - ctmag2 -c /APP/cluster_script/ctmag2_stop<br>Aug 29 00:16:26 NODE1 logger: RESULT : su - ctmag2 -c /APP/cluster_script/ctmag2_stop [ 0 ]<br>Aug 29 00:16:26 NODE1 logger: RESULT : pkill user [ 0 ]<br>Aug 29 00:16:26 NODE1 crmd[4007]:  notice: Operation RESC2_script_stop_0: ok (node=node1-hb, call=890, rc=0, cib-update=517, confirmed=true)<br>Aug 29 00:16:32 NODE1 IPaddr2(RESC2_vip)[57439]: INFO: IP status = ok, IP_CIP=<br>Aug 29 00:16:32 NODE1 crmd[4007]:  notice: Operation RESC2_vip_stop_0: ok (node=node1-hb, call=891, rc=0, cib-update=520, confirmed=true)<br>Aug 29 00:16:32 NODE1 Filesystem(LV_APP_con_ctmag2)[57492]: INFO: Running stop for /dev/mapper/SRPCTAAP2VG-APP_con_ctmag2 on /APP/controlm/ctmag2<br>Aug 29 00:16:32 NODE1 Filesystem(LV_APP_con_ctmag2)[57492]: INFO: Trying to unmount /APP/controlm/ctmag2<br>Aug 29 00:16:32 NODE1 kernel: XFS (dm-4): Unmounting Filesystem<br>Aug 29 00:16:33 NODE1 Filesystem(LV_APP_con_ctmag2)[57492]: INFO: unmounted /APP/controlm/ctmag2 successfully<br>Aug 29 00:16:33 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag2_stop_0: ok (node=node1-hb, call=892, rc=0, cib-update=521, confirmed=true)<br>Aug 29 00:16:33 NODE1 LVM(VG_SRPCTAAP2VG)[57571]: INFO: Deactivating volume group SRPCTAAP2VG<br>Aug 29 00:16:33 NODE1 multipathd: dm-4: remove map (uevent)<br>Aug 29 00:16:33 NODE1 multipathd: dm-4: devmap not registered, can't remove<br>Aug 29 00:16:33 NODE1 multipathd: dm-4: remove map (uevent)<br>Aug 29 00:16:33 NODE1 LVM(VG_SRPCTAAP2VG)[57571]: INFO: 0 logical volume(s) in volume group &quot;SRPCTAAP2VG&quot; now active<br>Aug 29 00:16:33 NODE1 LVM(VG_SRPCTAAP2VG)[57571]: INFO: LVM Volume SRPCTAAP2VG is not available (stopped)<br>Aug 29 00:16:33 NODE1 LVM(VG_SRPCTAAP2VG)[57571]: INFO: Stripping tag, pacemaker<br>Aug 29 00:16:33 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP2VG_stop_0: ok (node=node1-hb, call=893, rc=0, cib-update=522, confirmed=true)<br>Aug 29 00:16:33 NODE1 logger: EXCUTE : su - ctmag1 -c /APP/cluster_script/ctmag1_stop<br>Aug 29 00:16:33 NODE1 logger: RESULT : su - ctmag1 -c /APP/cluster_script/ctmag1_stop [ 0 ]<br>Aug 29 00:16:33 NODE1 logger: RESULT : pkill user [ 0 ]<br>Aug 29 00:16:33 NODE1 crmd[4007]:  notice: Operation RESC1_script_stop_0: ok (node=node1-hb, call=894, rc=0, cib-update=523, confirmed=true)<br>Aug 29 00:16:33 NODE1 IPaddr2(RESC1_vip)[57645]: INFO: IP status = ok, IP_CIP=<br>Aug 29 00:16:33 NODE1 crmd[4007]:  notice: Operation RESC1_vip_stop_0: ok (node=node1-hb, call=895, rc=0, cib-update=524, confirmed=true)<br>Aug 29 00:16:35 NODE1 ntpd[2727]: Deleting interface #7 eno16777736, 192.168.0.177#123, interface stats: received=0, sent=0, dropped=0, active_time=1797 secs<br>Aug 29 00:16:35 NODE1 ntpd[2727]: Deleting interface #6 eno16777736, 192.168.0.77#123, interface stats: received=0, sent=0, dropped=0, active_time=1814 secs<br>Aug 29 00:16:38 NODE1 Filesystem(LV_APP_con_ctmag1)[57782]: INFO: Running start for /dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 on /APP/controlm/ctmag1<br>Aug 29 00:16:38 NODE1 kernel: XFS (dm-3): Mounting V4 Filesystem<br>Aug 29 00:16:38 NODE1 kernel: XFS (dm-3): Ending clean mount<br>Aug 29 00:16:38 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag1_start_0: ok (node=node1-hb, call=896, rc=0, cib-update=525, confirmed=true)<br>Aug 29 00:16:41 NODE1 IPaddr2(RESC1_vip)[57914]: INFO: Adding inet address 192.168.0.77/24 with broadcast address 192.168.0.255 to device eno16777736<br>Aug 29 00:16:41 NODE1 IPaddr2(RESC1_vip)[57914]: INFO: Bringing device eno16777736 up<br>Aug 29 00:16:41 NODE1 IPaddr2(RESC1_vip)[57914]: INFO: /usr/libexec/heartbeat/send_arp -i 200 -r 5 -p /var/run/resource-agents/send_arp-192.168.0.77 eno16777736 192.168.0.77 auto not_used not_used<br>Aug 29 00:16:41 NODE1 crmd[4007]:  notice: Operation RESC1_vip_start_0: ok (node=node1-hb, call=898, rc=0, cib-update=527, confirmed=true)<br>Aug 29 00:16:41 NODE1 logger: EXCUTE : su - ctmag1 -c /APP/cluster_script/ctmag1_start<br>Aug 29 00:16:41 NODE1 logger: RESULT : su - ctmag1 -c /APP/cluster_script/ctmag1_start [ 0 ]<br>Aug 29 00:16:43 NODE1 ntpd[2727]: Listen normally on 8 eno16777736 192.168.0.77 UDP 123<br>Aug 29 00:16:43 NODE1 ntpd[2727]: new interface(s) found: waking up resolver<br>Aug 29 00:16:44 NODE1 crmd[4007]:  notice: Operation RESC1_script_start_0: ok (node=node1-hb, call=900, rc=0, cib-update=528, confirmed=true)<br>Aug 29 00:16:50 NODE1 LVM(VG_SRPCTAAP2VG)[58244]: INFO: Activating volume group SRPCTAAP2VG<br>Aug 29 00:16:50 NODE1 LVM(VG_SRPCTAAP2VG)[58244]: INFO: Reading all physical volumes. This may take a while... Found volume group &quot;vg0&quot; using metadata type lvm2 Found volume group &quot;SRPCTAAP1VG&quot; using metadata type lvm2 Found volume group &quot;SRPCTAAP2VG&quot; using metadata type lvm2<br>Aug 29 00:16:50 NODE1 LVM(VG_SRPCTAAP2VG)[58244]: INFO: New tag &quot;pacemaker&quot; added to SRPCTAAP2VG<br>Aug 29 00:16:50 NODE1 LVM(VG_SRPCTAAP2VG)[58244]: INFO: 1 logical volume(s) in volume group &quot;SRPCTAAP2VG&quot; now active<br>Aug 29 00:16:50 NODE1 crmd[4007]:  notice: Operation VG_SRPCTAAP2VG_start_0: ok (node=node1-hb, call=901, rc=0, cib-update=530, confirmed=true)<br>Aug 29 00:16:55 NODE1 Filesystem(LV_APP_con_ctmag2)[58467]: INFO: Running start for /dev/mapper/SRPCTAAP2VG-APP_con_ctmag2 on /APP/controlm/ctmag2<br>Aug 29 00:16:55 NODE1 kernel: XFS (dm-4): Mounting V4 Filesystem<br>Aug 29 00:16:55 NODE1 kernel: XFS (dm-4): Ending clean mount<br>Aug 29 00:16:55 NODE1 crmd[4007]:  notice: Operation LV_APP_con_ctmag2_start_0: ok (node=node1-hb, call=903, rc=0, cib-update=532, confirmed=true)<br>Aug 29 00:16:58 NODE1 IPaddr2(RESC2_vip)[58628]: INFO: Adding inet address 192.168.0.177/24 with broadcast address 192.168.0.255 to device eno16777736<br>Aug 29 00:16:58 NODE1 IPaddr2(RESC2_vip)[58628]: INFO: Bringing device eno16777736 up<br>Aug 29 00:16:59 NODE1 IPaddr2(RESC2_vip)[58628]: INFO: /usr/libexec/heartbeat/send_arp -i 200 -r 5 -p /var/run/resource-agents/send_arp-192.168.0.177 eno16777736 192.168.0.177 auto not_used not_used<br>Aug 29 00:16:59 NODE1 crmd[4007]:  notice: Operation RESC2_vip_start_0: ok (node=node1-hb, call=905, rc=0, cib-update=534, confirmed=true)<br>Aug 29 00:16:59 NODE1 logger: EXCUTE : su - ctmag2 -c /APP/cluster_script/ctmag2_start<br>Aug 29 00:16:59 NODE1 logger: RESULT : su - ctmag2 -c /APP/cluster_script/ctmag2_start [ 0 ]<br>Aug 29 00:17:00 NODE1 ntpd[2727]: Listen normally on 9 eno16777736 192.168.0.177 UDP 123<br>Aug 29 00:17:00 NODE1 ntpd[2727]: new interface(s) found: waking up resolver<br>Aug 29 00:17:02 NODE1 crmd[4007]:  notice: Operation RESC2_script_start_0: ok (node=node1-hb, call=907, rc=0, cib-update=535, confirmed=true)<br><br><publishedDate>2017-08-28T15:48:58Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KSeGDIA1"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-28T05:26:22Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-28T05:26:21Z</b><br><br>안녕하세요,<br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>8월 26일 오후 12시40분에 남겨주신 메시지에 대하여 추가 확인 부탁드립니다.<br><br>===============================================================================<br>4. 문제 볼륨 리소스에 대한 failcount 리셋<br>root@NODE1 /root # pcs resource cleanup LV_APP_con_ctmag1<br>Waiting for 16 replies from the CRMd................ OK<br>Cleaning up VG_SRPCTAAP1VG on node1-hb, removing fail-count-VG_SRPCTAAP1VG<br>Cleaning up VG_SRPCTAAP1VG on node2-hb, removing fail-count-VG_SRPCTAAP1VG<br>Cleaning up LV_APP_con_ctmag1 on node1-hb, removing fail-count-LV_APP_con_ctmag1<br>Cleaning up LV_APP_con_ctmag1 on node2-hb, removing fail-count-LV_APP_con_ctmag1<br>Cleaning up RESC1_vip on node1-hb, removing fail-count-RESC1_vip<br>Cleaning up RESC1_vip on node2-hb, removing fail-count-RESC1_vip<br>Cleaning up RESC1_script on node1-hb, removing fail-count-RESC1_script<br>Cleaning up RESC1_script on node2-hb, removing fail-count-RESC1_script<br>Cleaning up VG_SRPCTAAP2VG on node1-hb, removing fail-count-VG_SRPCTAAP2VG<br>Cleaning up VG_SRPCTAAP2VG on node2-hb, removing fail-count-VG_SRPCTAAP2VG<br>Cleaning up LV_APP_con_ctmag2 on node1-hb, removing fail-count-LV_APP_con_ctmag2<br>Cleaning up LV_APP_con_ctmag2 on node2-hb, removing fail-count-LV_APP_con_ctmag2<br>Cleaning up RESC2_vip on node1-hb, removing fail-count-RESC2_vip<br>Cleaning up RESC2_vip on node2-hb, removing fail-count-RESC2_vip<br>Cleaning up RESC2_script on node1-hb, removing fail-count-RESC2_script<br>Cleaning up RESC2_script on node2-hb, removing fail-count-RESC2_script<br>* The configuration prevents the cluster from stopping or starting 'RESC1_group' (unmanaged)<br>RESC1_group이 중지 혹은 시작 되지 않는다고 명시해주고 있습니다.<br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>Aug 25 23:59:49 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:50 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:51 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:52 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:54 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm1 [ NOT RUNNING ]<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC1_script_monitor_0: ok (node=node1-hb, call=110, rc=0, cib-update=106, confirmed=true)<br>Aug 25 23:59:54 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm2 [ NOT RUNNING ]<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC2_script_monitor_0: ok (node=node1-hb, call=126, rc=0, cib-update=107, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation LV_APP_con_ctmag2_monitor_0: ok (node=node1-hb, call=118, rc=0, cib-update=108, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC1_vip_monitor_0: ok (node=node1-hb, call=106, rc=0, cib-update=109, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC2_vip_monitor_0: ok (node=node1-hb, call=122, rc=0, cib-update=110, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation LV_APP_con_ctmag1_monitor_0: ok (node=node1-hb, call=102, rc=0, cib-update=111, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation VG_SRPCTAAP1VG_monitor_0: ok (node=node1-hb, call=98, rc=0, cib-update=112, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation VG_SRPCTAAP2VG_monitor_0: ok (node=node1-hb, call=114, rc=0, cib-update=113, confirmed=true)<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit dmraid-activation.service, ignoring: Unit dmraid-activation.service is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit alsa-restore.service, ignoring: Unit alsa-restore.service is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit avahi-daemon.socket, ignoring: Unit avahi-daemon.socket is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit alsa-state.service, ignoring: Unit alsa-state.service is masked.<br>Aug 26 00:01:01 NODE1 CROND[39770]: (root) CMD (run-parts /etc/cron.hourly)<br>Aug 26 00:01:01 NODE1 run-parts(/etc/cron.hourly)[39770]: starting 0anacron<br>Aug 26 00:01:02 NODE1 anacron[39780]: Anacron started on 2017-08-26<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39782]: finished 0anacron<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39770]: starting 0yum-hourly.cron<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39788]: finished 0yum-hourly.cron<br>Aug 26 00:01:02 NODE1 anacron[39780]: Normal exit (0 jobs run)<br>===============================================================================<br> <br>보내주신 로그를 보면 pcs resource cleanup LV_APP_con_ctmag1 명령어 입력 후 결과를 보면<br>resource 그룹에 있는 모든 resource에 대하여 cleaning up 작업을 진행합니다.<br> <br>요청드리는 테스트 결과는 resource group과 resource constraint를 제거하고 pcs resource cleanup LV_APP_con_ctmag1 명령어를 입력할 경우에도<br>모든 resource를 cleaning up 하는지 확인 요청 드립니다.<br> <br>감사합니다.<br><br><publishedDate>2017-08-28T05:26:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KSd1MIAT"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-28T01:10:09Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-28T01:10:09Z</b><br><br>안녕하세요,  <br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>추가 문의에 대하여 확인 중에 있습니다.<br> 확인 후 연락드리도록 하겠습니다.<br> <br>감사합니다.<br><br><publishedDate>2017-08-28T01:10:09Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KSTBvIAP"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-25T15:40:09Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-25T15:40:09Z</b><br><br>말씀주신대로 혹시나 제가 놓친 부분이 있지는 않을까해서 <br>로그를 tail -f로 하여 모니터링하면서 해당 사항을 같이 봤으나 <br>전혀 다른 리소스에 대해서는 흔들림이 없는것을 확인하였습니다.<br>아래 테스트 한 내용을 캡처하여 놓았으니 확인 부탁드립니다.<br><br>==================================================================<br>0. 정상적인 파일시스템 상태<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  463M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1   89M  4.8M   84M   6% /APP/controlm/ctmag1<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br><br><br>==========================================================================<br><br><br>1. 임의의 리소스 볼륨 umount<br>root@NODE1 /root # umount /APP/controlm/ctmag1<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  465M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>root@NODE1 /root #<br><br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>Aug 25 23:48:56 NODE1 kernel: XFS (dm-3): Unmounting Filesystem<br><br><br>&lt;&lt;&lt;&lt;pcs status &gt;&gt;&gt;&gt;<br>Every 0.5s: pcs status                                                                                                              Fri Aug 25 23:49:49 2017<br><br>Cluster name: testhalvm<br>Last updated: Fri Aug 25 23:49:49 2017          Last change: Fri Aug 25 23:47:03 2017 by root via cibadmin on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (failure ignored)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>Failed Actions:<br>* LV_APP_con_ctmag1_monitor_5000 on node1-hb 'not running' (7): call=50, status=complete, exitreason='none',<br>    last-rc-change='Fri Aug 25 23:48:58 2017', queued=0ms, exec=0ms<br><br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br>======================<br>========================================================================================================<br>2. umount 되었던 파일시스템 마운트 작업<br><br>root@NODE1 /root # mount /dev/mapper/SRPCTAAP1VG-APP_con_ctmag1 /APP/controlm/ctmag1<br>root@NODE1 /root #<br>root@NODE1 /root # df -h<br>Filesystem                              Size  Used Avail Use% Mounted on<br>/dev/sda2                                15G  5.3G  9.8G  35% /<br>devtmpfs                                899M     0  899M   0% /dev<br>tmpfs                                   3.6G   39M  3.6G   2% /dev/shm<br>tmpfs                                   913M  9.0M  904M   1% /run<br>tmpfs                                   913M     0  913M   0% /sys/fs/cgroup<br>/dev/sda1                              1014M  158M  857M  16% /boot<br>/dev/sda3                                10G  465M  9.6G   5% /var<br>/dev/mapper/vg0-lvol0                    97M  5.2M   92M   6% /disk1<br>tmpfs                                   183M     0  183M   0% /run/user/0<br>/dev/mapper/SRPCTAAP2VG-APP_con_ctmag2   89M  4.8M   84M   6% /APP/controlm/ctmag2<br>/dev/mapper/SRPCTAAP1VG-APP_con_ctmag1   89M  4.8M   84M   6% /APP/controlm/ctmag1<br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>Aug 25 23:54:07 NODE1 kernel: XFS (dm-3): Mounting V4 Filesystem<br>Aug 25 23:54:07 NODE1 kernel: XFS (dm-3): Ending clean mount<br><br>&lt;&lt;&lt;&lt;pcs status &gt;&gt;&gt;&gt;<br>Every 0.5s: pcs status                                                                                                              Fri Aug 25 23:55:31 2017<br><br>Cluster name: testhalvm<br>Last updated: Fri Aug 25 23:55:32 2017          Last change: Fri Aug 25 23:52:59 2017 by root via cibadmin on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (failure ignored)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>Failed Actions:<br>* LV_APP_con_ctmag1_monitor_5000 on node1-hb 'not running' (7): call=50, status=complete, exitreason='none',<br>    last-rc-change='Fri Aug 25 23:48:58 2017', queued=0ms, exec=0ms<br><br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br><br><br>========================================================================================================<br><br>3. failcount 리셋하기 위하여 해당 리소스에 대하여 unmanage 상태로 변경<br><br>root@NODE1 /root # pcs resource unmanage LV_APP_con_ctmag1<br>root@NODE1 /root #<br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>어떠한 로그도 찍히지 않음을 확인<br><br><br>&lt;&lt;&lt;&lt;pcs status &gt;&gt;&gt;&gt;<br><br>Every 0.5s: pcs status                                                                                                              Fri Aug 25 23:58:31 2017<br><br>Cluster name: testhalvm<br>Last updated: Fri Aug 25 23:58:32 2017          Last change: Fri Aug 25 23:56:09 2017 by root via crm_resource on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (unmanaged)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br><br><br>========================================================================================================<br>4. 문제 볼륨 리소스에 대한 failcount 리셋<br><br>root@NODE1 /root # pcs resource cleanup LV_APP_con_ctmag1<br>Waiting for 16 replies from the CRMd................ OK<br>Cleaning up VG_SRPCTAAP1VG on node1-hb, removing fail-count-VG_SRPCTAAP1VG<br>Cleaning up VG_SRPCTAAP1VG on node2-hb, removing fail-count-VG_SRPCTAAP1VG<br>Cleaning up LV_APP_con_ctmag1 on node1-hb, removing fail-count-LV_APP_con_ctmag1<br>Cleaning up LV_APP_con_ctmag1 on node2-hb, removing fail-count-LV_APP_con_ctmag1<br>Cleaning up RESC1_vip on node1-hb, removing fail-count-RESC1_vip<br>Cleaning up RESC1_vip on node2-hb, removing fail-count-RESC1_vip<br>Cleaning up RESC1_script on node1-hb, removing fail-count-RESC1_script<br>Cleaning up RESC1_script on node2-hb, removing fail-count-RESC1_script<br>Cleaning up VG_SRPCTAAP2VG on node1-hb, removing fail-count-VG_SRPCTAAP2VG<br>Cleaning up VG_SRPCTAAP2VG on node2-hb, removing fail-count-VG_SRPCTAAP2VG<br>Cleaning up LV_APP_con_ctmag2 on node1-hb, removing fail-count-LV_APP_con_ctmag2<br>Cleaning up LV_APP_con_ctmag2 on node2-hb, removing fail-count-LV_APP_con_ctmag2<br>Cleaning up RESC2_vip on node1-hb, removing fail-count-RESC2_vip<br>Cleaning up RESC2_vip on node2-hb, removing fail-count-RESC2_vip<br>Cleaning up RESC2_script on node1-hb, removing fail-count-RESC2_script<br>Cleaning up RESC2_script on node2-hb, removing fail-count-RESC2_script<br><br>  * The configuration prevents the cluster from stopping or starting 'RESC1_group' (unmanaged)<br>RESC1_group이 중지 혹은 시작 되지 않는다고 명시해주고 있습니다.<br><br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>Aug 25 23:59:49 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:50 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:51 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:52 NODE1 crmd[2846]:  notice: Notifications disabled<br>Aug 25 23:59:54 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm1 [ NOT RUNNING ]<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC1_script_monitor_0: ok (node=node1-hb, call=110, rc=0, cib-update=106, confirmed=true)<br>Aug 25 23:59:54 NODE1 logger: EXCUTE : Search Cluster Resource Process - ctrlm2 [ NOT RUNNING ]<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC2_script_monitor_0: ok (node=node1-hb, call=126, rc=0, cib-update=107, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation LV_APP_con_ctmag2_monitor_0: ok (node=node1-hb, call=118, rc=0, cib-update=108, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC1_vip_monitor_0: ok (node=node1-hb, call=106, rc=0, cib-update=109, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation RESC2_vip_monitor_0: ok (node=node1-hb, call=122, rc=0, cib-update=110, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation LV_APP_con_ctmag1_monitor_0: ok (node=node1-hb, call=102, rc=0, cib-update=111, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation VG_SRPCTAAP1VG_monitor_0: ok (node=node1-hb, call=98, rc=0, cib-update=112, confirmed=true)<br>Aug 25 23:59:54 NODE1 crmd[2846]:  notice: Operation VG_SRPCTAAP2VG_monitor_0: ok (node=node1-hb, call=114, rc=0, cib-update=113, confirmed=true)<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit dmraid-activation.service, ignoring: Unit dmraid-activation.service is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit alsa-restore.service, ignoring: Unit alsa-restore.service is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit avahi-daemon.socket, ignoring: Unit avahi-daemon.socket is masked.<br>Aug 26 00:00:06 NODE1 systemd: Cannot add dependency job for unit alsa-state.service, ignoring: Unit alsa-state.service is masked.<br>Aug 26 00:01:01 NODE1 CROND[39770]: (root) CMD (run-parts /etc/cron.hourly)<br>Aug 26 00:01:01 NODE1 run-parts(/etc/cron.hourly)[39770]: starting 0anacron<br>Aug 26 00:01:02 NODE1 anacron[39780]: Anacron started on 2017-08-26<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39782]: finished 0anacron<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39770]: starting 0yum-hourly.cron<br>Aug 26 00:01:02 NODE1 run-parts(/etc/cron.hourly)[39788]: finished 0yum-hourly.cron<br>Aug 26 00:01:02 NODE1 anacron[39780]: Normal exit (0 jobs run)<br><br><br>&lt;&lt;&lt;&lt;&lt; pcs status &gt;&gt;&gt;&gt;<br>Every 0.5s: pcs status                                                                                                              Sat Aug 26 00:03:52 2017<br><br>Cluster name: testhalvm<br>Last updated: Sat Aug 26 00:03:52 2017          Last change: Fri Aug 25 23:59:53 2017 by hacluster via crmd on node2-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb (unmanaged)<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br><br>========================================================================================================<br>5. 문제 리소스에 대해서 manage 상태로 변경<br><br>root@NODE1 /root # pcs resource manage LV_APP_con_ctmag1<br><br>&lt;&lt;&lt;&lt; log &gt;&gt;&gt;&gt;&gt;<br>로그 없음<br>&lt;&lt;&lt;&lt;&lt; pcs status &gt;&gt;&gt;&gt;<br>Every 0.5s: pcs status                                                                                                              Sat Aug 26 00:06:15 2017<br><br>Cluster name: testhalvm<br>Last updated: Sat Aug 26 00:06:15 2017          Last change: Sat Aug 26 00:04:29 2017 by root via cibadmin on node1-hb<br>Stack: corosync<br>Current DC: node2-hb (version 1.1.13-10.el7_2.4-44eb2dd) - partition with quorum<br>2 nodes and 11 resources configured<br><br>Online: [ node1-hb node2-hb ]<br><br>Full list of resources:<br><br> Resource Group: RESC1_group<br>     VG_SRPCTAAP1VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag1  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC1_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC1_script       (ocf::status:MySAP):    Started node1-hb<br>     VG_SRPCTAAP2VG     (ocf::heartbeat:LVM):   Started node1-hb<br>     LV_APP_con_ctmag2  (ocf::heartbeat:Filesystem):    Started node1-hb<br>     RESC2_vip  (ocf::heartbeat:IPaddr2):       Started node1-hb<br>     RESC2_script       (ocf::status:MySAP):    Started node1-hb<br> Clone Set: ping-clone [ping]<br>     Started: [ node1-hb node2-hb ]<br> kdump_stonith  (stonith:fence_kdump):  Started node2-hb<br><br>PCSD Status:<br>  node1-hb: Online<br>  node2-hb: Online<br><br>Daemon Status:<br>  corosync: active/disabled<br>  pacemaker: active/disabled<br>  pcsd: active/enabled<br>======================<br>========================================================================================================<br>6. failcount 상태 체크<br>이상없음을 확인<br><br>root@NODE1 /root # pcs resource failcount show LV_APP_con_ctmag1<br>No failcounts for LV_APP_con_ctmag1<br><br><publishedDate>2017-08-25T15:40:09Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KSOArIAP"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-25T09:06:02Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-25T09:06:02Z</b><br><br>안녕하세요,  <br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>유선상으로 말씀 드린것처럼 테스트 환경에서 한번 더 확인 부탁드립니다.<br><br>감사합니다!<br><br><publishedDate>2017-08-25T09:06:02Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KSNpIIAX"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-25T08:39:19Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-25T08:39:19Z</b><br><br>그렇게 되는것 때문에 제가 unmanage모드로 한후 해당 리소스를 정상으로 변경후 <br>해당 리소스에 대한 failcount 값을 리셋하고 manage모드로 하여 정상을 유지하고 자 하는것입니다.<br>그렇게 되면 하부 리소스들은 재시작 되지 않는것 같습니다.<br>확인 부탁드립니다.<br>급한건입니다. 일요일에 적용하여야 합니다.<br><br><publishedDate>2017-08-25T08:39:19Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KSN6LIAX"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-25T07:20:10Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-25T07:20:10Z</b><br><br>안녕하세요,  <br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>resource cleanup 명령어는 resource group / ordering constraint 가 설정된 경우 다른 resource 도 영향을 받게 됩니다.<br>다른 resource에 영향 없이 cleanup을 사용하시려면 resource group 에서 제거하고 / ordering constraint 에서도 제거되어야 합니다.<br>문의하신 방법으로 cleanup을 하게 되면 다른 resource도 재시작 될 것입니다.<br>현재 상황에서는 pcs cluster 명령어 결과에서 failed action을 제거하기는 어려울 것 같습니다.<br><br>혹시 필요하실수 있을 것 같아서 추가합니다.<br>아래는 failed action 기록 삭제와 무관하며, 다른 리소스에 영향을 주지 않는 특정 resource의 fail count를 조회하고 삭제하는 명령어 입니다.<br><br>조회 : crm_failcount -r &lt;resource id&gt;<br>삭제 : crm_failcount -r &lt;resource id&gt; -D<br><br>감사합니다.<br><br><publishedDate>2017-08-25T07:20:10Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KSDpNIAX"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-24T15:44:08Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-24T15:44:07Z</b><br><br>추가 질문이 있어 재차 문의 드립니다.<br>아래와 같은 2개의 리소스가 말씀드린데로  리소스가 umount 된다 하더라도 <br><br>1. failover가 되지 않게 하고<br>2. 정상으로 변경하기위해 하위 리소스에 영향을 받지 않게 하여 자신만 정상으로 돌리는<br>방법으로 아래와 같이 진행하여도 무방한지 여쭙고 싶습니다.<br><br>1. monitor옵션에서 on-fail=ignore를 설정<br>2. a) umount된 리소스를 마운트하여 정상으로 돌린 후<br>     b) 해당 리소스를 pcs resource manage LV_dbadmin 메니지 모드로 변경 후 <br>     c) pcs resource cleanup LV_dbadmin 으로 failcount를 리셋 시킨 후 <br>     d) pcs resource unmanage LV_dbadmin 로 복구 <br>======================<br>  Resource: LV_dbadmin (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-dbadmin directory=/dbadmin fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_dbadmin-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_dbadmin-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_dbadmin-monitor-interval-5s)<br>  Resource: LV_LOG (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-LOG directory=/LOG fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_LOG-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_LOG-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_LOG-monitor-interval-5s)<br><br><publishedDate>2017-08-24T15:44:07Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KRjkRIAT"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-23T04:49:44Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-23T06:40:04Z</b><br><br>안녕하세요,  <br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>의미적으로 본다면 monitor enabled=false 설정은 모니터링을 하지 않는 것이며,<br>on-fail=ignore는 모니터링은 하지만 resource가 실패하였을때 무시 하는 것입니다.<br> <br>일반적인 상황에서 enabled=false 설정은 on-fail=ignore 설정처럼 resource 실패시 recovery 동작을 하지 않습니다.<br>또한 enable=false 설정이 설정하지 않은 다른 resource에 영향을 주는 부분은 없습니다.<br>enable=false 설정이 되지 않은 다른 resource가 실패한 경우 failover(resource relocate)도 잘 작동 될 것입니다.<br> <br>감사합니다.<br><br><publishedDate>2017-08-23T04:49:43Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KRQCRIA5"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-22T08:52:08Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-22T08:52:08Z</b><br><br>번거롭지만 한가지 좋은 방법이 될꺼 같아 문의 드립니다<br><br>모든 리소스의 모니터링 옵션을 on-fail=restart로 하고 <br>특정 lvm리소스만 문제가 생기더라도 fail over가 되지 않게 하기 위해서 <br>op monitor 옵션을 enabled=false로 변경하여 해당 리소스는 영향을 미치지 않게 하고자 합니다.<br>변경한 리소스를 제외하고 다른 리소스에 문제가 발생할시 정상 fail-over에 영향이 없는지 확인 부탁드립니다.<br><br><br>&lt;&lt; 현재  &gt;&gt;<br>Resource: LV_dbadmin (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-dbadmin directory=/dbadmin fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_dbadmin-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_dbadmin-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_dbadmin-monitor-interval-5s)<br>&lt;&lt; 변경 &gt;&gt;<br>Resource: LV_dbadmin (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-dbadmin directory=/dbadmin fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_dbadmin-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_dbadmin-start-interval-0s)<br>               monitor monitor interval=60s enabled=false (LV_dbadmin-monitor-interval-5s)<br><br><publishedDate>2017-08-22T08:52:08Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KRNenIAH"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-22T05:00:27Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-22T05:46:13Z</b><br><br>안녕하세요,  <br> <br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br> <br>문의하신 내용에 대한 답변 드립니다.<br> <br>=============================================================================<br> <br>1.  on-fail=ignore인 경우와 on-fail=restart  인경우에 어떤 차이가 있는지 확인 부탁드립니다<br><br> - 클러스터의 resource는 start / stop / monitor 이렇게 3가지 operation이 있습니다. on-fail은 각 3가지 operation이 실패 했을 경우에 다음 동작을 지정하는 속성입니다.<br>따라서 각 operation의 속성을 어떻게 지정하는지에 따라 클러스터의 동작이 변경될 수 있습니다.<br> - on-fail=ignore : resource가 실패시 resource가 실패하지 않는 것 처럼 보이게 하며, 이 후 아무 동작을 하지 않습니다.<br>중요도가 낮은 resource 실패로 인하여, 중요도 높은 서비스가 재시작되거나 정지를 원하지 않는 경우 사용합니다.<br> - on-fail=restart : resource가 실패시 resource를 재시작합니다.<br>앞서 말씀드린 것 처럼 Ordering Constraints 설정이 된 경우 실패한 resource 이후에 실행되는 resource 모두 재시작 됩니다.<br>- 추가 설정에 관한 문서 링크입니다.<br>https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/High_Availability_Add-On_Reference/index.html#s1-resourceoperate-HAAR<br> <br>2. 말씀해주신 Ordering Constraints 설정을 해제할 경우 resource 에  영향은 말씀해주신 부분만 바뀌고 다른 이슈될만한 것은 없는지 확인 부탁드립니다.<br><br> - Ordering Constraints 정의된 순서대로 resource 가 시작됩니다. 운영적인 측면에서 시작과 정지할때 의존성이 있는 resource 가 있는 경우에 설정하시면 됩니다.<br>일반적으로 파일시스템 -&gt; Virtual IP address -&gt; Service 순서로 시작하는것이 예가 될 수 있습니다.<br> - Ordering Constraints 설정을 하지 않게 되면 resource 를 생성한 순서대로  시작 됩니다.<br> <br>3. on-fail=ignore 설정을 유지하면서 해당 설정으로 유지했을 때의 상태 및 유지할 경우 문제가 될 수있는 상황이 생길 수 있는지 확인 부탁드립니다.<br><br> - 중요한 resource 가 fail이 되었을때 아무 동작도 하지 않게 됩니다. 이 경우 관리자가 수동적으로 대처하게 되면 서비스 지연 시간이 길어질 수 있습니다.<br>서비스에 영향이 있는 resource는 ignore 설정을 권장하지 않습니다.<br>- 관련하여 아래의 링크에서 추가 정보를 얻으실 수 있습니다.<br>https://access.redhat.com/solutions/1296573<br><br>=============================================================================<br> <br>감사합니다.<br><br><publishedDate>2017-08-22T05:00:27Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KRMUBIA5"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-22T01:39:15Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-22T01:39:15Z</b><br><br>dropbox에 올렸다는 내용은 제가 다른 케이스에 올린 내용인데 이쪽에 커멘트되었습니다<br><br>해당 내용은 신경 안쓰셔도 됩니다 추가 문의사항만 부탁드립니다<br><br>감사합니다<br><br><publishedDate>2017-08-22T01:39:15Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KRAvGIAX"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-21T10:02:50Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-21T10:02:50Z</b><br><br>추가적으로 net.ipv4.ip_local_port_range 설정 값이 서로 다른 서버들이 있습니다<br>설정은 동일하지만 값에 차이가 있는데요 SAP이 설정된 서버와 설정되지 않은 non-SAP이 차이가 있는데요<br>검토 부탁드립니다 <br>설정 관련 아래 서버 sosreport를 dropbox에 업로드 하였습니다.<br> <br> SAP 설정된 서버       - sosreport-PCRMAL01SL-20170821183354.tar.xz<br> non-SAP default 서버 - sosreport-PDBEAL01SL-20170821183354.tar.xz<br><br>감사합니다<br><br><publishedDate>2017-08-21T10:02:50Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000IscTPIAZ"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-21T05:32:34Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-21T05:32:34Z</b><br><br>답변 감사합니다<br>몇가지 추가 문의사항이 있습니다<br><br>1.  on-fail=ignore인 경우와 on-fail=restart  인경우에 어떤 차이가 있는지 확인 부탁드립니다<br><br>2. 말씀해주신 Ordering Constraints 설정을 해제할 경우 리소스에  영향은 말씀해주신 부분만 바뀌고 다른 이슈될만한 것은 없는지<br>확인 부탁드립니다<br> <br>3. on-fail=ignore 설정을 유지하면서 해당 설정으로 유지했을 때의 상태 및 유지할 경우 문제가 될 수있는 상황이 생길 수 있는지 확인 부탁드립니다<br><br><publishedDate>2017-08-21T05:32:34Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000KNJiTIAX"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-18T02:29:05Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-18T02:29:05Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service를 이용해 주셔서 감사합니다.<br><br>문의주신 내용중에 3번에 대한 답변 드립니다.<br>일반적으로 database start 또는 stop시에 클러스터 리소스로 등록하여 사용하시는 것이 안정적입니다.<br>혹시 oracle database를 사용하신다면, 아래 링크를 참조 하시면 도움되실 것 같습니다.<br><br>https://access.redhat.com/solutions/25970<br><br>감사합니다.<br><br><publishedDate>2017-08-18T02:29:05Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KN6tIIAT"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-17T08:15:58Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-17T08:20:47Z</b><br><br>안녕하세요,   <br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>문의하신 내용에 대한 답변 드립니다.<br><br>1. 이 처럼 해당 리소스 이후에 리소스들이 재시작되는것이 정상적인 경우인가요?<br><br>-&gt; 기본적으로 클러스터는 리소스의 fail이 발생하면 문제가 발생한 리소스만 restart를 시도합니다.<br><br>아래는 현재 클러스터에 설정된 Ordering Constraints 리소스 목록입니다.<br>시작 될때는 1번에서 시작하여 12번까지의 순서로 진행됩니다.<br>정지 될때는 12번에서 시작하여 1번까지의 역순으로 진행됩니다.<br>각 리소스는 자신보다 상위에 위치한 리소스가 정상적으로 시작된 이후에 시작이 되도록 되어 있습니다.<br><br>1. VG_SRPIBAARCVG<br>2. VG_SRPIBADATVG<br>3. VG_SRPIBAORAVG<br>4. LV_oraarch_PIBA<br>5. LV_oradata_PIBA<br>6. LV_CTM<br>7. LV_orasys_PIBA<br>8. LV_oracle<br>9. LV_dbadmin<br>10. LV_LOG<br>11. PIBA_VIP<br>12. PIBA_script<br><br>failover Test 시에 4번 항목인 LV_oraarch_PIBA가 unmount 된 후 다시 마운트 되는 것이 기본적인 예상 동작입니다.<br>하지만 Ordering constraints가 설정이 되어 있으므로, 4 번 이후의 리소스들은 4번을 다시 마운트 하기 전에 <br>12부터 4번까지 역순으로 모든 리소스를 stop 시킨 이후에 다시 4번 부터 12번을 순서대로 리소스를 다시 실행 하게 됩니다.<br><br>SLPIBADL02 의 /var/log/messages 내용에서 아래와 같은 부분을 확인할 수 있습니다.<br><br>8 / 12 17:01:05 Unmounting Filesystem (LV_oraarch_PIBA)이 발생 : PIBA_script부터 LV-oraarch_PIBA까지 차례로 stop을 진행합니다.<br>8 / 12 17:02:20 notice: Initiating action 29: start LV_oraarch_PIBA 발생 : LV_oraarch_PIBA부터 PIBA_sciprt까지 순서대로 start를 실행하는 내용을 보실 수 있습니다.<br><br>2. 만약 그렇다면 해당 리소스만을 umount를 실행하여도 해당 기능이 동작하지 않도록 구성하는 방법이 무엇이 있을지 확인 부탁드립니다.<br>참고적으로 현제는 on-fail=ignore로 바꿔놓은 상태입니다.<br><br>-&gt; 1번에서 말씀드린것 처럼 Ordering Constraints 설정으로 인하여 다른 리소스까지 영향을 받게 되었습니다. Ordering Constraints를 해제하시면 문제의 리소스만 재시작 될 것입니다.<br><br>3. 해당 서버는 DB서버로 디스크에 대한 부분은 pacemaker에 등록되어 실행시 디스크 환경이 만들어지면 수동으로 서비스(DB 등)올려주는 과정을 실행하고 있습니다 .<br>데이터의 안정성등을 고려하여 최대한 클러스터 종료시 안정적으로 서비스 내리는 과정을 추가하고 싶은데요. 최대한 정상적이고 안정적으로 내릴수 있는 방법이 있는지 검토 부탁드립니다.<br><br>-&gt; 해당 부분은 추가확인중에 있습니다.<br><br>감사합니다.<br><br><publishedDate>2017-08-17T08:15:58Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000KAEjmIAH"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-17T02:05:20Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-17T02:05:20Z</b><br><br>요청주신 시간별 명령어 실행 내용입니다 감사합니다<br><br>root@SLPIBADL02 /root # history | grep -i umount<br> <br>  846  2017-08-12 [17:01:1502524865]umount /oraarch_PIBA ← cluster service restart됨<br><br>  892  2017-08-12 [17:35:1502526939]date;umount /dbadmin ← option ignore로 변경 후 umount<br> <br>888  2017-08-12 [17:34:1502526876]pcs resource update LV_dbadmin Filesystem device=&quot;/dev/mapper/SRPIBAORAVG-dbadmin&quot; directory=&quot;/dbadmin&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10 ← dbadmin만 변경 후 test<br> <br>아래는 dbadmin 테스트 후 나머지 반영 내역<br>  914  2017-08-12 [17:45:1502527500]pcs resource update LV_oraarch_PIBA Filesystem device=&quot;/dev/mapper/SRPIBAARCVG-oraarch_PIBA&quot; directory=&quot;/oraarch_PIBA&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br>  915  2017-08-12 [17:45:1502527510]pcs resource update LV_oradata_PIBA Filesystem device=&quot;/dev/mapper/SRPIBADATVG-oradata_PIBA&quot; directory=&quot;/oradata_PIBA&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br>  916  2017-08-12 [17:45:1502527530]pcs resource update LV_CTM Filesystem device=&quot;/dev/mapper/SRPIBAORAVG-CTM&quot; directory=&quot;/CTM&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br>  917  2017-08-12 [17:45:1502527539]pcs resource update LV_orasys_PIBA Filesystem device=&quot;/dev/mapper/SRPIBAORAVG-orasys_PIBA&quot; directory=&quot;/orasys_PIBA&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br>  918  2017-08-12 [17:45:1502527545]pcs resource update LV_oracle Filesystem device=&quot;/dev/mapper/SRPIBAORAVG-oracle&quot; directory=&quot;/oracle&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br>  919  2017-08-12 [17:45:1502527551]pcs resource update LV_LOG Filesystem device=&quot;/dev/mapper/SRPIBAORAVG-LOG&quot; directory=&quot;/LOG&quot; fstype=&quot;xfs&quot; run_fsck=&quot;no&quot; op start start-delay=5s monitor on-fail=ignore interval=5s timeout=30 OCF_CHECK_LEVEL=10<br> - SLPIBADL02 fc path 절체 내역<br> host4(fc host) path 절체<br>Aug 12 17:52:01 SLPIBADL02 CROND[106378]: (root) CMD (/usr/lib64/sa/sa1 1 1)<br>Aug 12 17:52:32 SLPIBADL02 kernel: qla2xxx [0000:84:00.1]-500b:4: LOOP DOWN detected (2 7 0 0).<br>Aug 12 17:52:37 SLPIBADL02 kernel: sd 4:0:1:0: rejecting I/O to offline device<br> host1(fc host) path 절체<br>Aug 12 17:53:01 SLPIBADL02 CROND[111729]: (root) CMD (/usr/lib64/sa/sa1 1 1)<br>Aug 12 17:53:02 SLPIBADL02 kernel: qla2xxx [0000:0b:00.0]-500b:1: LOOP DOWN detected (2 7 0 0).<br>Aug 12 17:53:07 SLPIBADL02 multipathd: 8:80: mark as failed<br><br><publishedDate>2017-08-17T02:05:20Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000K9uYrIAJ"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-16T08:16:46Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-16T08:16:46Z</b><br><br>고객이  기술 문의 01910534로 올리신 내용인데요 문의  내용입니다  이슈 사항 이해하시는데 도움이 되실까 해서 다시 올려드립니다<br>감사합니다<br><br><br>공유 파일시스템을 resource로 가지고 RHCS(Active-Standby) 구성되어있는 서버들에서 resource에 등록된 공유 파일시스템 umount 시에 해당 resource만 다시 mount 시도가 가능한지 문의드리려고 합니다.<br>안녕하세요<br><br>SDS 김지민 선임입니다.<br><br>공유 파일시스템을 resource로 가지고 RHCS(Active-Standby) 구성되어있는 서버들에서 resource에 등록된 공유 파일시스템 umount 시에 해당 resource만 다시 mount 시도가 가능한지 문의드리려고 합니다.<br><br>해당 서버의 파일시스템 및 cluster config 정보는 아래와 같고<br>예를 들면 /dbadmin이나 /LOG 이런 파일시스템이 cluster resource의 config에서 on-fail에 대한 값을 restart로 하였을 때 umount가 되면 해당 노드에서 cluster가 restart가 되어버렸는데요.<br><br>/dbadmin이나 /LOG와 같은 부분은 일단 서비스에 직접적인 영향이 없어서 일단 아무 동작을 하지 않는 ignore로 변경하였습니다.<br><br>문의드릴 점은 공유 파일시스템 umount 시에 해당 resource만 다시 mount 시도를 수행할 수 있도록 하는  값이 있는지 문의드리려고 합니다.<br><br>root@SLPIBADL01 / # df -Ph<br>Filesystem                            Size  Used Avail Use% Mounted on<br>/dev/sda2                              30G  6.1G   24G  21% /<br>devtmpfs                               63G     0   63G   0% /dev<br>tmpfs                                  63G   54M   63G   1% /dev/shm<br>tmpfs                                  63G   18M   63G   1% /run<br>tmpfs                                  63G     0   63G   0% /sys/fs/cgroup<br>/dev/sda3                              20G  1.9G   19G  10% /var<br>/dev/sda1                            1016M  257M  759M  26% /boot<br>/dev/mapper/vg9-CRASH                 171G   33M  171G   1% /CRASH<br>/dev/mapper/vg0-home                   10G  150M  9.9G   2% /home<br>/dev/mapper/vg0-sysadmin               20G  9.7G   11G  49% /sysadmin<br>tmpfs                                  13G     0   13G   0% /run/user/0<br>tmpfs                                  13G     0   13G   0% /run/user/32762<br>/dev/mapper/SRPIBAARCVG-oraarch_PIBA  100G  1.5G   99G   2% /oraarch_PIBA<br>/dev/mapper/SRPIBADATVG-oradata_PIBA  300G   23G  278G   8% /oradata_PIBA<br>/dev/mapper/SRPIBAORAVG-CTM           100G   27G   74G  27% /CTM<br>/dev/mapper/SRPIBAORAVG-orasys_PIBA    20G  6.3G   14G  32% /orasys_PIBA<br>/dev/mapper/SRPIBAORAVG-oracle         30G  6.9G   24G  23% /oracle<br>/dev/mapper/SRPIBAORAVG-dbadmin        50G   12G   39G  24% /dbadmin<br>/dev/mapper/SRPIBAORAVG-LOG            20G  132M   20G   1% /LOG<br>tmpfs                                  13G     0   13G   0% /run/user/1021<br><br> - 공유 volume resource<br>/dev/mapper/SRPIBAARCVG-oraarch_PIBA  100G  1.5G   99G   2% /oraarch_PIBA<br>/dev/mapper/SRPIBADATVG-oradata_PIBA  300G   23G  278G   8% /oradata_PIBA<br>/dev/mapper/SRPIBAORAVG-CTM           100G   27G   74G  27% /CTM<br>/dev/mapper/SRPIBAORAVG-orasys_PIBA    20G  6.3G   14G  32% /orasys_PIBA<br>/dev/mapper/SRPIBAORAVG-oracle         30G  6.9G   24G  23% /oracle<br>/dev/mapper/SRPIBAORAVG-dbadmin        50G   12G   39G  24% /dbadmin<br>/dev/mapper/SRPIBAORAVG-LOG            20G  132M   20G   1% /LOG<br><br> - pcs config<br>root@SLPIBADL01 / # pcs config<br>Cluster Name: IBA_DB_cluster<br>Corosync Nodes:<br> SLPIBADL01-HB SLPIBADL02-HB<br>Pacemaker Nodes:<br> SLPIBADL01-HB SLPIBADL02-HB<br><br>Resources:<br> Group: PIBA_group<br>  Resource: VG_SRPIBAARCVG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPIBAARCVG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPIBAARCVG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPIBAARCVG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPIBAARCVG-start-interval-0s)<br>  Resource: VG_SRPIBADATVG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPIBADATVG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPIBADATVG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPIBADATVG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPIBADATVG-start-interval-0s)<br>  Resource: VG_SRPIBAORAVG (class=ocf provider=heartbeat type=LVM)<br>   Attributes: volgrpname=SRPIBAORAVG exclusive=true<br>   Operations: stop interval=0s timeout=30 (VG_SRPIBAORAVG-stop-interval-0s)<br>               monitor interval=10 timeout=30 (VG_SRPIBAORAVG-monitor-interval-10)<br>               start interval=0s start-delay=5s (VG_SRPIBAORAVG-start-interval-0s)<br>  Resource: LV_oraarch_PIBA (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAARCVG-oraarch_PIBA directory=/oraarch_PIBA fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_oraarch_PIBA-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_oraarch_PIBA-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_oraarch_PIBA-monitor-interval-5s)<br>  Resource: LV_oradata_PIBA (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBADATVG-oradata_PIBA directory=/oradata_PIBA fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_oradata_PIBA-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_oradata_PIBA-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_oradata_PIBA-monitor-interval-5s)<br>  Resource: LV_CTM (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-CTM directory=/CTM fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_CTM-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_CTM-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_CTM-monitor-interval-5s)<br>  Resource: LV_orasys_PIBA (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-orasys_PIBA directory=/orasys_PIBA fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_orasys_PIBA-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_orasys_PIBA-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_orasys_PIBA-monitor-interval-5s)<br>  Resource: LV_oracle (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-oracle directory=/oracle fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_oracle-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_oracle-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_oracle-monitor-interval-5s)<br>  Resource: LV_dbadmin (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-dbadmin directory=/dbadmin fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_dbadmin-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_dbadmin-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_dbadmin-monitor-interval-5s)<br>  Resource: LV_LOG (class=ocf provider=heartbeat type=Filesystem)<br>   Attributes: device=/dev/mapper/SRPIBAORAVG-LOG directory=/LOG fstype=xfs run_fsck=no<br>   Operations: stop interval=0s timeout=60 (LV_LOG-stop-interval-0s)<br>               start interval=0s start-delay=5s (LV_LOG-start-interval-0s)<br>               monitor interval=5s timeout=30 on-fail=ignore OCF_CHECK_LEVEL=10 (LV_LOG-monitor-interval-5s)<br>  Resource: PIBA_VIP (class=ocf provider=heartbeat type=IPaddr2)<br>   Attributes: ip=100.254.145.178<br>   Operations: stop interval=0s timeout=20s (PIBA_VIP-stop-interval-0s)<br>               start interval=0s start-delay=3s (PIBA_VIP-start-interval-0s)<br>               monitor interval=5s start-delay=5s timeout=10s (PIBA_VIP-monitor-interval-5s)<br>  Resource: PIBA_script (class=ocf provider=status type=MySAP)<br>   Attributes: monitor=/etc/corosync/oracle_monitor start=/etc/corosync/oracle_start stop=/etc/corosync/oracle_stop<br>   Operations: stop interval=0s timeout=60 (PIBA_script-stop-interval-0s)<br>               monitor interval=60 timeout=60 (PIBA_script-monitor-interval-60)<br>               start interval=0s start-delay=3s (PIBA_script-start-interval-0s)<br> Clone: ping-clone<br>  Resource: ping (class=ocf provider=pacemaker type=ping)<br>   Attributes: dampen=5s multiplier=10 attempts=7 host_list=100.254.145.1<br>   Operations: start interval=0s timeout=60 (ping-start-interval-0s)<br>               stop interval=0s timeout=20 (ping-stop-interval-0s)<br>               monitor interval=5s (ping-monitor-interval-5s)<br><br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;SLPIBADL01-HB SLPIBADL02-HB&quot; pcmk_off_timeout=120s timeout=120s<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_list=SLPIBADL01-HB ipaddr=172.18.24.105 login=Redhat passwd=nl!con00 lanplus=on auth=password delay=15 action=reboot<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_list=SLPIBADL02-HB ipaddr=172.18.24.106 login=Redhat passwd=nl!con00 lanplus=on auth=password delay=15 action=reboot<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br><br> Node: SLPIBADL01-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: SLPIBADL02-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>Location Constraints:<br>  Resource: LV_CTM<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_CTM-SLPIBADL01-HB-10)<br>  Resource: LV_LOG<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_LOG-SLPIBADL01-HB-10)<br>  Resource: LV_dbadmin<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_dbadmin-SLPIBADL01-HB-10)<br>  Resource: LV_oraarch_PIBA<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_oraarch_PIBA-SLPIBADL01-HB-10)<br>  Resource: LV_oracle<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_oracle-SLPIBADL01-HB-10)<br>  Resource: LV_oradata_PIBA<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_oradata_PIBA-SLPIBADL01-HB-10)<br>  Resource: LV_orasys_PIBA<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-LV_orasys_PIBA-SLPIBADL01-HB-10)<br>  Resource: PIBA_VIP<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-PIBA_VIP-SLPIBADL01-HB-10)<br>    Constraint: location-PIBA_VIP<br>      Rule: score=-INFINITY boolean-op=or  (id:location-PIBA_VIP-rule)<br>        Expression: pingd lt 1  (id:location-PIBA_VIP-rule-expr)<br>        Expression: not_defined pingd  (id:location-PIBA_VIP-rule-expr-1)<br>  Resource: PIBA_group<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-PIBA_group-SLPIBADL01-HB-10)<br>  Resource: PIBA_script<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-PIBA_script-SLPIBADL01-HB-10)<br>  Resource: VG_SRPIBAARCVG<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-VG_SRPIBAARCVG-SLPIBADL01-HB-10)<br>  Resource: VG_SRPIBADATVG<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-VG_SRPIBADATVG-SLPIBADL01-HB-10)<br>  Resource: VG_SRPIBAORAVG<br>    Enabled on: SLPIBADL01-HB (score:10) (id:location-VG_SRPIBAORAVG-SLPIBADL01-HB-10)<br>  Resource: ipmilan_stonith1<br>    Disabled on: SLPIBADL01-HB (score:-INFINITY) (id:const_ipmilan1)<br>  Resource: ipmilan_stonith2<br>    Disabled on: SLPIBADL02-HB (score:-INFINITY) (id:const_ipmilan2)<br>Ordering Constraints:<br>  start VG_SRPIBAARCVG then start VG_SRPIBADATVG (kind:Mandatory) (id:order-VG_SRPIBAARCVG-VG_SRPIBADATVG-mandatory)<br>  start VG_SRPIBADATVG then start VG_SRPIBAORAVG (kind:Mandatory) (id:order-VG_SRPIBADATVG-VG_SRPIBAORAVG-mandatory)<br>  start VG_SRPIBAORAVG then start LV_oraarch_PIBA (kind:Mandatory) (id:order-VG_SRPIBAORAVG-LV_oraarch_PIBA-mandatory)<br>  start LV_oraarch_PIBA then start LV_oradata_PIBA (kind:Mandatory) (id:order-LV_oraarch_PIBA-LV_oradata_PIBA-mandatory)<br>  start LV_oradata_PIBA then start LV_CTM (kind:Mandatory) (id:order-LV_oradata_PIBA-LV_CTM-mandatory)<br>  start LV_CTM then start LV_orasys_PIBA (kind:Mandatory) (id:order-LV_CTM-LV_orasys_PIBA-mandatory)<br>  start LV_orasys_PIBA then start LV_oracle (kind:Mandatory) (id:order-LV_orasys_PIBA-LV_oracle-mandatory)<br>  start LV_oracle then start LV_dbadmin (kind:Mandatory) (id:order-LV_oracle-LV_dbadmin-mandatory)<br>  start LV_dbadmin then start LV_LOG (kind:Mandatory) (id:order-LV_dbadmin-LV_LOG-mandatory)<br>  start LV_LOG then start PIBA_VIP (kind:Mandatory) (id:order-LV_LOG-PIBA_VIP-mandatory)<br>  start PIBA_VIP then start PIBA_script (kind:Mandatory) (id:order-PIBA_VIP-PIBA_script-mandatory)<br>Colocation Constraints:<br><br>Resources Defaults:<br> resource-stickiness: 100<br> migration-threshold: 3<br>Operations Defaults:<br> No defaults set<br><br>Cluster Properties:<br> cluster-infrastructure: corosync<br> cluster-name: IBA_DB_cluster<br> dc-version: 1.1.13-10.el7_2.4-44eb2dd<br> have-watchdog: false<br> last-lrm-refresh: 1500804995<br> no-quorum-policy: stop<br> stonith-enabled: true<br><br><publishedDate>2017-08-16T08:16:46Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000K9uQRIAZ"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2017-08-16T08:01:25Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2017-08-16T08:01:25Z</b><br><br>제가 혼선을 드렸습니다 12일(토요일) 입니다<br><br>전화는 어느때라도 가능합니다<br><br><publishedDate>2017-08-16T08:01:25Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000K9tb0IAB"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-16T06:39:22Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-16T06:39:22Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>보내주신 sosreport 내용에 8/14일의 로그가 없는 것으로 보아 이전에 sosreport를 생성하신것으로 예상됩니다.<br>정확한 상황 확인을 위해서 연락처 / 통화가능 시간을 적어주시면 확인 후 연락드리겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-08-16T06:39:22Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000K9sWbIAJ"><br>======================<br><b>생성계정 : Park, HyunYick</b><br><b>생성날짜 : 2017-08-16T04:15:54Z</b><br><b>마지막 답변자 : Park, HyunYick</b><br><b>마지막 수정 일자 : 2017-08-16T04:15:54Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>박현익 입니다.<br>현재 올려주신 케이스의 내용을 확인하고 있습니다.<br>정리되는대로 업데이트 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2017-08-16T04:15:54Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>