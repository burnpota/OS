======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2018-08-01T02:10:24Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2018-09-06T10:10:30Z</b><br><b>id : 500A000000bVSEeIAO</b><br>======================<br><br><b><font size=15>
제목  : scsi fence 사용중 FC장애시 페일오버 관련
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요.<br>동일한 건이지만 다른 호스트라 새롭게 케이스 오픈합니다.<br><br>현재 RHCS6 Cman 으로 scsi펜싱, 쿼럼에 휴리스틱을 사용중인 환경입니다.<br><br>몇가지 시나리오를 가정해 페일오버 테스트 중에 있습니다.<br>한쪽노드의 모든FC가 제거 되었을때 서비스가 정상적이지 않습니다.<br><br>1호기(FLPIDMM01)에서<br>대략 <br>Jul 29 14:36:23 FLPIDMM01 kernel: rport-4:0-0: blocked FC remote port time out: removing target and saving binding // FC제거 <br>Jul 29 14:36:23 FLPIDMM01 kernel: sd 4:0:0:0: rejecting I/O to offline device<br>Jul 29 14:36:23 FLPIDMM01 kernel: sd 4:0:0:0: rejecting I/O to offline device<br>// IO offline발생<br>..생략<br><br>2호기(FLPIDMM02)<br>Jul 29 14:38:09 FLPIDMM02 corosync[21678]:   [TOTEM ] A processor failed, forming new configuration. //약 2분뒤 cman fail감지 <br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [QUORUM] Members[1]: 2<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [TOTEM ] A processor joined or left the membership and a new membership was formed.  // 재구성시작<br>Jul 29 14:38:11 FLPIDMM02 rgmanager[29125]: State change: FLPIDMM01.CS DOWN<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [CPG   ] chosen downlist: sender r(0) ip(192.168.1.52) r(1) ip(100.254.180.52) ; members(old:2 left:1)<br>Jul 29 14:38:11 FLPIDMM02 kernel: dlm: closing connection to node 1<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [MAIN  ] Completed service synchronization, ready to provide service.<br>Jul 29 14:38:11 FLPIDMM02 fenced[22477]: fencing node FLPIDMM01.CS // 장애노드 key값제거<br>Jul 29 14:38:43 FLPIDMM02 fenced[22477]: fence FLPIDMM01.CS success // 성공시그널 응답<br>... 생략<br>Jul 29 14:38:43 FLPIDMM02 rgmanager[9014]: [fs] mounting /dev/dm-39 on /DATA<br>Jul 29 14:38:43 FLPIDMM02 rgmanager[9036]: [fs] mount -t ext4 -o rw,nobarrier /dev/dm-39 /DATA<br>Jul 29 14:38:43 FLPIDMM02 kernel: EXT4-fs (dm-39): barriers disabled<br>Jul 29 14:38:43 FLPIDMM02 kernel: EXT4-fs (dm-39): warning: maximal mount count reached, running e2fsck is recommended<br>Jul 29 14:38:43 FLPIDMM02 kernel: EXT4-fs (dm-39): recovery complete<br><br><br>정리하면 이렇습니다.<br>Jul 29 14:36:23 정도에 1호기의 FC케이블을 모두 제거 하였습니다.<br>그후 IO에러를 발생하였고<br>2호기에서  약 2분후에 cman down을 감지 재구성 후 리소스 페일오버를 진행하였습니다.<br><br>정상적인 진행절차로 보이지만 실질적으로 1호기의 서비스중 fs, ip 는 정상중지가 되었으나 스크립트가 정상적으로 중지가 되지 않았고 중지가 되지 않은 상태에서 1호기의 cman통신이 되지 않아 2호기에서 중복해서 서비스가 실행되어, 결론적으로 정상적인 서비스 상태가 아니였습니다.<br><br>그리고 1호기에 현재 와치독 서비스를 사용중인데 2호기에서 펜싱 시그널을 날린 후 성공 시그널을 받았음에도 불구하고 1호기는 펜싱이 되지 않았습니다.<br><br>예상은 LUN에 등록된 13, 14 키는 FC를 뽑는 순간 1호기에서 모두 사라져 버리는것을 확인 하였습니다. 결국 1호기는 현재 어느 키가 제거되었는지 판단이 불가능하여 펜싱이 안되는 것으로 보입니다.<br>절체된 FC케이블을 다시 꽃는 순간 1호기는 펜싱이 되어버리는 것으로 보아 가능성이 있어 보입니다.<br><br>만약 위의 경우가 맞다면, <br>다른 펜싱 방법을 고려해 봐야 할것으로 보이는데요. 차선대안이 있을까요?<br>개인적인 생각으로는 messages에 IO관련 메세지를 카운트해서 10개의 카운트가 누적시 트리거에 echo b를 날리는 스크립트를 service에 등록해볼까도 생각중입니다.<br><br>만약 위의 경우가 아니라면 <br>왜 펜싱이 안된것인가요?<br><br>현재까지 위의 현상을 해결하기 위해서 진행사항입니다.<br>- fence_agent 최신버전으로 업데이트<br>- multipath.conf에 devices 설정변경<br><br>이상입니다.<br><br>추가적으로 현재 fs에 대해서 halvm이 구성이 안되어 있습니다. 만약 halvm구성된 상태일 경우 동일한 테스트시 1호기에서 볼륨이 inactive가 되지 않아서 2호기로 페일오버 조차 실패를 했습니다.<br><br>추가 궁금하신 점 있으시면 연락부탁드립니다.<br>위와 비슷한 껀에 대해서 다른 케이스가 있으나 다른 호스트이기에 새롭게 케이스 오픈드리니,<br>업무에 참고 부탁드립니다.<br><br>송기호/01036209917<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.4</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>FLPIDMM01 / FLPIDMM02</hostname><br><br><br><comment id="a0aA000000LeTmLIAV"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-21T00:41:58Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-21T00:41:58Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>volume_list 가 각 노드별로 설정되어야 한다는 명기가 생략된 것 같아 보입니다만,<br>그외에 설정 자체에 특별히 오류는 없어 보입니다.<br>적용 및 동작 검증하신 후, 이슈나 의문점은 언제든지 업데이트 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-21T00:41:58Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000LeDHmIAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-20T02:28:49Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-20T02:28:49Z</b><br><br>안녕하세요. 송기호입니다. <br>금일 주말에 아래와 같은 작업을 진행할예정입니다.<br>설정파일에 대한 전문을 보내드리겠습니다.<br><br>보시고 추가 해야하거나 변동해야할 부분이 있다면 회신부탁드립니다.<br>지속적인 지원감사드립니다.<br><br>이번작업으로 요구상항에 대해 해소되기를...<br><br><publishedDate>2018-08-20T02:28:49Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000N10eRIAR"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-13T01:30:32Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-13T01:30:55Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>현재 까지 올려주신 내용에 대하여 정리하여 안내드립니다.<br><br>1. 스카시펜싱으로 RHCS가 구성되어있습니다. 스카시펜싱 어떠한 상황에서  동작을 하는지 확인부탁드립니다.<br>===&gt;<br>power fencing, scsi fencing 모두 클러스터 그룹안의 특정 노드에 대한 통신부분(heartbeat) 또는 해당 노드가 크래쉬 되는 등의 문제가 있어 상대 노드의 상태를 알 수 없을 경우,<br>확실하게 상대를 fencing 함으로써 클러스터로부터 배제하여 유저 데이터를 보호하기 위한 기능입니다.<br>쿼럼 디스크 이빅션이 발생하였을 경우도, 결국 문제가 있는 노드의 cman 서비스를 죽여 양 노드간에 통신을 단절시켜서 fencing 을 동작하게 합니다.<br>즉, 정리하면 일반적으로 fencing 이 발생하는 상황은 통신 단절(heartbeat), 시스템 패닉 등 상대와 정상적으로 통신이 안되어 상대의 상태를 알 수 없는 상황에 발생합니다.<br>power fencing, scsi fencing 은 이후 fencing 처리 방식의 차이입니다.<br><br>2. 스카시펜싱의 경우 LUN에 해당 키값이 저장되는데 LUN자체에 장애가 발생할 경우 페일오버 혹은 펜싱의 기능이 불가능한 건지 답변부탁드립니다.<br>===&gt;<br>우선 공유디스크이자 클러스터 리소스로 등록된 LUN 자체에 장애가 발생하면<br>일반적으로 service recovery policy 에 따라서 서비스 복구를 위해 동일 노드에서 restart 또는 다른 노드로 relocation(failover) 을 시도하게 됩니다만<br>리소스가 정상적으로 stop, start 가 되지 않아 실패할 것입니다.<br>만약 상기 처리에 실패하고 리소스에 self_fence 옵션이 활성화되어 있다면 self fencing(reboot) 을 시도할 것입니다.<br>self fencing 이 성공하면 다른 노드상에 서비스 failover가 이루어질 것입니다. <br><br>3. FC케이블이 단절되었을 경우 페일오버가 혹은 펜싱기능이 불가능한 건지 답변부탁드립니다.<br>===&gt;<br>FC 케이블이 단절되어도 서비스 failover 자체는 될 수 있습니다.<br>다만, 이슈가 발생한 노드로부터 서비스 모니터링 실패 이후, 정상적으로 서비스가 종료되든지<br>어떠한 형태로든 fencing 이 성공해야 정상적으로 failover 처리가 진행될 것입니다.<br>여러가지로 조사해 보았지만 scsi fencing 만으로는 구조상 어렵습니다.<br>따라서, 이 부분은 리소스 또는 쿼럼 디스크의 옵션을 통해 self fencing 하는 대안을 찾는 것이 도움이 되실 것으로 보입니다.(6, 7번 안내 참조)<br><br>4. 스카시펜싱의 경우 각각노드에서 각각의 키값의 오너쉽을 가지고 있다가 해당 키값이 제거되면 오너쉽을 변경되어 페일오버 되는 것으로 알고 있는데요.<br>이와같은 매카니즘이 맞는지 확인부탁드립니다.<br>===&gt;<br>scsi fencing 및 ownership 변경에 대한 개념은 제가 파악하고 있는 바와도 동일합니다. 다만, service failover 관점까지 포함하면 2, 3 번의 안내를 참고하여 주십시요.<br><br>5. rgmanager의 프리징 설정을 한후 파일시스템을 unmount 하였을때 클러스터가 페일오버 혹은 펜싱의 동작을 하는지 답변부탁드립니다.<br>===&gt;<br><br>서비스를 freezing 하게 되면 서비스에 대한 상태 체크를 하지 않으므로 unmount 하셔도 failover 가 일어나지 않습니다.<br>====<br>       -Z &lt;service&gt;<br>              Freezes  the  service  named  service on the cluster member on which it is cur-<br>              rently running.  This will prevent status checks of  the  service  as  well  as<br>              failover in the event the node fails or rgmanager is stopped.<br><br>       -U &lt;service&gt;<br>              Unfreezes  the  user service named service on the cluster member on which it is<br>              currently running.  This will re-enable status checks.<br>====<br><br>6. 현재 구성상에서 FC케이블의 단절시에 정상적인  서비스를 유지하기 위해서 권고및 해결 방안을되는 방안을 유사사례에 근거하여  알려주시기 바랍니다.<br>===&gt;<br>우선 LVM 디바이스 상에 작성된 fs 리소스를 등록해서 사용하실 경우, HA-LVM 구성을 반드시 하셔야 합니다.<br>만약, 구성하지 않으실 경우는 LVM 디바이스에 대해서 클러스터 구성노드들에 의해 동시 접근이 가능하기 때문에 정책상 기술 지원대상이 되지 않습니다.<br>따라서, 반드시 HA-LVM 로 구성을 하시고, 해당 리소스에 대해서 self_fence 를 활성화시켜서 FC 케이블 단절시 self fencing(reboot) 을 통한<br>서비스 failover 를 시도해보시는게 좋을 것 같습니다.<br><br>Cluster service containing lvm resource fails instead of relocating when volume cannot be deactivated in RHEL <br>  https://access.redhat.com/solutions/139043<br><br>7. 쿼럼 디스크의 io_timeout=&quot;1&quot; 옵션 사용 가능 여부<br>===&gt;<br>해당 옵션을 사용하시는 것에 대해 특별한 주의점은 확인되지 않습니다.<br>쿼럼 디스크와 관련하여 문제가 있어 tko*interval 보다 갱신 시간이 길어지면 self fencing(reboot) 이 발생하는 옵션입니다.<br><br>Why do I see the warning messages &quot;qdisk cycle took more than X seconds to complete&quot; in a RHEL cluster with a quorum device? <br>  https://access.redhat.com/solutions/19495<br><br>마지막으로 클러스터에서 지원하는 여러 옵션이 있습니다만,<br>고객 환경에 적합한 환경을 구축하기 위해 테스트를 통해 최적값을 도출할 필요가 있습니다.<br>이 부분은 실제 컨설팅 및 구축에 해당해서 다소 시간이 걸리고 있는 점 양해부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-13T01:30:31Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NPnfZIAT"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-10T08:15:28Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-10T08:15:27Z</b><br><br>-------------------2페이지<br>그런데 제가 로그를 보니까 rgmanager에서 리부팅까지의 순서를 보니<br>커널 - 멀티패스 -쿼럼 - rgmanager 순의 우선순위를 가지고 하는것 같아보여서 만약 볼륨이 많다면  rgmanager까지 오는 시간이 길어질것이라 판단하여<br>쿼럼에서 감지후 리부팅 하는 옵션이 있는지 찾아보았는데요.<br>io_timeout=&quot;1&quot; 이라는 옵션이 있는것을 찾았는데요.<br>해당기능 쿼럼쪽에 넣은후<br>..생략<br>  &lt;quorumd interval=&quot;3&quot; io_timeout=&quot;1&quot; label=&quot;QDISK&quot; min_score=&quot;1&quot; tko=&quot;10&quot;&gt;<br>                &lt;heuristic interval=&quot;2&quot; program=&quot;ping -c1 -w1 10.1.0.1&quot; tko=&quot;5&quot;/&gt;<br>        &lt;/quorumd&gt;<br>..생략<br><br>동일하게 FC케이블을 제거해보니<br>1호기<br>Aug 10 17:01:00 node1 rgmanager[5955]: [script] Executing /etc/init.d/httpd status<br>Aug 10 17:01:11 node1 rgmanager[6215]: [script] Executing /etc/init.d/httpd status<br>Aug 10 17:01:15 node1 kernel: lpfc 0000:07:00.1: 1:1305 Link Down Event x2 received Data: x2 x20 x110 x0 x0<br>Aug 10 17:01:15 node1 kernel: lpfc 0000:07:00.0: 0:1305 Link Down Event x2 received Data: x2 x20 x110 x0 x0<br>Aug 10 17:01:33 node1 qdiskd[2714]: qdiskd: read (system call) has hung for 15 seconds<br>Aug 10 17:01:33 node1 qdiskd[2714]: In 15 more seconds, we will be evicted<br>Aug 10 17:01:45 node1 multipathd: 8:128: mark as failed<br>Aug 10 17:01:45 node1 multipathd: qdisk1: remaining active paths: 1<br>Aug 10 17:01:45 node1 multipathd: 8:192: mark as failed<br>Aug 10 17:01:45 node1 multipathd: data3: remaining active paths: 1<br>Aug 10 17:01:45 node1 kernel: rport-4:0-5: blocked FC remote port time out: removing target and saving binding<br>Aug 10 17:01:45 node1 kernel: rport-4:0-6: blocked FC remote port time out: removing target and saving binding<br>Aug 10 17:01:45 node1 kernel: lpfc 0000:07:00.1: 1:(0):0203 Devloss timeout on WWPN 20:78:00:c0:ff:d5:d1:ff NPort x0101ef Data: x0 x8 x0<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:1: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:5: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: rport-4:0-3: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: rport-4:0-4: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: rport-4:0-2: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:128.<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:192.<br>Aug 10 17:01:45 node1 kernel: lpfc 0000:07:00.1: 1:(0):0203 Devloss timeout on WWPN 20:70:00:c0:ff:d5:d1:ff NPort x010000 Data: x0 x8 x0<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:0: [sdh] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:0: [sdh]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:1: [sdi] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:1: [sdi]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 multipathd: sdh: remove path (uevent)<br>Aug 10 17:01:45 node1 multipathd: sdh: path already removed<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:2: [sdj] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:2: [sdj]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 multipathd: sdi: remove path (uevent)<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:3: [sdk] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:3: [sdk]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:4: [sdl] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:4: [sdl]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:5: [sdm] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 4:0:1:5: [sdm]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: rport-3:0-4: blocked FC remote port time out: removing target and saving binding<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:1: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:3: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:3: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:32.<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:5: rejecting I/O to offline device<br>Aug 10 17:01:45 node1 kernel: rport-3:0-5: blocked FC remote port time out: removing target and saving binding<br>Aug 10 17:01:45 node1 kernel: lpfc 0000:07:00.0: 0:(0):0203 Devloss timeout on WWPN 20:70:00:c0:ff:d5:d1:ff NPort x010000 Data: x0 x8 x0<br>Aug 10 17:01:45 node1 kernel: rport-3:0-3: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: lpfc 0000:07:00.0: 0:(0):0203 Devloss timeout on WWPN 20:78:00:c0:ff:d5:d1:ff NPort x0101ef Data: x0 x8 x0<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:96.<br>Aug 10 17:01:45 node1 kernel: rport-3:0-2: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: rport-3:0-7: blocked FC remote port time out: removing rport<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:0: [sdb] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:0: [sdb]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:1: [sdc] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:1: [sdc]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:2: [sdd] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:2: [sdd]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:3: [sde] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:3: [sde]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:4: [sdf] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:4: [sdf]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:5: [sdg] Synchronizing SCSI cache<br>Aug 10 17:01:45 node1 kernel: sd 3:0:0:5: [sdg]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 17:01:45 node1 multipathd: qdisk1: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:32 1]<br>Aug 10 17:01:45 node1 multipathd: sdi: path removed from map qdisk1<br>Aug 10 17:01:45 node1 multipathd: sdj: remove path (uevent)<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:32.<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:32.<br>Aug 10 17:01:45 node1 multipathd: qdisk2: load table [0 59392 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:48 1]<br>Aug 10 17:01:45 node1 multipathd: sdj: path removed from map qdisk2<br>Aug 10 17:01:45 node1 multipathd: sdk: remove path (uevent)<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:48.<br>Aug 10 17:01:45 node1 multipathd: data1: load table [0 40001536 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:64 1]<br>Aug 10 17:01:45 node1 multipathd: sdk: path removed from map data1<br>Aug 10 17:01:45 node1 multipathd: sdl: remove path (uevent)<br>Aug 10 17:01:45 node1 kernel: device-mapper: multipath: Failing path 8:64.<br>Aug 10 17:01:46 node1 multipathd: data2: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:80 1]<br>Aug 10 17:01:46 node1 multipathd: sdl: path removed from map data2<br>Aug 10 17:01:46 node1 multipathd: sdm: remove path (uevent)<br>Aug 10 17:01:46 node1 kernel: device-mapper: multipath: Failing path 8:80.<br>Aug 10 17:01:46 node1 multipathd: data3: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:96 1]<br>Aug 10 17:01:46 node1 multipathd: sdm: path removed from map data3<br>Aug 10 17:01:46 node1 multipathd: sdb: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: sdb: path already removed<br>Aug 10 17:01:46 node1 multipathd: 8:48: mark as failed<br>Aug 10 17:01:46 node1 multipathd: qdisk2: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: qdisk2: remaining active paths: 0<br>Aug 10 17:01:46 node1 multipathd: 8:80: mark as failed<br>Aug 10 17:01:46 node1 kernel: device-mapper: multipath: Failing path 8:96.<br>Aug 10 17:01:46 node1 multipathd: data2: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: data2: remaining active paths: 0<br>Aug 10 17:01:46 node1 multipathd: 8:64: mark as failed<br>Aug 10 17:01:46 node1 multipathd: data1: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: data1: remaining active paths: 0<br>Aug 10 17:01:46 node1 multipathd: sde: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: data1: map in use<br>Aug 10 17:01:46 node1 multipathd: data1: can't flush<br>Aug 10 17:01:46 node1 kernel: device-mapper: multipath: Failing path 8:96.<br>Aug 10 17:01:46 node1 multipathd: data1: load table [0 40001536 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 17:01:46 node1 multipathd: data1: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: sde: path removed from map data1<br>Aug 10 17:01:46 node1 multipathd: sdc: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: qdisk1: map in use<br>Aug 10 17:01:46 node1 multipathd: qdisk1: can't flush<br>Aug 10 17:01:46 node1 multipathd: qdisk1: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 17:01:46 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: sdc: path removed from map qdisk1<br>Aug 10 17:01:46 node1 multipathd: sdd: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: qdisk2: map in use<br>Aug 10 17:01:46 node1 multipathd: qdisk2: can't flush<br>Aug 10 17:01:46 node1 multipathd: qdisk2: load table [0 59392 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 17:01:46 node1 multipathd: qdisk2: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: sdd: path removed from map qdisk2<br>Aug 10 17:01:46 node1 multipathd: sdf: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: data2: map in use<br>Aug 10 17:01:46 node1 multipathd: data2: can't flush<br>Aug 10 17:01:46 node1 multipathd: data2: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 17:01:46 node1 multipathd: data2: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: sdf: path removed from map data2<br>Aug 10 17:01:46 node1 multipathd: sdg: remove path (uevent)<br>Aug 10 17:01:46 node1 multipathd: data3: map in use<br>Aug 10 17:01:46 node1 multipathd: data3: can't flush<br>Aug 10 17:01:46 node1 multipathd: data3: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 17:01:46 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: sdg: path removed from map data3<br>Aug 10 17:01:46 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 17:01:46 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 17:02:30 node1 corosync[2655]:   [CMAN  ] lost contact with quorum device<br>Aug 10 17:02:30 node1 corosync[2655]:   [QUORUM] Members[2]: 1 2<br>Aug 10 17:03:15 node1 multipathd: qdisk2: Disable queueing<br>Aug 10 17:03:15 node1 multipathd: data1: Disable queueing<br>Aug 10 17:03:15 node1 multipathd: data2: Disable queueing<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59264<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7408<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-2, sector 40001408<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59264<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7408<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59376<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7422<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59376<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7422<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 0<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 0<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 0<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 0<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 8<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 1<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59384<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7423<br>Aug 10 17:03:15 node1 kernel: end_request: I/O error, dev dm-1, sector 59384<br>Aug 10 17:03:15 node1 kernel: Buffer I/O error on device dm-1, logical block 7423<br>Aug 10 17:03:16 node1 multipathd: data3: Disable queueing<br>Aug 10 17:03:16 node1 multipathd: qdisk1: Disable queueing<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 1<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 2<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 3<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 4<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 5<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 6<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 7<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 8<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 9<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 10<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 11<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 12<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 13<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 14<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 15<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error reading node ID block 16<br>Aug 10 17:03:16 node1 qdiskd[2714]: diskRawWriteShadow: Input/output error<br>Aug 10 17:03:16 node1 qdiskd[2714]: diskRawWriteShadow: aligned write returned -1, not 512<br>Aug 10 17:03:16 node1 qdiskd[2714]: diskRawWriteShadow<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error writing node ID block 1<br>Aug 10 17:03:16 node1 qdiskd[2714]: Error writing to quorum disk<br>Aug 10 17:03:16 node1 qdiskd[2714]: Failed to send a heartbeat within 30 seconds (121.290000) - REBOOTING<br><br>rgmanager까지 가기전에 노드를 리부팅시키는것으로 보이네요...<br><br>이기능을 사용해도 괜찮은 방법일까요? ㅠ<br><br><publishedDate>2018-08-10T08:15:27Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NPneHIAT"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-10T08:14:07Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-10T08:14:06Z</b><br><br>-----------------1페이지<br><br>안녕하세요.<br>보내주신데로 HALVM 에 self fence를 조합하여 물리구성에서 테스트 해보니 정상적으로 페일오버되었습니다.<br><br>저는 구성을<br>노드1 과 노드2에 각각의 서비스가 액티브한 상태에서 테스트를 하였는데요.<br>..생략<br>        &lt;/fencedevices&gt;<br>        &lt;rm&gt;<br>                &lt;failoverdomains&gt;<br>                        &lt;failoverdomain name=&quot;C1-F&quot; nofailback=&quot;1&quot; ordered=&quot;1&quot;&gt;<br>                                &lt;failoverdomainnode name=&quot;node1-hb&quot; priority=&quot;1&quot;/&gt;<br>                                &lt;failoverdomainnode name=&quot;node2-hb&quot; priority=&quot;1&quot;/&gt;<br>                        &lt;/failoverdomain&gt;<br>                &lt;/failoverdomains&gt;<br>                &lt;resources&gt;<br>                        &lt;ip address=&quot;10.1.0.208&quot; monitor_link=&quot;on&quot; sleeptime=&quot;1&quot;/&gt;<br>                        &lt;ip address=&quot;10.1.0.206&quot; monitor_link=&quot;on&quot; sleeptime=&quot;1&quot;/&gt;<br>                        &lt;script file=&quot;/etc/init.d/nfs&quot; name=&quot;nfsd_service&quot;/&gt;<br>                        &lt;script file=&quot;/etc/init.d/httpd&quot; name=&quot;httpd_service&quot;/&gt;<br>                        &lt;lvm __enforce_timeouts=&quot;1&quot; name=&quot;halvm1&quot; self_fence=&quot;1&quot; vg_name=&quot;vg00&quot;/&gt;<br>                        &lt;lvm __enforce_timeouts=&quot;1&quot; name=&quot;halvm2&quot; self_fence=&quot;1&quot; vg_name=&quot;vg01&quot;/&gt;<br>                        &lt;lvm __enforce_timeouts=&quot;1&quot; name=&quot;halvm3&quot; self_fence=&quot;1&quot; vg_name=&quot;vg02&quot;/&gt;<br>                        &lt;fs __enforce_timeouts=&quot;1&quot; device=&quot;/dev/mapper/vg00-lvol0&quot; force_unmount=&quot;1&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/data1&quot; name=&quot;halvm-fs1&quot; options=&quot;rw,nobarrier&quot; self_fence=&quot;1&quot;/&gt;<br>                        &lt;fs __enforce_timeouts=&quot;1&quot; device=&quot;/dev/mapper/vg01-lvol0&quot; force_fsck=&quot;0&quot; force_unmount=&quot;1&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/data2_1&quot; name=&quot;halvm-fs2_1&quot; options=&quot;rw,nobarrier&quot; self_fence=&quot;1&quot;/&gt;<br>                        &lt;fs __enforce_timeouts=&quot;1&quot; device=&quot;/dev/mapper/vg01-lvol1&quot; force_fsck=&quot;0&quot; force_unmount=&quot;1&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/data2_2&quot; name=&quot;halvm-fs2_2&quot; options=&quot;rw,nobarrier&quot; self_fence=&quot;1&quot;/&gt;<br>                        &lt;fs __enforce_timeouts=&quot;1&quot; device=&quot;/dev/mapper/vg02-lvol0&quot; force_fsck=&quot;0&quot; force_unmount=&quot;1&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/data3_1&quot; name=&quot;halvm-fs3_1&quot; options=&quot;rw,nobarrier&quot; self_fence=&quot;1&quot;/&gt;<br>                        &lt;fs __enforce_timeouts=&quot;1&quot; device=&quot;/dev/mapper/vg02-lvol1&quot; force_fsck=&quot;0&quot; force_unmount=&quot;1&quot; fstype=&quot;ext4&quot; mountpoint=&quot;/data3_2&quot; name=&quot;halvm-fs3_2&quot; options=&quot;rw,nobarrier&quot; self_fence=&quot;1&quot;/&gt;<br>                &lt;/resources&gt;<br>                &lt;service autostart=&quot;0&quot; domain=&quot;C1-F&quot; name=&quot;C1-NFS&quot; recovery=&quot;relocate&quot;&gt;<br>                        &lt;ip ref=&quot;10.1.0.206&quot;/&gt;<br>                        &lt;lvm ref=&quot;halvm1&quot;/&gt;<br>                        &lt;lvm ref=&quot;halvm2&quot;/&gt;<br>                        &lt;fs ref=&quot;halvm-fs1&quot;/&gt;<br>                        &lt;fs ref=&quot;halvm-fs2_1&quot;/&gt;<br>                        &lt;fs ref=&quot;halvm-fs2_2&quot;/&gt;<br>                        &lt;script ref=&quot;nfsd_service&quot;/&gt;<br>                &lt;/service&gt;<br>                &lt;service autostart=&quot;0&quot; domain=&quot;C1-F&quot; name=&quot;C2-HTTPD&quot; recovery=&quot;relocate&quot;&gt;<br>                        &lt;ip ref=&quot;10.1.0.208&quot;/&gt;<br>                        &lt;lvm ref=&quot;halvm3&quot;/&gt;<br>                        &lt;fs ref=&quot;halvm-fs3_1&quot;/&gt;<br>                        &lt;fs ref=&quot;halvm-fs3_2&quot;/&gt;<br>                        &lt;script ref=&quot;httpd_service&quot;/&gt;<br>                &lt;/service&gt;<br>        &lt;/rm&gt;<br>        &lt;quorumd interval=&quot;3&quot; label=&quot;QDISK&quot; min_score=&quot;1&quot; tko=&quot;10&quot;&gt;<br>                &lt;heuristic interval=&quot;2&quot; program=&quot;ping -c1 -w1 10.1.0.1&quot; tko=&quot;5&quot;/&gt;<br>        &lt;/quorumd&gt;<br>        &lt;totem token=&quot;75000&quot;/&gt;<br>&lt;/cluster&gt;<br>..생략<br>이때 노드1의 FC가 단절되면<br>&lt;노드1&gt;<br>Aug 10 16:33:23 node1 rgmanager[15290]: [script] Executing /etc/init.d/httpd status<br>Aug 10 16:33:31 node1 kernel: lpfc 0000:07:00.0: 0:1305 Link Down Event x2 received Data: x2 x20 x110 x0 x0<br>Aug 10 16:33:31 node1 kernel: lpfc 0000:07:00.1: 1:1305 Link Down Event x2 received Data: x2 x20 x110 x0 x0<br>Aug 10 16:33:31 node1 kernel: lpfc 0000:07:00.0: 0:1303 Link Up Event x3 received Data: x3 x0 x10 x0 x0 x0 0<br>Aug 10 16:33:31 node1 kernel: lpfc 0000:07:00.0: 0:(0):0237 Pending Link Event during Discovery: State x9<br>Aug 10 16:33:31 node1 kernel: lpfc 0000:07:00.0: 0:1305 Link Down Event x4 received Data: x4 x9 x18114 x0 x0<br>Aug 10 16:33:47 node1 qdiskd[11493]: qdiskd: read (system call) has hung for 15 seconds<br>Aug 10 16:33:47 node1 qdiskd[11493]: In 15 more seconds, we will be evicted<br>Aug 10 16:34:01 node1 kernel: rport-3:0-4: blocked FC remote port time out: removing target and saving binding<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:1: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:3: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:3: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:5: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: rport-3:0-5: blocked FC remote port time out: removing target and saving binding<br>Aug 10 16:34:01 node1 kernel: lpfc 0000:07:00.0: 0:(0):0203 Devloss timeout on WWPN 20:70:00:c0:ff:d5:d1:ff NPort x010000 Data: x0 x8 x0<br>Aug 10 16:34:01 node1 kernel: rport-3:0-3: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: rport-3:0-2: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: rport-3:0-7: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:32.<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:96.<br>Aug 10 16:34:01 node1 kernel: lpfc 0000:07:00.0: 0:(0):0203 Devloss timeout on WWPN 20:78:00:c0:ff:d5:d1:ff NPort x0101ef Data: x0 x8 x0<br>Aug 10 16:34:01 node1 multipathd: 8:32: mark as failed<br>Aug 10 16:34:01 node1 multipathd: qdisk1: remaining active paths: 1<br>Aug 10 16:34:01 node1 multipathd: 8:96: mark as failed<br>Aug 10 16:34:01 node1 multipathd: data3: remaining active paths: 1<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:0: [sdb] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:0: [sdb]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 multipathd: sdb: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: sdb: path already removed<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:1: [sdc] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:1: [sdc]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 multipathd: sdc: remove path (uevent)<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:2: [sdd] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:2: [sdd]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:3: [sde] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:3: [sde]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:4: [sdf] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:4: [sdf]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:5: [sdg] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 3:0:0:5: [sdg]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: rport-4:0-5: blocked FC remote port time out: removing target and saving binding<br>Aug 10 16:34:01 node1 kernel: rport-4:0-6: blocked FC remote port time out: removing target and saving binding<br>Aug 10 16:34:01 node1 kernel: lpfc 0000:07:00.1: 1:(0):0203 Devloss timeout on WWPN 20:78:00:c0:ff:d5:d1:ff NPort x0101ef Data: x0 x8 x0<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:1: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:5: rejecting I/O to offline device<br>Aug 10 16:34:01 node1 kernel: rport-4:0-2: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: lpfc 0000:07:00.1: 1:(0):0203 Devloss timeout on WWPN 20:70:00:c0:ff:d5:d1:ff NPort x010000 Data: x0 x8 x0<br>Aug 10 16:34:01 node1 kernel: rport-4:0-4: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:128.<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:192.<br>Aug 10 16:34:01 node1 kernel: rport-4:0-3: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:0: [sdh] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:0: [sdh]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:1: [sdi] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:1: [sdi]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:2: [sdj] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:2: [sdj]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:3: [sdk] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:3: [sdk]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:4: [sdl] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:4: [sdl]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:5: [sdm] Synchronizing SCSI cache<br>Aug 10 16:34:01 node1 kernel: sd 4:0:1:5: [sdm]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK<br>Aug 10 16:34:01 node1 multipathd: qdisk1: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:128 1]<br>Aug 10 16:34:01 node1 multipathd: sdc: path removed from map qdisk1<br>Aug 10 16:34:01 node1 multipathd: sdd: remove path (uevent)<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:128.<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:128.<br>Aug 10 16:34:01 node1 multipathd: qdisk2: load table [0 59392 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:144 1]<br>Aug 10 16:34:01 node1 multipathd: sdd: path removed from map qdisk2<br>Aug 10 16:34:01 node1 multipathd: sde: remove path (uevent)<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:144.<br>Aug 10 16:34:01 node1 multipathd: data1: load table [0 40001536 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:160 1]<br>Aug 10 16:34:01 node1 multipathd: sde: path removed from map data1<br>Aug 10 16:34:01 node1 multipathd: sdf: remove path (uevent)<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:160.<br>Aug 10 16:34:01 node1 kernel: rport-3:0-1: blocked FC remote port time out: removing rport<br>Aug 10 16:34:01 node1 multipathd: data2: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:176 1]<br>Aug 10 16:34:01 node1 multipathd: sdf: path removed from map data2<br>Aug 10 16:34:01 node1 multipathd: sdg: remove path (uevent)<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:176.<br>Aug 10 16:34:01 node1 multipathd: data3: load table [0 19531776 multipath 1 queue_if_no_path 0 1 1 round-robin 0 1 1 8:192 1]<br>Aug 10 16:34:01 node1 multipathd: sdg: path removed from map data3<br>Aug 10 16:34:01 node1 multipathd: sdh: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: sdh: path already removed<br>Aug 10 16:34:01 node1 multipathd: sdi: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: qdisk1: map in use<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:192.<br>Aug 10 16:34:01 node1 multipathd: qdisk1: can't flush<br>Aug 10 16:34:01 node1 kernel: device-mapper: multipath: Failing path 8:192.<br>Aug 10 16:34:01 node1 multipathd: qdisk1: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 16:34:01 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 16:34:01 node1 multipathd: sdi: path removed from map qdisk1<br>Aug 10 16:34:01 node1 multipathd: sdj: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: qdisk2: map in use<br>Aug 10 16:34:01 node1 multipathd: qdisk2: can't flush<br>Aug 10 16:34:01 node1 multipathd: qdisk2: load table [0 59392 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 16:34:01 node1 multipathd: qdisk2: Entering recovery mode: max_retries=18<br>Aug 10 16:34:01 node1 multipathd: sdj: path removed from map qdisk2<br>Aug 10 16:34:01 node1 multipathd: sdk: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: data1: map in use<br>Aug 10 16:34:01 node1 multipathd: data1: can't flush<br>Aug 10 16:34:01 node1 multipathd: data1: load table [0 40001536 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 16:34:01 node1 multipathd: data1: Entering recovery mode: max_retries=18<br>Aug 10 16:34:01 node1 multipathd: sdk: path removed from map data1<br>Aug 10 16:34:01 node1 multipathd: sdl: remove path (uevent)<br>Aug 10 16:34:01 node1 multipathd: data2: map in use<br>Aug 10 16:34:01 node1 multipathd: data2: can't flush<br>Aug 10 16:34:02 node1 multipathd: data2: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 16:34:02 node1 multipathd: data2: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: sdl: path removed from map data2<br>Aug 10 16:34:02 node1 multipathd: sdm: remove path (uevent)<br>Aug 10 16:34:02 node1 multipathd: data3: map in use<br>Aug 10 16:34:02 node1 multipathd: data3: can't flush<br>Aug 10 16:34:02 node1 multipathd: data3: load table [0 19531776 multipath 1 queue_if_no_path 0 0 0]<br>Aug 10 16:34:02 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: sdm: path removed from map data3<br>Aug 10 16:34:02 node1 multipathd: data2: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: data1: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: qdisk2: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: qdisk1: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 16:34:02 node1 multipathd: data3: Entering recovery mode: max_retries=18<br>Aug 10 16:34:16 node1 ntpd[1993]: synchronized to 203.248.240.140, stratum 3<br>Aug 10 16:34:44 node1 corosync[11423]:   [CMAN  ] lost contact with quorum device<br>Aug 10 16:34:44 node1 corosync[11423]:   [QUORUM] Members[2]: 1 2<br>Aug 10 16:35:31 node1 multipathd: data2: Disable queueing<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531648<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441456<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531648<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441456<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531760<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441470<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531760<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441470<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 0<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 0<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 0<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 0<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 8<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 1<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531768<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441471<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531768<br>Aug 10 16:35:31 node1 kernel: Buffer I/O error on device dm-3, logical block 2441471<br>Aug 10 16:35:31 node1 kernel: end_request: I/O error, dev dm-3, sector 19531768<br>Aug 10 16:35:32 node1 multipathd: qdisk1: Disable queueing<br>Aug 10 16:35:32 node1 multipathd: qdisk2: Disable queueing<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 1<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 2<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 3<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 4<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 5<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 6<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 7<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 8<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 9<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 10<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 11<br>Aug 10 16:35:32 node1 multipathd: data1: Disable queueing<br>Aug 10 16:35:32 node1 multipathd: data3: Disable queueing<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 12<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 13<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 14<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 15<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error reading node ID block 16<br>Aug 10 16:35:32 node1 qdiskd[11493]: diskRawWriteShadow: Input/output error<br>Aug 10 16:35:32 node1 qdiskd[11493]: diskRawWriteShadow: aligned write returned -1, not 512<br>Aug 10 16:35:32 node1 qdiskd[11493]: diskRawWriteShadow<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error writing node ID block 1<br>Aug 10 16:35:32 node1 qdiskd[11493]: Error writing to quorum disk<br>Aug 10 16:35:32 node1 qdiskd[11493]: qdisk cycle took more than 3 seconds to complete (119.850000)<br>Aug 10 16:35:32 node1 rgmanager[16659]: [fs] fs:halvm-fs3_1: is_alive: failed read test on [/data3_1]. Return code: 2<br>Aug 10 16:35:32 node1 rgmanager[16768]: [fs] fs:halvm-fs3_1: Mount point is not accessible!<br>Aug 10 16:35:32 node1 rgmanager[12034]: status on fs &quot;halvm-fs3_1&quot; returned 1 (generic error)<br>Aug 10 16:35:32 node1 rgmanager[12034]: Stopping service service:C2-HTTPD<br>Aug 10 16:35:32 node1 rgmanager[16807]: [script] Executing /etc/init.d/httpd stop<br>Aug 10 16:35:32 node1 rgmanager[16866]: [ip] Removing IPv4 address 10.1.0.208/24 from eth0<br>Aug 10 16:35:33 node1 ntpd[1993]: Deleting interface #8 eth0, 10.1.0.208#123, interface stats: received=0, sent=0, dropped=0, active_time=155 secs<br>Aug 10 16:35:33 node1 rgmanager[17025]: [fs] unmounting /data3_2<br>Aug 10 16:35:34 node1 rgmanager[17153]: [fs] unmounting /data3_1<br>Aug 10 16:35:34 node1 rgmanager[17189]: [lvm] HA LVM: Unable to get volume group attributes for vg02<br>Aug 10 16:35:34 node1 rgmanager[17225]: [lvm] Unable to deactivate  REBOOT  &lt;------------리부팅<br><br>리부팅 후에 노드2로 페일오버 되었습니다.<br><br><br>게시판제한에 걸려서 위을 참고해주세요<br><br><publishedDate>2018-08-10T08:14:06Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NLYP1IAP"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-10T01:32:31Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-10T01:32:31Z</b><br><br>안녕하세요. <br>유선상으로도 말씀드렸지만 몇가지 확인사항이 있어서 케이스문의드립니다.<br><br>1. 스카시펜싱으로 RHCS가 구성되어있습니다. 스카시펜싱 어떠한 상황에서  동작을 하는지 확인부탁드립니다.<br>2. 스카시펜싱의 경우 LUN에 해당 키값이 저장되는데 LUN자체에 장애가 발생할 경우 페일오버 혹은 펜싱의 기능이 불가능한 건지 답변부탁드립니다.<br>3. FC케이블이 단절되었을 경우 페일오버가 혹은 펜싱기능이 불가능한 건지 답변부탁드립니다.<br>4. 스카시펜싱의 경우 각각노드에서 각각의 키값의 오너쉽을 가지고 있다가 해당 키값이 제거되면 오너쉽을 변경되어 페일오버 되는 것으로 알고 있는데요.<br>이와같은 매카니즘이 맞는지 확인부탁드립니다.<br>5. rgmanager의 프리징 설정을 한후 파일시스템을 unmount 하였을때 클러스터가 페일오버 혹은 펜싱의 동작을 하는지 답변부탁드립니다.<br>6. 현재 구성상에서 FC케이블의 단절시에 정상적인  서비스를 유지하기 위해서 권고및 해결 방안을되는 방안을 유사사례에 근거하여  알려주시기 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-10T01:32:31Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NKyuLIAT"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-08T00:28:18Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-08T00:28:18Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>force_unmount 의 경우, resource stop 시에 mount 포인트에 대해서 다른 프로세스들이 사용하고 있을 때도 해당 프로세스들을 kill 해서<br>강제로 unmount 를 시도하게 하는 기능입니다.(즉, unmount 를 보다 확실하게 하는 안전장치)<br>만약 force_unmount 가 비활성화일 경우, 사용자가 self_fence 를 의도하지 않았음에도 불구하고 단순히 resource 를 다른 노드로 재배치 시키거나 단지 해당 resource 를 정지시키길 원하는 등<br>원하지 않는 타이밍에서 resource stop 시 unmount 가 실패할 가능성이 높아질 수 있으며,<br>이로 인해 self_fence 가 발생할 수 있습니다.<br><br>즉, 결론적으로 현재 상황에서 force_unmount=0 및 self_fence=1 조합을 통해서 FC 케이블 제거 시나리오에 대해서는 효과가 있을 것으로 보입니다.<br>다만, force_unmount 비활성화시에는 unmount 실패시 해당 노드가 리부팅 될 수 있는 점을 운영상에 추가로 고려하실 필요가 있습니다.<br>이에 대해서는 특별히 권장사항은 없으며 고객 선택 사항입니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-08T00:28:18Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NKkxeIAD"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-07T07:41:03Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-07T07:41:03Z</b><br><br>답변감사합니다.<br>제가 금일 이것저것 테스트 해보았는데요.<br><br>시스템에서 기본적으로 제공하는 /usr/share/cluster/fs.sh 내용을 살펴보던중<br> &lt;parameter name=&quot;self_fence&quot;&gt;<br>            &lt;longdesc lang=&quot;en&quot;&gt;<br>                If set and unmounting the file system fails, the node will<br>                immediately reboot.  Generally, this is used in conjunction<br>                with force-unmount support, but it is not required.<br>            &lt;/longdesc&gt;<br>            &lt;shortdesc lang=&quot;en&quot;&gt;<br>                Seppuku Unmount<br>            &lt;/shortdesc&gt;<br>            &lt;content type=&quot;boolean&quot;/&gt;<br>        &lt;/parameter&gt;<br>라는 내용이 있고<br><br>FC케이블을 제거하였을때 force  방식이 아니라면 umount가 안될꺼라 판단해<br><br>cluster.conf에서 resource 부분에<br>force_unmount=&quot;0&quot;  self_fence=&quot;1&quot; 두부분을 조합해서 넣으니 서버가 리부팅이 되는것 같습니다.<br><br>테스트 장비에서 로그를 보니 아래와 같은 로그가 발생하였습니다.<br>Aug 07 16:00:30 rgmanager [fs] unmounting /data<br>Aug 07 16:00:30 rgmanager [fs] umount failed: 1<br>Aug 07 16:00:35 rgmanager [fs] unmounting /data<br>Aug 07 16:00:35 rgmanager [fs] umount failed: 1<br>Aug 07 16:00:40 rgmanager [fs] unmounting /data<br>Aug 07 16:00:40 rgmanager [fs] umount failed: 1<br>Aug 07 16:00:40 rgmanager [fs] 'umount /data' failed, error=1<br>Aug 07 16:00:40 rgmanager [fs] umount failed - REBOOTING<br><br>5초간격으로 시도를 하고 3번을 실패하니 서버를 리부팅 시켜버리더군요.<br><br>테스트 서버의 구성은 san 스위치로 구성된 물리 장치입니다.<br><br>혹시 위와 같이 구성하였을때 발생하는 영향도를 알수 있을까요?? ㅠ<br>감사합니다.<br><br><publishedDate>2018-08-07T07:41:03Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NKX9QIAX"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-06T13:55:44Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-06T13:55:44Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>네, 현재 구성에서 scsi fencing 을 통해 원하시는 FC 장애 시나리오를 만족시키기는 어려워보입니다.<br>power fencing 이 있었다면 qdisk eviction 에 의해 cman 이 죽은 후,<br>노드2에 의해 노드1 은 약 75초(totem timeout) + 30초(fencing delay) 후에 power fencing 이 되어 재부팅이 이루어졌을 것입니다.<br><br>====<br>Jul 29 14:36:51 FLPIDMM01 corosync[40314]:   [CMAN  ] lost contact with quorum device<br>Jul 29 14:36:51 FLPIDMM01 corosync[40314]:   [QUORUM] Members[2]: 1 2<br>Jul 29 14:36:54 FLPIDMM01 corosync[40314]: cman killed by node 2 because we were killed by cman_tool or other application<br>====<br><br>====<br>Jul 29 14:36:51 FLPIDMM02 qdiskd[21748]: Assuming master role<br>Jul 29 14:36:54 FLPIDMM02 qdiskd[21748]: Writing eviction notice for node 1<br>Jul 29 14:36:57 FLPIDMM02 qdiskd[21748]: Node 1 evicted<br>....<br>Jul 29 14:38:09 FLPIDMM02 corosync[21678]:   [TOTEM ] A processor failed, forming new configuration.<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [QUORUM] Members[1]: 2<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [TOTEM ] A processor joined or left the membership and a new membership was formed.<br>Jul 29 14:38:11 FLPIDMM02 rgmanager[29125]: State change: FLPIDMM01.CS DOWN<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [CPG   ] chosen downlist: sender r(0) ip(192.168.1.52) r(1) ip(100.254.180.52) ; members(old:2 left:1)<br>Jul 29 14:38:11 FLPIDMM02 kernel: dlm: closing connection to node 1<br>Jul 29 14:38:11 FLPIDMM02 corosync[21678]:   [MAIN  ] Completed service synchronization, ready to provide service.<br>Jul 29 14:38:11 FLPIDMM02 fenced[22477]: fencing node FLPIDMM01.CS<br>Jul 29 14:38:43 FLPIDMM02 fenced[22477]: fence FLPIDMM01.CS success<br>Jul 29 14:38:43 FLPIDMM02 rgmanager[29125]: Taking over service service:service from down member FLPIDMM01.CS<br>====<br><br>감사합니다.<br><br><publishedDate>2018-08-06T13:55:44Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NKSUyIAP"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-06T08:28:54Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-06T08:28:53Z</b><br><br>답변감사합니다.<br>결국 파워펜싱을 사용하지 않는다면 펜싱은 불가능한거네요 ㅠ<br><br><publishedDate>2018-08-06T08:28:53Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NKS5FIAX"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-06T07:53:52Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-06T07:59:52Z</b><br><br>*** 일부 수정드립니다.<br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>현재까지 올려주신 내용을 토대로 내부 자료 및 재현 테스트 등을 통해<br>정리한 결과를 말씀드리면 다음과 같습니다.<br><br>watchdog 서비스 및 fence_scsi_check_hardreboot.pl 를 이용하여 scsi fencing 시 시스템을 리부팅시키기 위해서는<br>/var/run/cluster 디렉토리의 fence_scsi dev 및 key 파일의 내용을 토대로 관리 대상 스토리지에 접근하여 key 삭제 여부에 대해서 조회해야 합니다.<br>따라서, FC 케이블을 제거하여 해당 스토리지에 접근 자체가 불가능하다면<br>key 조회 또한 불가능하기에 watchdog 이 동작하기 어려울 것입니다.<br>이는 이미 테스트하신 결과에서도 확인되실 것으로 생각됩니다.<br><br>따라서, 현 상황에서 FC 장애시 보다 확실하게 Fail-over 를 성공시키기 위해서는<br>power fencing 을 고려하시는 것이 보다 나을 것으로 보입니다.<br>이 때도 동작시에 결국 멀티패스나 스토리지단의 타임아웃 값에 영향을 받을 수 있으므로 이 부분도 확인 및 조정이 필요할 것입니다.  <br><br>상기 내용들과 관련하여 궁금하신 내용은 업데이트 주시기 바랍니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-06T07:53:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NK4vHIAT"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-03T02:56:23Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-03T02:56:23Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>상황 확인 감사드립니다.<br>말씀주신 내용을 토대로 추가 조사 후 업데이트 드리도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-03T02:56:23Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NK4SUIA1"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-03T01:45:37Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-03T01:45:36Z</b><br><br>안녕하세요. 작업당시 오전에 1회 오후에 1회 fence_node 명령어를 이용하여 테스트를 하였고<br>성공확인을 하였는데요. 로그에 리부팅된 흔적이 없네요. 실제적으로 해당 서버가 reboot된것까지 확인을 하였습니다.<br><br>담당자에게 전화해보니 마찬가지로 오전1회 오후 1회를 실행하였고 모두 성공확인 까지 하였다는 답변을 받았습니다.<br><br>2호기 fence.log입니다.<br><br>FC절체시의 Jul 29 14:38:11 fenced fencing node FLPIDMM01.CS 밖에 찍힌것이 없네요.<br>FC절체시는 fence_node 명령어를 실행하지 않았는데...묘하게 저러네요.<br><br>Jun 24 10:53:44 fenced fenced 3.0.12.1 started<br>Jul 29 09:46:51 fenced fenced 3.0.12.1 started<br>Jul 29 10:56:56 fenced fenced 3.0.12.1 started<br>Jul 29 11:08:31 fenced fenced 3.0.12.1 started<br>Jul 29 12:27:34 fenced fenced 3.0.12.1 started<br>Jul 29 14:13:54 fenced fenced 3.0.12.1 started<br>Jul 29 14:28:23 fenced fenced 3.0.12.1 started<br>Jul 29 14:38:11 fenced fencing node FLPIDMM01.CS<br>Jul 29 14:38:43 fenced fence FLPIDMM01.CS success<br>Jul 29 14:59:37 fenced fenced 3.0.12.1 started<br><br>결론적으로 말씀드리면 로그에서는 뭔가 정확치 않치만<br>오전1회 오후2회의 fence_node를 실행하였습니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-03T01:45:36Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NJqJoIAL"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-02T08:14:52Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-02T08:14:52Z</b><br><br>안녕하세요. 제가 금일 휴가중이라서요..<br>내일중으로 업데이트 하겠습니다.<br><br>감사합니다.`<br><br><publishedDate>2018-08-02T08:14:52Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NJoxZIAT"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-02T06:36:52Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-02T06:36:52Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>담당자분께 확인해보셨나요?<br><br>감사합니다.<br><br><publishedDate>2018-08-02T06:36:52Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NJY9bIAH"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-01T08:30:59Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-01T08:30:59Z</b><br><br>답변감사합니다.<br>fence_node 를 이용하여 펜싱테스트를 분명히 진행한것으로 기억하는데요.<br>저도 정확치가 않아서 로그를 찾아보니 펜싱시 서버가 펜싱되었다는 로그가 보이지가 않네요... 이상하게도요.<br><br>우선 담당자가 휴가중이라서 내일중으로 다시 물어보고 업데이트 하도록하겠습니다.<br>고생이 많으시네요.<br>감사합니다.<br><br><publishedDate>2018-08-01T08:30:59Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NJXUiIAP"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-01T07:34:25Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-01T07:34:25Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>펜싱테스트를 언제 하셨는지 알 수 있을까요?<br><br>감사합니다.<br><br><publishedDate>2018-08-01T07:34:25Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NJWCAIA5"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-08-01T05:44:49Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-08-01T05:44:49Z</b><br><br>watchdog 서비스 설정후, 펜싱 및 리부팅 테스트시에는 정상적으로 동작되나요? 네<br><br>fence_node [node_name]하였을때 정상적으로 펜싱이 진행되었습니다.<br><br>추가적으로 서비스IP제거시 게이트웨이와 통신장애가 발생하여도 정상적으로 펜싱이 발생하였습니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-01T05:44:49Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0aA000000NJVUSIA5"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-01T04:17:14Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-01T04:48:24Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>상황 확인차 질문 드립니다만,<br>watchdog 서비스 설정후, 펜싱 및 리부팅 테스트시에는 정상적으로 동작되나요?<br>====<br>5) The fence_scsi_check.pl or fence_scsi_check_hardreboot.pl watchdog script should trigger a reboot when a cluster node has been successfully fenced via the fence_scsi agent. To test this, simply use the fence_node utility. The cluster node that was fenced should reboot itself.<br><br># fence_node &lt;nodename&gt;<br>====<br><br>확인 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-01T04:17:13Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0aA000000NJVU3IAP"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-08-01T04:16:19Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-08-01T04:16:56Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Service 를 이용해주셔서 감사합니다.<br><br>저는 Platform Support 를 담당하고 있는<br>Senior Technical Support Engineer 신재욱입니다.<br><br>현재 올려주신 문의내용은 자세히 살펴보고 있는중이며,<br>곧 관련된 내용을 업데이트 하도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-08-01T04:16:19Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>