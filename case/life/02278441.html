======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2018-12-19T07:58:44Z</b><br><b>마지막 답변자 : 타임게이트 타임게이트</b><br><b>마지막 수정 일자 : 2019-01-10T07:00:57Z</b><br><b>id : 5002K00000czOLFQA2</b><br>======================<br><br><b><font size=15>
제목  : [기술문의]RHCS scsi fence key값관련
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요. 타임게이트 송기호입니다.<br>rhel6 rhcs로 구성되어 있고 scsi fence를 사용중입니다.<br>초기 구성할때<br>sg_persist -k /dev/mapper/mpatha 의 결과를 보면 <br>  PR generation=0xd, 4 registered reservation keys follow:<br>    0x10<br>    0x10<br>    0x11<br>    0x11<br>예약된 키값이 정상적으로 보였는데요<br><br>시스템이 리부팅 된 이후로는 <br>sg_persist -k /dev/mapper/mpatha  의 결과값을 보면 키값이 보이지 않습니다.<br><br>알아보니 리부팅 후 cman을 시작하기 전에 <br>키값을 클리어 하라고 하는데요. 이러한 자료가 있는지 공유좀 부탁드립니다.<br><br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.4</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>FLPIDMM01/FLPIDMM02</hostname><br><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-31T07:50:51Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-31T07:50:51Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>전문엔지니어의 분석 의견입니다.<br><br>문제가 발생하기 전에 2번 노드가 수동으로 내려진 것을 볼 수 있습니다.<br>we can see prior to the event that this node was taken offline manually:<br><br>Dec 10 12:01:40 FLPIDMM02 rgmanager[27347]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:02:10 FLPIDMM02 rgmanager[28700]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:02:40 FLPIDMM02 rgmanager[30183]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:03:11 FLPIDMM02 rgmanager[31485]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:03:51 FLPIDMM02 rgmanager[33560]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:04:21 FLPIDMM02 rgmanager[34546]: [script] Executing /etc/cluster/dbsafer status<br>Dec 10 12:04:52 FLPIDMM02 rgmanager[37587]: Shutting down<br>Dec 10 12:04:52 FLPIDMM02 rgmanager[37587]: Stopping service service:service<br>Dec 10 12:04:52 FLPIDMM02 rgmanager[35817]: [script] Executing /etc/cluster/dbsafer stop<br>Dec 10 12:04:53 FLPIDMM02 rgmanager[36169]: [ip] Removing IPv4 address 100.254.180.53/24 from bond0<br>Dec 10 12:04:54 FLPIDMM02 ntpd[25044]: Deleting interface #9 bond0, 100.254.180.53#123, interface stats: received=0, sent=0, dropped=0, active_time=43202 secs<br>Dec 10 12:05:03 FLPIDMM02 rc_pam_acl[36428]: session opened for user 'sliida'(from flpidmm01) with service 'sshd' [#1]<br>Dec 10 12:05:03 FLPIDMM02 rc_pam_acl[36463]: session opened for user 'sliida'(from flpidmm01) with service 'sshd' [#1]<br>Dec 10 12:05:03 FLPIDMM02 rc_pam_acl[36515]: session opened for user 'sliida'(from flpidmm01) with service 'sshd' [#1]<br>Dec 10 12:05:03 FLPIDMM02 rgmanager[36690]: [fs] unmounting /DATA<br>Dec 10 12:05:03 FLPIDMM02 rc_pam_acl[36669]: session opened for user 'sliida'(from flpidmm01) with service 'sshd' [#1]<br>Dec 10 12:05:10 FLPIDMM02 multipathd: dm-41: remove map (uevent)<br>Dec 10 12:05:10 FLPIDMM02 multipathd: dm-41: devmap not registered, can't remove<br>Dec 10 12:05:13 FLPIDMM02 rgmanager[37149]: [lvm] Stripping tag, FLPIDMM02.CS<br>Dec 10 12:05:31 FLPIDMM02 rgmanager[37587]: Service service:service is stopped<br>Dec 10 12:05:31 FLPIDMM02 rgmanager[37587]: Disconnecting from CMAN<br>Dec 10 12:05:46 FLPIDMM02 rgmanager[37587]: Exiting<br>Dec 10 12:05:56 FLPIDMM02 modclusterd: shutdown succeeded<br>Dec 10 12:06:01 FLPIDMM02 kernel: dlm: closing connection to node 1<br>Dec 10 12:06:01 FLPIDMM02 kernel: dlm: closing connection to node 2<br>Dec 10 12:06:02 FLPIDMM02 qdiskd[25805]: Unregistering quorum device.<br>Dec 10 12:06:02 FLPIDMM02 cpglockd[37547]: cman requested shutdown. Exiting.<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Unloading all Corosync service engines.<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync extended virtual synchrony service<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync configuration service<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync cluster closed process group service v1.01<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync cluster config database access v1.01<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync profile loading service<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: openais checkpoint service B.01.01<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync CMAN membership service 2.90<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [SERV  ] Service engine unloaded: corosync cluster quorum service v0.1<br>Dec 10 12:06:02 FLPIDMM02 corosync[25733]:   [MAIN  ] Corosync Cluster Engine exiting with status 0 at main.c:1894.<br><br>이 시간대에 노드 1번에서 관련 로그를 볼 수 없지만 corosync.log에서 12:19에 클러스터가 시작된 것을 확인할 수 있습니다.<br>We don't have messages logs from node 1 covering this time period, but we can see in corosync.log that the cluster started on node at 12:19:<br><br>ec 10 12:19:01 corosync [MAIN  ] Corosync Cluster Engine ('1.4.1'): started and ready to provide service.<br>Dec 10 12:19:01 corosync [MAIN  ] Corosync built-in features: nss dbus rdma snmp<br>Dec 10 12:19:01 corosync [MAIN  ] Successfully read config from /etc/cluster/cluster.conf<br>Dec 10 12:19:01 corosync [MAIN  ] Successfully parsed cman config<br>Dec 10 12:19:01 corosync [TOTEM ] Initializing transport (UDP/IP Multicast).<br>Dec 10 12:19:01 corosync [TOTEM ] Initializing transmit/receive security: libtomcrypt SOBER128/SHA1HMAC (mode 0).<br>Dec 10 12:19:01 corosync [TOTEM ] Initializing transport (UDP/IP Multicast).<br>Dec 10 12:19:01 corosync [TOTEM ] Initializing transmit/receive security: libtomcrypt SOBER128/SHA1HMAC (mode 0).<br>Dec 10 12:19:02 corosync [TOTEM ] The network interface [192.168.1.51] is now up.<br>Dec 10 12:19:02 corosync [QUORUM] Using quorum provider quorum_cman<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync cluster quorum service v0.1<br>Dec 10 12:19:02 corosync [CMAN  ] CMAN 3.0.12.1 (built Jan  7 2013 13:59:45) started<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync CMAN membership service 2.90<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: openais checkpoint service B.01.01<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync extended virtual synchrony service<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync configuration service<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync cluster closed process group service v1.01<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync cluster config database access v1.01<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync profile loading service<br>Dec 10 12:19:02 corosync [QUORUM] Using quorum provider quorum_cman<br>Dec 10 12:19:02 corosync [SERV  ] Service engine loaded: corosync cluster quorum service v0.1<br>Dec 10 12:19:02 corosync [MAIN  ] Compatibility mode set to whitetank.  Using V1 and V2 of the synchronization engine.<br>Dec 10 12:19:02 corosync [TOTEM ] The network interface [100.254.180.51] is now up.<br>Dec 10 12:19:02 corosync [TOTEM ] A processor joined or left the membership and a new membership was formed.<br>Dec 10 12:19:02 corosync [QUORUM] Members[1]: 1<br>Dec 10 12:19:02 corosync [QUORUM] Members[1]: 1<br><br>노드 1번만 확인되었는데, 노드 2번이 오프라인 상태였기 때문이고 이로인해 노드 2번이 펜싱되었습니다.<br>it only found its self, because the cluster is offline on node 2. This results in us fencing node 2:<br><br>Dec 10 12:19:55 fenced fenced 3.0.12.1 started<br>Dec 10 12:20:01 fenced fencing node FLPIDMM02.CS<br>Dec 10 12:20:02 fenced fence FLPIDMM02.CS success<br><br>노드가 펜싱되면 공유스토리지에 접근할 수 없게 되는데, 이것은 예상되는 동작입니다. 이 동작 이외의 문의할 사항이 있습니까?<br>When the node was fenced, it lost access to all shared storage. This is all anticipated behavior. Were there any questions outside of this behavior?<br><br><br>전문엔지니어의 의견 살펴보시고, 추가로 궁금하신 사항이 있으시면 알려주시기 바랍니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2018-12-31T07:50:51Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000Ol1hkQAB"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-28T07:39:24Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-28T07:40:09Z</b><br><br>안녕하세요. 레드햇 김마현 입니다.<br>현재 로그 분석 및 2가지 사항에 대해 해외 전문 엔지니어와 함께 확인 중입니다.<br><br>1) scsi fence가 발생하는 조건<br>exts0 (360060e8010215890051114c900000084) dm-19 HITACHI,DF600F<br>size=150G features='0' hwhandler='0' wp=rw<br>|-+- policy='round-robin 0' prio=1 status=active<br>| `- 1:0:0:16 sdaj 66:48  active ready running<br>`-+- policy='round-robin 0' prio=0 status=enabled<br>  `- 4:0:0:16 sdaf 65:240 active ready running<br>* 위는 2번 노드의 샘플임.<br><br>다중의 path 중 하나의 path만 문제가 발생해도 scsi fence가 발생하는 조건이 될 수 있는지?<br><br>2) reservation conflict 원인<br>kernel, rgmanager 중 어떠한 레이어단에서 인지하는지?<br><br><br>* 추가적으로 올려주신 1번 노드 서버(FLPIDMM01)의 sosreport 파일을 확인해 보니<br>messages 파일의 내용이 누락(2018.12.14 이전 로그가 없음)된 것으로 보입니다.<br><br>1번 노드의 messages 파일을 압축 후 업로드를 요청드립니다.<br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-28T07:39:24Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OkiuTQAR"><br>======================<br><b>생성계정 : Song, Chang-An</b><br><b>생성날짜 : 2018-12-26T05:41:24Z</b><br><b>마지막 답변자 : Song, Chang-An</b><br><b>마지막 수정 일자 : 2018-12-26T05:41:24Z</b><br><br>안녕하세요? Red Hat 송 창 안 입니다.<br><br>현재 공유 해주신 정보에 대해서 관련 전문 엔지니어와 확인 작업 중에 있습니다.<br>확인 되는 내용에 대해서는 추가적으로 업데이트 하도록 하겠습니다.<br><br>감사합니다.<br>송 창 안 드림.<br><br><publishedDate>2018-12-26T05:41:24Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000Okhp8QAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-26T01:40:19Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-26T01:40:18Z</b><br><br>안녕하세요. 오늘로 확인해보니 키값이 정상적으로 들어가있네요.<br>금일 수집한 정보입니다.<br><br>root@FLPIDMM01 /root # sg_persist -k /dev/mapper/exts0<br>  HITACHI   DF600F            0000<br>  Peripheral device type: disk<br>  PR generation=0x69, 4 registered reservation keys follow:<br>    0x13<br>    0x13<br>    0x14<br>    0x14<br><br>root@FLPIDMM02 /root # sg_persist -k /dev/mapper/exts0<br>  HITACHI   DF600F            0000<br>  Peripheral device type: disk<br>  PR generation=0x69, 4 registered reservation keys follow:<br>    0x13<br>    0x13<br>    0x14<br>    0x14<br><br>하지만 당시에는 정상적으로 키값이 보이지 않았습니다.<br>해당 히스토리를 참고하셔서 분석의뢰 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-26T01:40:18Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OkDnDQAV"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-21T05:59:52Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-21T05:59:52Z</b><br><br>안녕하세요. 송기호입니다.<br>답변 감사합니다.<br><br>펜싱분석 요청드립니다.<br><br>최초 1호기의 /var 디렉토리의 inode가 풀이차서 클러스터 서비스가 비정상이 되었고 2호기로 페일오버 되었습니다.<br>2호기도 마찬가지로 inode가 풀이차서 정상적으로 서비스가 진행되지 않았고<br><br>수동으로 DR 3호기로 서비스를 가동했습니다.<br><br>inode를 모두 확보후 1호기를 리부팅 한 후 클러스터를 가동하였는데<br>2호기서버가 리부팅이 되었습니다.<br><br>Dec 10 12:19:54 FLPIDMM01 corosync[27538]:   [CMAN  ] quorum regained, resuming activity<br>Dec 10 12:19:54 FLPIDMM01 corosync[27538]:   [QUORUM] This node is within the primary component and will provide service.<br>Dec 10 12:19:54 FLPIDMM01 corosync[27538]:   [QUORUM] Members[1]: 1<br>Dec 10 12:19:55 FLPIDMM01 fenced[33171]: fenced 3.0.12.1 started<br>Dec 10 12:19:55 FLPIDMM01 dlm_controld[33197]: dlm_controld 3.0.12.1 started<br>Dec 10 12:19:56 FLPIDMM01 gfs_controld[33257]: gfs_controld 3.0.12.1 started<br>Dec 10 12:19:58 FLPIDMM01 fence_node[33281]: unfence FLPIDMM01.CS success<br>Dec 10 12:20:01 FLPIDMM01 fenced[33171]: fencing node FLPIDMM02.CS<br><br>개인적인 예상에는 스카시펜싱을 사용중인데 1호기 클러스터를 가동후 와치독에서 2호기 키값이 없는것을 판단 2호기가 펜싱된것으로 판단됩니다.<br>여기까지는 정상적인 프로세스라고 판단이 됩니다.<br><br>문제는 <br>1호기 2호기 모두 리부팅후 클러스터 서비스를 시작하면 스카시펜싱 키값이 예약되지 않습니다.<br>로그상에서는 키값이 충돌이 난다고 발생되고 있는 상황입니다.<br>Dec 10 12:25:47 FLPIDMM02 kernel: sd 4:0:0:16: [sdaf] Test WP failed, assume Write Enabled<br>Dec 10 12:25:47 FLPIDMM02 kernel: sd 1:0:0:16: reservation conflict<br><br>최초 리부팅후 키값의 충돌을 방지하기 위해서 sg_persist -C를 사용하여 키값을 clear해줘야 한다고 들었는데요.<br>정확한 프로세스 부탁드리며<br><br>스카시펜싱의 키값이 충돌나는 원인분석 부탁드립니다.<br> <br>감사합니다.<br><br><publishedDate>2018-12-21T05:59:52Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000Ojk1fQAB"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-19T09:13:16Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-19T09:13:16Z</b><br><br>안녕하세요. 레드햇 김마현 입니다.<br><br>먼저 두가지 부탁에 대해 협조해 주시면 감사하겠습니다.<br><br>1) 관련하여 로그 및 환경구성 등을 파악하기 위해 sosreport가 필요할 것으로 보입니다.<br>sosreport를 업로드 해주시기 바랍니다.<br><br>2) 정상적인 절차로 리부팅을 했는지 여부와 리부팅 시점이 언제입니까?<br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-19T09:13:16Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OjjCDQAZ"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-19T08:20:57Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-19T08:20:57Z</b><br><br>안녕하세요. 레드햇 김마현 입니다.<br><br>문의 주신 내용에 대해 확인 후 관련자료를 안내해 드리겠습니다.<br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-19T08:20:57Z</publishedDate><createdByType>Associate</createdByType><br>======================<br></comments><br>