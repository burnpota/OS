======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2018-12-10T01:04:09Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2019-01-08T11:07:13Z</b><br><b>id : 5002K00000cx6DSQAY</b><br>======================<br><br><b><font size=15>
제목  : [장애문의]RHCS 갑작스런 중단후 정상적인 서비스가 안되는문제
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요. 타임게이트 송기호입니다. 해당서버가 운영서버라 급하게 답변 부탁드립니다<br>RHCS로 구성된 서버중 1호기가 장애가 발생하여<br>2호기로 페일오버 되었습니다.<br><br>1호기는 펜싱되어 리부팅이 된상태입니다.<br><br>리부팅후 cman을 start하고자 하였는데<br>정상적으로 시작이 되지 않고 pvs, vgs, lvs를 하였을때 아래와 같은 메시지가 발생하고있습니다.<br><br>root@FLPIDMM01 /CRASH # vgscan<br>  /var/lock/lvm/P_global:aux: open failed: No space left on device<br>  Unable to obtain global lock.<br>root@FLPIDMM01 /CRASH # vgs<br>  /var/lock/lvm/V_vg1:aux: open failed: No space left on device<br>  Can't get lock for vg1<br>  Cannot process volume group vg1<br>  /var/lock/lvm/V_vg0:aux: open failed: No space left on device<br>  Can't get lock for vg0<br>  Cannot process volume group vg0<br>  /var/lock/lvm/V_vg3:aux: open failed: No space left on device<br>  Can't get lock for vg3<br>  Cannot process volume group vg3<br>  /var/lock/lvm/V_vg2:aux: open failed: No space left on device<br>  Can't get lock for vg2<br>  Cannot process volume group vg2<br>root@FLPIDMM01 /CRASH #<br>root@FLPIDMM01 /CRASH #<br>root@FLPIDMM01 /CRASH # lvs<br>  /var/lock/lvm/V_vg1:aux: open failed: No space left on device<br>  Can't get lock for vg1<br>  Cannot process volume group vg1<br>  /var/lock/lvm/V_vg0:aux: open failed: No space left on device<br>  Can't get lock for vg0<br>  Cannot process volume group vg0<br>  /var/lock/lvm/V_vg3:aux: open failed: No space left on device<br>  Can't get lock for vg3<br>  Cannot process volume group vg3<br>  /var/lock/lvm/V_vg2:aux: open failed: No space left on device<br>  Can't get lock for vg2<br>  Cannot process volume group vg2<br>root@FLPIDMM01 /CRASH # pvs<br>  /var/lock/lvm/P_global:aux: open failed: No space left on device<br>  Unable to obtain global lock<br><br>추가적으로 스카시펜싱을 사용중인데요<br>root@FLPIDMM01 /CRASH # sg_persist -i -k  /dev/mapper/ext11<br>  HITACHI   DF600F            0000<br>  Peripheral device type: disk<br>  PR generation=0x0, there are NO registered reservation keys<br>1호기 2호기모두 키값이 존재하고 있지 않습니다.<br><br>해결방법 부탁드립니다. 일단 양쪽 서버 모두 리부팅 예정입니다<br><br><br>장애  히스토리는<br>1호기에서 최초 공유볼륨디스크(vg3)과 통신이 되지 않아 REBOOT이 발생하였습니다.<br><br>Dec  9 23:12:56 FLPIDMM01 rgmanager[23408]: [lvm] WARNING: vg3 should not be active<br>Dec  9 23:12:56 FLPIDMM01 rgmanager[23452]: [lvm] WARNING: FLPIDMM01.CS does not own vg3<br>Dec  9 23:12:56 FLPIDMM01 rgmanager[23547]: [lvm] WARNING: Attempting shutdown of vg3<br><br>Dec  9 23:12:57 FLPIDMM01 rgmanager[39139]: status on lvm &quot;halvm1&quot; returned 1 (generic error)<br>Dec  9 23:12:57 FLPIDMM01 rgmanager[39139]: Stopping service service:service<br>Dec  9 23:12:57 FLPIDMM01 rgmanager[23627]: [script] Executing /etc/cluster/dbsafer stop<br>Dec  9 23:13:02 FLPIDMM01 rc_pam_acl[23798]: session opened for user 'sliida'(from 100.254.180.21) with service 'sshd' [#1]<br>Dec  9 23:14:26 FLPIDMM01 rgmanager[27425]: [ip] Removing IPv4 address 100.254.180.53/24 from bond0<br>Dec  9 23:14:28 FLPIDMM01 ntpd[26689]: Deleting interface #8 bond0, 100.254.180.53#123, interface stats: received=0, sent=0, dropped=0, active_time=7888476 secs<br>Dec  9 23:14:36 FLPIDMM01 rgmanager[27818]: [fs] unmounting /DATA<br>Dec  9 23:14:41 FLPIDMM01 rgmanager[27894]: [lvm] Unable to deactivate  REBOOT<br>Dec  9 23:14:41 FLPIDMM01 kernel: sd 6:2:1:0: [sdam] Synchronizing SCSI cache<br>Dec  9 23:14:41 FLPIDMM01 kernel: sd 6:2:0:0: [sdal] Synchronizing SCSI cache<br>Dec  9 23:14:41 FLPIDMM01 kernel: igb 0000:44:00.1: PCI INT B disabled<br>Dec  9 23:14:41 FLPIDMM01 kernel: igb 0000:44:00.0: PCI INT A disabled<br>Dec  9 23:14:41 FLPIDMM01 kernel: bonding: bond0: link status definitely down for interface p2p2, disabling it<br>Dec  9 23:14:41 FLPIDMM01 kernel: bonding: bond1: link status definitely down for interface p2p1, disabling it<br>Dec  9 23:14:41 FLPIDMM01 kernel: bonding: bond1: now running without any active interface !<br>Dec  9 23:14:41 FLPIDMM01 kernel: bonding: bond1: link status definitely down for interface p3p2, disabling it<br><br> <br><br>쿼럼디스크와도 통신을 하지 못하여 쿼럼 eviction으로 인해 1호기에 펜싱신호를 날리고 정상적으로 펜싱시그널이 성공하였습니다. <br><br>Dec  9 23:15:28 FLPIDMM02 qdiskd[25805]: Writing eviction notice for node 1<br>Dec  9 23:15:31 FLPIDMM02 qdiskd[25805]: Node 1 evicted<br><br>Dec  9 23:15:54 FLPIDMM02 fenced[31636]: fencing node FLPIDMM01.CS<br>Dec  9 23:16:25 FLPIDMM02 fenced[31636]: fence FLPIDMM01.CS success<br><br>그 후 2호기로 모든 서비스가 넘어간것으로 보입니다.<br><br>Dec  9 23:16:27 FLPIDMM02 rgmanager[38263]: [lvm] Starting volume group, vg3<br>Dec  9 23:16:27 FLPIDMM02 rgmanager[38300]: [lvm] I can claim this volume group<br>Dec  9 23:16:28 FLPIDMM02 rgmanager[38325]: [lvm] Stripping tag, FLPIDMM01.CS<br>Dec  9 23:16:29 FLPIDMM02 rgmanager[41371]: [lvm] New tag &quot;FLPIDMM02.CS&quot; added to vg3<br>Dec  9 23:16:30 FLPIDMM02 rgmanager[44519]: [fs] mounting /dev/dm-41 on /DATA<br>Dec  9 23:16:30 FLPIDMM02 rgmanager[44541]: [fs] mount -t ext4 -o rw,nobarrier /dev/dm-41 /DATA<br>같습니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 6.4</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>FLPIDMM01</hostname><br><br>======================<br><b>생성계정 : Li, Xingbin</b><br><b>생성날짜 : 2018-12-10T03:02:23Z</b><br><b>마지막 답변자 : Li, Xingbin</b><br><b>마지막 수정 일자 : 2018-12-10T03:02:23Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services를 이용해주셔서 감사합니다.<br><br>자세한 정보 감사드립니다.<br>sosreport를 올려주시는 대로 케이스 진행하도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-10T03:02:23Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000Ohd1qQAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-10T02:01:14Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-10T02:01:13Z</b><br><br>안녕하세요. 추가 내용 업데이트 해드립니다.<br>현재 <br>df -i하였을때 /var에 inode가 풀찬것이 확인되어 파일을 삭제하였더니<br>vgs가 정상적으로 보이는 있습니다.<br><br>다만 업무쪽에서 2호기로 넘기 리소스가 정상적으로 보이지 않는다고 해서 자세히 살펴보는 중입니다.<br>sosreport를 수집중이니 수집완료 후 업데이트 해드리겠습니다.<br><br>감사합니다<br><br><publishedDate>2018-12-10T02:01:13Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>