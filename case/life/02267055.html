======================<br><b>생성계정 : 타임게이트 타임게이트</b><br><b>생성날짜 : 2018-12-04T04:46:50Z</b><br><b>마지막 답변자 : GSS Tools</b><br><b>마지막 수정 일자 : 2019-01-16T11:00:44Z</b><br><b>id : 5002K00000cvhZ1QAI</b><br>======================<br><br><b><font size=15>
제목  : [생명]리소스 작업 시간 초과 이슈
</font></b><br><br>======================<br><b>사전문의<br></b><br>안녕하세요.타임게이트 입니다.<br>금일 resource timeout 이 발생하면서  12:08에 reboot 되어 원인분석 요청드립니다.<br>vmcore 는 생성이 안되었으며 sosreport 는 드롭박스에 업로드 해놓았습니다.<br>파일명은 sosreport-PCTAAL01SL-20181204123839.tar.xz 입니다.<br>감사합니다.<br>=======================<br><b>상태 : Closed</b><br><b>제품명  : Red Hat Enterprise Linux</b><br><b>버젼  : 7.2</b><br><b>타입  : Account / Customer Service Request</b><br><b>계정 번호  : 1648604</b><br><b>심각도  : 3 (Normal)</b><br><hostname>PCTAAL01SL</hostname><br><br><br><comment id="a0a2K00000OkBuYQAV"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-21T01:14:42Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-21T01:14:41Z</b><br><br>안녕하세요, 레드햇 김마현입니다.<br><br>문의하신 내용에 대하여 안내해 드리겠습니다.<br><br>송중석 수석님께서 유선상으로 아래와 같이 문의를 주신것에 대해 답변 드리겠습니다.<br>&gt;&gt; ping에 관련해서 마이그레이션 쓰레드홀드가 정상적으로 3회 내려간 로그를 확인해줄 수 있나?<br><br>말씀 주신것처럼 migration-threshold 값은 3 입니다.<br># cat /tmp/sosreport-PCTAAL01SL-20181204123839/sos_commands/cluster/pcs_config | grep threshold<br> migration-threshold: 3<br><br>우선 migration-threshold 파라메터가 의미하는 바를 먼저 설명드리겠습니다.<br>(이해를 됩기 위해 이슈 상황처럼 A노드, B노드로 설정하여 설명드립니다.)<br><br>migration-threshold는 리소스 복구중에 적용되는 파라메터로, 클러스터가 B노드로 리소스 재 할당을 하기 전에 A노드에서 실패한(모니터링) 리소스를 다시 시작하려고 할때 시도해야하는 횟수를 정의합니다.<br>이슈가 된 서버는 3으로 정의되어 있습니다.<br><br>해당 서버처럼 3회라고 가정한다면 다음 절차대로 움직입니다.<br>리소스 실패 -&gt; 리소스 stop -&gt; 리소스 start,<br>실패했다면 -&gt; 리소스 stop -&gt; 리소스 start,<br>실패했다면 -&gt; 리소스 stop -&gt; 리소스 start,<br>실패했다면 -&gt; 다른 노드에서 리소스 start<br><br>하지만 migration-threshold 3을 채우기 전, 첫번째 사이클 과정에서 리소스 stop을 성공하지 못했습니다.<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>* 해당 로그는 위와 같이 B노드에서도 확인이 됩니다.<br><br>stop 동작은 기본적으로 on-fail=fence 를 가집니다.<br>따라서 리소스의 stop 동작이 실패해서 data corruption 방지를 위해 fence_kdump가 동작하게 됩니다.<br><br>이번 건의 경우 migration-threshold 에 정의된 3번의 임계치에 접근하기도 전에 이미 첫번째 사이클 과정의 stop에서 실패가 되었고, 그로인해 fence_kdump가 동작을 한 케이스로 파악이 됩니다.<br><br><br>유선상으로 말씀드린바와 같이 이슈가 되었던 시점에 sar 로그의 시각이 밀렸습니다.<br>[root@pcm01 tmp]# sar -f PCTAAL01SL_sa04 -s 22:00:00 -e 22.10:00<br>Linux 3.10.0-327.53.1.el7.x86_64 (PCTAAL01SL) <br>12/03/2018 <br>_x86_64_<br>(16 CPU)<br><br>10:00:01 PM     CPU     %user     %nice   %system   %iowait    %steal     %idle<br>10:01:01 PM     all      0.88      0.00      6.31      0.05      0.00     92.76<br>10:02:01 PM     all      0.63      0.00      6.17      0.05      0.00     93.16<br>10:03:01 PM     all      0.84      0.00      6.36      0.05      0.00     92.75<br>10:04:01 PM     all      0.91      0.00      6.75      0.05      0.00     92.29<br>10:05:01 PM     all      0.67      0.00      6.32      0.05      0.00     92.96<br>10:06:16 PM     all      0.12      0.00      0.81     80.18      0.00     18.90   --&gt; * high load<br>10:07:20 PM     all      0.43      0.00      1.11     95.88      0.00      2.57   --&gt; * high load<br>10:08:01 PM     all      0.95      0.00      7.29      2.79      0.00     88.97<br><br>* 참고로 sar 로깅 시각은 이슈서버와 제 분석서버간 time locale 차이로 인해 시간 차이가 있습니다.<br><br>따라서 클러스터 관련 로그도 정확한 시각으로 로그를 찍었다고 보기 어렵지만, 앞서 설명드린 동작 매커니즘 상에는 맞게 로깅이 되었다고 판단됩니다.<br><br><br>이상입니다.<br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-21T01:14:41Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OjfoAQAR"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-19T02:09:19Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-19T02:09:19Z</b><br><br>답변 감사합니다.<br>cpu부하에 의해서 로그생성이 원활하지 않은것인지요?<br>맞다면 <br>다른 정상적인 케이스의 로그를 예로 설명 부탁드리겠습니다.<br>감사합니다.<br><br><publishedDate>2018-12-19T02:09:19Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OjROyQAN"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-18T07:48:21Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-18T07:48:20Z</b><br><br>답변 감사합니다. 다른 문의 사항이 있으면 문의 드리겠습니다.<br><br><publishedDate>2018-12-18T07:48:20Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OjPwFQAV"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-18T05:35:55Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-18T06:07:19Z</b><br><br>안녕하세요, 레드햇의 김마현입니다.<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br>문의하신 내용에 대하여 안내해 드리겠습니다.<br><br><br>&gt;&gt; 1.  12시 06분 11초에서 12시 07분 20초까지의 로그에서<br>&gt;&gt; Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br>&gt;&gt; 위 로그로 인해 fencing이 진입하지 않은 이유는 무엇인지 궁금합니다.<br><br>답변 : 노드 A의 로그를 확인시 <br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br>Dec 04 12:06:11 [5318] PCTAAL01SL       lrmd:     info: log_execute:<br>executing - rsc:ping action:stop call_id:77                                                                                                  <br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:     info: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Cancelled  call=54 key=ping_monitor_5000 confirmed=true<br>--&gt; call=54 는 Cancelled 됨이 확인 됩니다.<br><br>fence_kdump는 call_id:77에 의해 수행된것으로 판단됩니다.<br>======================<br>&gt;&gt; 2. fencing을 진입하게된 결정적인 로그는<br>&gt;&gt; Dec 04 12:06:50 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for VG_SRPCTAAP3VG on PCTAAL01SL-HB: Timed Out  call=56 key=VG_SRPCTAAP3VG_monitor_10000 timeout=30000ms<br>&gt;&gt; 위 로그때문에 발생한것인지요? 타임아웃 30초를 더하면 12시 07분 20초가 됩니다. 이는 kdump 팬싱 웨이팅 시작하는 시간과 일치합니다.<br><br><br>답변 : 최초 fence_kdump를 진입하게된 계기는 아래 로그에서 확인됩니다.<br>Dec 04 12:07:20 [5318] PCTAAL01SL       lrmd:     info: log_finished:<br>finished - rsc:ping action:stop call_id:77 pid:16065 exit-code:1 exec-time:68934ms queue-time:0ms<br>--&gt; call=77의 ping stop 수행을 마침<br>Dec 04 12:07:20 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of stop operation for ping on PCTAAL01SL-HB: Timed Out  call=77 key=ping_stop_0 timeout=20000ms<br>--&gt; call=77의 ping stop 동작에 에러 발생<br><br>언급해 주신 VG_SRPCTAAP3VG 리소스를 포함한 다른 리소스들은 모니터링 결과 ok 시그널이 확인됩니다<br># cat corosync.log | grep Dec\ 04\ 12:0[6-9] | grep '(ok)'<br>Dec 04 12:07:19 [5321] PCTAAL01SL       crmd:     info: process_lrm_event:<br>Result of monitor operation for LV_APP_con_ctmag901 on PCTAAL01SL-HB: 0 (ok)  call=58 key=LV_APP_con_ctmag901_monitor_5000 confirmed=false cib-update=51<br>Dec 04 12:07:20 [5321] PCTAAL01SL       crmd:     info: process_lrm_event:<br>Result of monitor operation for VG_SRPCTAAP3VG on PCTAAL01SL-HB: 0 (ok)  call=56 key=VG_SRPCTAAP3VG_monitor_10000 confirmed=false cib-update=53<br>Dec 04 12:07:20 [5321] PCTAAL01SL       crmd:     info: process_lrm_event:<br>Result of monitor operation for PCTA1_vip on PCTAAL01SL-HB: 0 (ok)  call=60 key=PCTA1_vip_monitor_5000 confirmed=false cib-update=54<br>Dec 04 12:07:51 [5321] PCTAAL01SL       crmd:     info: process_lrm_event:<br>Result of monitor operation for ipmilan_stonith2 on PCTAAL01SL-HB: 0 (ok)  call=53 key=ipmilan_stonith2_monitor_60000 confirmed=false cib-update=55<br><br><br>노드 B는 Stop of failed resource ping 를 이유로 노드 A에 fence_kdump를 수행하고 노드 A로부터 메시지를 기다리게 됩니다.<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Node PCTAAL01SL-HB will be fenced because of resource failure(s)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]:  notice: Stop of failed resource ping:0 is implicit after PCTAAL01SL-HB is fenced<br>.<br>.<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Dec  4 12:07:20 PCTAAL02SL fence_kdump[13549]: waiting for message from 'xxx.xx.xx.151'<br>======================<br>&gt;&gt; 3. 해당 sosreport의 링크 부탁드립니다.<br>아래 URL을 이용하시면 됩니다.<br>https://access.redhat.com/rs/cases/02267055/attachments/e4566abd-eb37-4739-89fe-998e0a6063b3<br><br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-18T05:35:55Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OjBVSQA3"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-17T08:55:32Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-17T08:55:32Z</b><br><br>답변 감사합니다.<br>아직 궁금한 사항이 있어 여쭤봅니다.<br><br>1.  12시 06분 11초에서 12시 07분 20초까지의 로그에서 <br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br>위 로그로 인해 fencing이 진입하지 않은 이유는 무엇인지 궁금합니다.<br><br>2. fencing을 진입하게된 결정적인 로그는<br>Dec 04 12:06:50 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for VG_SRPCTAAP3VG on PCTAAL01SL-HB: Timed Out  call=56 key=VG_SRPCTAAP3VG_monitor_10000 timeout=30000ms<br>위 로그때문에 발생한것인지요? 타임아웃 30초를 더하면 12시 07분 20초가 됩니다. 이는 kdump 팬싱 웨이팅 시작하는 시간과 일치합니다.<br><br>3. 해당 sosreport의 링크 부탁드립니다.<br><br><publishedDate>2018-12-17T08:55:32Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000Oj830QAB"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-17T02:46:58Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-17T03:51:03Z</b><br><br>안녕하세요, 레드햇의 김마현입니다.<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br>문의하신 내용에 대하여 안내해 드리겠습니다.<br><br>로그를 바탕으로 전체적인 내용을 정리하자면 아래와 같습니다.<br>1) 노드 A의 CPU와 I/O 로드가 높은 상태에서 노드 A의 리소스 stop의 실패.<br>2) 노드 B가 노드 A에게 kdump_fence 진행 요청 (token timeout이 원인이 아님, 리소스 stop 실패가 원인)<br>3) 노드 B는 120초간 노드 A로부터 kdump message를 받지 못함.<br>4) 노드 B는 token timeout을 감지 후 노드 B를 impi(ILO)를 통해 reboot 진행.<br><br>따라서 최초 token timeout 감지는 아래 시각입니다.<br>Dec  4 12:09:44 PCTAAL02SL corosync[14285]: [TOTEM ] A processor failed, forming new configuration.<br><br><br>아래는 해당 로그들을 바탕으로 한 설명입니다.<br><br>## 노드 A에서 리소스들의 모니터링에 문제가 있음이 확인됨.<br>gateway로 ping을 수행하는 리소스 모니터링에 timeout이 감지되어 리소스 stop을 진행 하지만 stop도 timed out이 됨.<br># cat corosync.log | grep Dec\ 04\ 12:0[6-9] | grep -i -e error -e stop | grep -v info:<br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br>Dec 04 12:06:44 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for PCTA1_vip on PCTAAL01SL-HB: Timed Out  call=60 key=PCTA1_vip_monitor_5000 timeout=10000ms<br>Dec 04 12:06:50 [5316] PCTAAL01SL        cib:    error: handle_new_connection:<br>Error in connection setup (5316-16086-14): Broken pipe (32)<br>Dec 04 12:06:50 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for VG_SRPCTAAP3VG on PCTAAL01SL-HB: Timed Out  call=56 key=VG_SRPCTAAP3VG_monitor_10000 timeout=30000ms<br>Dec 04 12:06:50 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for LV_APP_con_ctmag901 on PCTAAL01SL-HB: Timed Out  call=58 key=LV_APP_con_ctmag901_monitor_5000 timeout=30000ms<br>Dec 04 12:06:50 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ipmilan_stonith2 on PCTAAL01SL-HB: Timed Out  call=53 key=ipmilan_stonith2_monitor_60000 timeout=20000ms<br>Dec 04 12:07:08 [5318] PCTAAL01SL       lrmd:  warning: child_timeout_callback:<br>ping_stop_0 process (PID 16065) timed out<br>Dec 04 12:07:14 [5318] PCTAAL01SL       lrmd:     crit: child_timeout_callback:<br>ping_stop_0 process (PID 16065) will not die!<br>Dec 04 12:07:20 [5318] PCTAAL01SL       lrmd:  warning: operation_finished:<br>ping_stop_0:16065 - timed out after 20000ms<br>Dec 04 12:07:20 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of stop operation for ping on PCTAAL01SL-HB: Timed Out  call=77 key=ping_stop_0 timeout=20000ms<br>======================<br>## 노드 B에서 노드 A의 리소스 monitor와 stop에 에러가 있음을 감지함<br># cat PCTAAL02SL_corosync2 | grep Dec\ \ 4\ 12:0[6-9] | grep unknown\ error<br>Dec  4 12:06:11 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:06:11 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for VG_SRPCTAAP3VG on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for PCTA1_vip on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for LV_APP_con_ctmag901 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for ipmilan_stonith2 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for VG_SRPCTAAP3VG on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for PCTA1_vip on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for LV_APP_con_ctmag901 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for ipmilan_stonith2 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op stop for ping:0 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for VG_SRPCTAAP3VG on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for PCTA1_vip on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for LV_APP_con_ctmag901 on PCTAAL01SL-HB: unknown error (1)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Processing failed op monitor for ipmilan_stonith2 on PCTAAL01SL-HB: unknown error (1)<br><br># cat PCTAAL02SL_corosync2 | grep Dec\ \ 4\ 12:0[6-9] | grep Stop | grep failed<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]:  notice: Stop of failed resource ping:0 is implicit after PCTAAL01SL-HB is fenced<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]:  notice: Stop of failed resource ping:0 is implicit after PCTAAL01SL-HB is fenced<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]:  notice: Stop of failed resource ping:0 is implicit after PCTAAL01SL-HB is fenced<br>======================<br>## 노드 B는 resource failure에 의해 노드 A가 펜싱될 것이라고 경고<br># cat PCTAAL02SL_corosync2 | grep Dec\ \ 4\ 12:0[6-9] | grep 'resource failure'<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Node PCTAAL01SL-HB will be fenced because of resource failure(s)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Node PCTAAL01SL-HB will be fenced because of resource failure(s)<br>Dec  4 12:07:20 PCTAAL02SL pengine[14305]: warning: Node PCTAAL01SL-HB will be fenced because of resource failure(s)<br><br><br>## 노드 B는 fence_kdump Agent를 통해 노드 B를 kdump 트리거 시그널을 보냄, 그리고 노드 A로 부터 메시지(kdump 캡쳐커널 부팅 후 네트워크 신호)를 기다림<br>Dec  4 12:07:20 PCTAAL02SL crmd[14306]:  notice: Requesting fencing (reboot) of node PCTAAL01SL-HB #007 action=53 timeout=60000<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]:  notice: Client crmd.14306.3427cb87 wants to fence (reboot) 'PCTAAL01SL-HB' with device '(any)'<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]:  notice: Requesting peer fencing (reboot) of PCTAAL01SL-HB #007 id=8ca052ee-5769-4884-87e4-8ae802577fa2 state=0<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]:  notice: kdump_stonith can fence (reboot) PCTAAL01SL-HB: static-list<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]:  notice: ipmilan_stonith1 can fence (reboot) PCTAAL01SL-HB: static-list<br>Dec  4 12:07:20 PCTAAL02SL stonith-ng[14302]: warning: Agent 'fence_kdump' does not advertise support for 'reboot', performing 'off' action instead<br>Dec  4 12:07:20 PCTAAL02SL fence_kdump[13549]: waiting for message from 'xxx.xx.xx.151'<br><br><br>## 노드 B는 노드 A로부터 timeout 시간동안(120초) 메시지를 받지 못함.<br># cat PCTAAL02SL_corosync2 | grep Dec\ \ 4\ 12:0[6-9] | grep 13549<br>Dec  4 12:07:20 PCTAAL02SL fence_kdump[13549]: waiting for message from 'xxx.xx.xx.151'<br>Dec  4 12:09:20 PCTAAL02SL stonith-ng[14302]:  notice: Child process 13549 performing action 'off' timed out with signal 15<br>Dec  4 12:09:20 PCTAAL02SL stonith-ng[14302]:   error: Operation 'reboot' [13549] (call 2 from crmd.14306) for host 'PCTAAL01SL-HB' with device 'kdump_stonith' returned: -62 (Timer expired)<br><br><br>## 노드 B에서 토큰 timeout을 감지하고 ipmi(ILO)를 통해 reboot을 수행.<br># cat PCTAAL02SL_corosync2 | grep Dec\ \ 4\ 12:0[6-9] | grep 'A processor failed' -A 1<br>Dec  4 12:09:44 PCTAAL02SL corosync[14285]: [TOTEM ] A processor failed, forming new configuration.<br>Dec  4 12:09:47 PCTAAL02SL stonith-ng[14302]:  notice: Operation 'reboot' [16561] (call 2 from crmd.14306) for host 'PCTAAL01SL-HB' with device 'ipmilan_stonith1' returned: 0 (OK)<br><br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-17T02:46:58Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OipmhQAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-14T07:59:56Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-14T07:59:56Z</b><br><br>답변 감사합니다.<br>이해가 안되는 부분이 있어 말씀드립니다.<br>아래 로그가 토큰 타임아웃이라고 말씀하셨는데요.<br><br>Dec  4 12:09:44 PCTAAL02SL corosync[14285]: [TOTEM ] A processor failed, forming new configuration.<br> <br>아래 로그는 fencing 로그입니다. <br>Dec  4 12:07:20 PCTAAL02SL fence_kdump[13549]: waiting for message from 'xxx.xx.xx.151'<br>Dec  4 12:09:20 PCTAAL02SL stonith-ng[14302]:  notice: Child process 13549 performing action 'off' timed out with signal 15<br><br>토큰 타임아웃이 발생하기 전에 팬싱 시그널이 발생되지는 않을것 같습니다.<br><br>이에 아래의 질문에 답변 부탁드립니다.<br><br>1. 최초 토큰 타임아웃이 발생된 시간이 몇시인가요?<br>2. 토큰 타임아웃이 발생된 로그에 대해서 말씀 주세요... <br>3. 어떠한 부분을보고 토탬타임아웃이 발생되었다고 말씀주시는지요?<br><br>3개의 질문에 대한 답변 꼭 부탁드립니다.<br><br><publishedDate>2018-12-14T07:59:56Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OinqTQAR"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-14T04:25:43Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-14T05:51:55Z</b><br><br>안녕하세요, 레드햇의 김마현입니다.<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br>문의하신 내용에 대하여 안내를 드리면 다음과 같습니다.<br><br>&gt;&gt; 1. 아래 메시지가 token timeout을 의미하는것이 맞는지요? 아니라면 어떠한 메시지인가요? <br>&gt;&gt; Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br><br>해당 메시지는 token timeout과 관련된 메시지가 아닙니다.<br>cib.xml 에 정의(하기와 같이 정의되어 있음)된데로 리소스 모니터링(ping을 통한 host 체크)에 의해 발생한 것으로 token timeout과 무관한 메시지 입니다.<br>&lt;op id=&quot;ping-monitor-interval-5s&quot; interval=&quot;5s&quot; name=&quot;monitor&quot;/&gt;<br><br>그리고 token timeout 관련 메시지는 B 노드에서 하기와 같이 한번만 로깅된 것으로 확인됩니다.<br>Dec  4 12:09:44 PCTAAL02SL corosync[14285]: [TOTEM ] A processor failed, forming new configuration.<br><br><br>감사합니다.<br>김마현 드림.<br><br><publishedDate>2018-12-14T04:25:43Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OimfoQAB"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-14T01:23:17Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-14T01:23:17Z</b><br><br>답변 감사합니다.<br>12시 07분 20초 전의 로그에 대해서 묻고 싶습니다.<br>아래와 같이 <br>Dec  4 12:07:20 PCTAAL02SL fence_kdump[13549]: waiting for message from 'xxx.xx.xx.151'<br>Dec  4 12:09:20 PCTAAL02SL stonith-ng[14302]:  notice: Child process 13549 performing action 'off' timed out with signal 15<br><br><br>kdump fencing을 기다리고 있었고 이전에 토큰 타임아웃 되었다는 메시지가 어떤것인지 확인하고 싶습니다.<br>아래와 같이 로그를 발생시켰었는데요...<br><br>Dec 04 12:06:06 [5318] PCTAAL01SL       lrmd:  warning: child_timeout_callback:<br>ping_monitor_5000 process (PID 15940) timed out<br>Dec 04 12:06:11 [5318] PCTAAL01SL       lrmd:  warning: operation_finished:<br>ping_monitor_5000:15940 - timed out after 20000ms<br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br><br>12시 06분 11초 에서 12분 07분 20초 사이에 어떠한 이벤트가 발생했었는지 문의 드립니다.<br><br>1. 아래 메시지가 token timeout을 의미하는것이 맞는지요? 아니라면 어떠한 메시지인가요? <br>Dec 04 12:06:11 [5321] PCTAAL01SL       crmd:    error: process_lrm_event:<br>Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out  call=54 key=ping_monitor_5000 timeout=20000ms<br><br>2.  맞다고 하면 kdump fencing을 진입하는데 1분 15초가 걸렸는지 설명 부탁드립니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-14T01:23:17Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OiXNpQAN"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-13T07:15:17Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-13T08:04:22Z</b><br><br>안녕하세요, 레드햇의 김마현입니다.<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br>문의하신 내용에 대하여 안내를 드리면 다음과 같습니다.<br><br><br>- 직전에 답변 드렸듯, 특정 노드가 token 메시지를 보내지 못하여 token timeout 상태가되면 상대측 클러스터 로그에 하기 메시지가 찍힙니다.<br>'A processor failed, forming new configuration'<br><br><br>- 노드 B의 corosync 로그를 확인해보면 관련된 로그를 하기와 같이 확인할 수 있습니다.<br>Dec  4 12:09:44 PCTAAL02SL corosync[14285]: [TOTEM ] A processor failed, forming new configuration.<br>Dec  4 12:09:47 PCTAAL02SL stonith-ng[14302]:  notice: Operation 'reboot' [16561] (call 2 from crmd.14306) for host 'PCTAAL01SL-HB' with device 'ipmilan_stonith1' returned: 0 (OK)<br><br><br>- 위 로그를 기준으로 token timeout과 펜싱 과정 설명<br>1) 노드 B는 노드 A로 부터 5초간 토큰 메시지를 수신 받지 못 해서 12:09:44에 'A processor failed, forming new configuration.' 메시지를 로깅<br>   -&gt; 메시지가 찍히기 5초전부터가 token 메시지를 수진 받지 못한 시발점으로 볼 수 있습니다.<br><br>2) 노드 B는 데이터 정합성을 위해 노드 A를 펜싱(ipmi를 통해서)하고 12:09:47에 펜싱 성공 메시지를 로깅<br><br><br>- token timeout 설정값<br>sosreport 를 통해 5000ms(5초)로 확인 됩니다.<br>$ cat sosreport-PCTAAL01SL-20181204123839/etc/corosync/corosync.conf| grep totem -A 8<br>totem {<br>    version: 2<br>    secauth: off<br>    cluster_name: CTA_AP_cluster<br>    transport: udpu<br>    token: 5000<br>    rrp_mode: passive<br>}<br><br><br>- 노드 A의 토큰 통신 실패(CPU 부하 원인)로 노드 B가 펜싱을 통해 노드 A를 클러스터 그룹에서 차단하고 노드 B에 리소스를 구동한 케이스입니다.<br>  따라서 노드 A에서의 리소스 stop 진행과는 무관합니다.<br><br><br>감사합니다.<br>김마현 드림<br><br><publishedDate>2018-12-13T07:15:16Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OiUs0QAF"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-13T02:32:00Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-13T02:32:00Z</b><br><br>답변 감사합니다.<br>좀더 구체적인 설명을 듣고 싶어 문의 드립니다.<br><br>1번 답에 대해 아래와 같은 로그 시간별로 설명 부탁드리겠습니다.<br>-  token fail이 발생한 시간에 대한 시간로그<br>- token timeout 이 발생한 시간에 대한 시간로그<br>- token timeout 발생후 리소스가 stop 되는 시간로그<br>- 리소스 stop이 진행되지 않아 fencing이 진행되는 시간로그<br><br>확인 부탁드리겠습니다.<br><br><publishedDate>2018-12-13T02:32:00Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OiCccQAF"><br>======================<br><b>생성계정 : Kim, Mahyun</b><br><b>생성날짜 : 2018-12-12T01:02:31Z</b><br><b>마지막 답변자 : Kim, Mahyun</b><br><b>마지막 수정 일자 : 2018-12-12T01:07:50Z</b><br><br>안녕하세요, 레드햇 김마현입니다.<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br>문의하신 내용에 대하여 안내를 드리면 다음과 같습니다.<br><br>&gt; 1. CPU 부하가 발생할 경우 토큰통신 실패일때 어떤과정을 거쳐서 페일오버 절차가 이루어 지는지요?<br>&gt; 예) token 통신주기, token timeout , token timeout 이후 이루어 지는 절차<br><br>일반적으로 토큰 손실에 대한 반응(펜싱)은 토큰 손실의 원인(네트워크 문제, 높은로드, 커널 패닉 등)에 상관없이 항상 동일합니다.<br>해당 케이스와 같이 2노드 클러스터 구성에서 노드 A에서 클러스터 통신에 방해가 되는 CPU 문제가 있을 경우,<br>노드 B는 노드 A가 응답 할 수 없는 이유를 알지 못하지만 데이터 정합성 유지를 위해 다른 노드가 응답해야 할 때까지의 최대 시간(token timeout)을 정의합니다.<br>노드 A가 token timeout 시간내에 reply를 하지 않으면 토큰을 손실한 것으로 간주하고 노드 B가 노드 A에 대해서 펜싱을 시도하게 됩니다.<br>이때 다른 노드에 대한 fencing 이 성공한 것이 확인된 후에만 페일오버가 시작되게 됩니다.<br>만약 fencing 이 실패되면 데이터 정합성을 보장할 수 없기 때문에 페일오버 처리가 시작되지 않습니다. <br><br>관련 링크:<br>How to change totem token timeout value in a RHEL 5, 6, or 7 High Availability cluster?<br>  https://access.redhat.com/solutions/221263<br><br><br>&gt; 2. 토큰 통신이 문제일 경우 어디서 해당 로그를 살펴 볼수 있는지요<br>corosync[XXXXX]: [TOTEM ] A processor failed, forming new configuration.<br>토근 통신이 문제가 있을 경우 일반적으로 상기 로그를 통해 알 수 있습니다.<br><br>관련 링크:<br>A node was fenced after &quot;A processor failed, forming new configuration&quot; in a RHEL 6 or 7 High Availability cluster<br>  https://access.redhat.com/solutions/122293<br><br><br>감사합니다.<br>김마현 드림<br><br><publishedDate>2018-12-12T01:02:31Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000Ohw6AQAR"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-12-11T04:53:06Z</b><br><b>마지막 답변자 : Shin, Jake Jaewook</b><br><b>마지막 수정 일자 : 2018-12-11T04:53:05Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>현재 응답 드릴 내용을 정리중에 있습니다.<br>정리가 끝나는 대로 업데이트 드리도록 하겠습니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-11T04:53:05Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OheBZQAZ"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-10T04:59:06Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-10T04:59:06Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>아직 로그 분석중에 있습니다. 분석을 마치는데로 다시 업데이트 드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2018-12-10T04:59:06Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OQXN1QAP"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-07T06:32:06Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-07T06:32:06Z</b><br><br>보내주신 자료 검토하여 업데이트 드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2018-12-07T06:32:06Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OQWWRQA5"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-07T04:44:33Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-07T04:44:33Z</b><br><br>담당자 요청으로 케이스 다시 오픈 합니다.<br>요청하신 2호기 로그를 보내드립니다.<br>추가적인 질문사항은 다음과 같습니다.<br><br>1. CPU 부하가 발생할 경우 토큰통신 실패일때  어떤과정을 거쳐서 페일오버 절차가 이루어 지는지요?<br>예) token 통신주기, token timeout , token timeout 이후 이루어 지는 절차<br><br><br>2. 토큰 통신이 문제일 경우 어디서 해당 로그를 살펴 볼수 있는지요?<br><br><publishedDate>2018-12-07T04:44:33Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OQViqQAH"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-07T02:38:14Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-07T02:38:14Z</b><br><br>확인 감사합니다.<br><br>케이스를 종료하겠습니다.<br><br><br>(In reply to KIM, JIMIN)<br>&gt; 안녕하세요<br>&gt; <br>&gt; SDS 김지민 선임입니다. <br>&gt; <br>&gt; 해당 건은 application 담당자의 작업 실수로 <br>&gt; 부하가 걸려서 문제의 원인이 된 것으로 확인이 되었습니다.<br>&gt; <br>&gt; 저도 작업 수행 상세 내역은 공유를 받지 못해서 <br>&gt; 해당 내역으로만 알려드립니다.<br>&gt; <br>&gt; 이번 문의 건은 종료해주셔도 될 것 같습니다.<br>&gt; <br>&gt; 감사합니다.<br><br><publishedDate>2018-12-07T02:38:14Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OQVZAQA5"><br>======================<br><b>생성계정 : KIM, JIMIN</b><br><b>생성날짜 : 2018-12-07T02:14:59Z</b><br><b>마지막 답변자 : KIM, JIMIN</b><br><b>마지막 수정 일자 : 2018-12-07T02:14:59Z</b><br><br>안녕하세요<br><br>SDS 김지민 선임입니다. <br><br>해당 건은 application 담당자의 작업 실수로 <br>부하가 걸려서 문제의 원인이 된 것으로 확인이 되었습니다.<br><br>저도 작업 수행 상세 내역은 공유를 받지 못해서 <br>해당 내역으로만 알려드립니다.<br><br>이번 문의 건은 종료해주셔도 될 것 같습니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-07T02:14:59Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OQDVLQA5"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-06T02:29:06Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-06T02:29:06Z</b><br><br>안녕하세요.<br>dropbox 에 파일명 : PCTAAL01SL_sa04 업로드 했습니다.<br>감사합니다.<br><br><publishedDate>2018-12-06T02:29:06Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OQCQtQAP"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-06T00:28:21Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-06T00:28:21Z</b><br><br>전달해주신 sosreport 파일에 있는 sar 파일이 12월 1일자 데이터만 포함되어 있습니다.<br>문제 발생 시점의 데이터가 있는지 확인하셔서 전달해주시기 바랍니다.<br><br>(In reply to 타임게이트, 타임게이트)<br>&gt; 안녕하세요. 답변 감사합니다~<br>&gt; 생성된 vmcore 는 없다는 담당자의 답변이 있었습니다.<br>&gt; 시스템 부하로 인한 토큰값 손실 관련 sar 분석 부탁드립니다.<br>&gt; 감사합니다.<br><br><publishedDate>2018-12-06T00:28:21Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OPudYQAT"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-05T00:48:28Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-05T00:48:27Z</b><br><br>안녕하세요. 답변 감사합니다~<br>생성된 vmcore 는 없다는 담당자의 답변이 있었습니다.<br>시스템 부하로 인한 토큰값 손실 관련 sar 분석 부탁드립니다.<br>감사합니다.<br><br><publishedDate>2018-12-05T00:48:27Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OPfCWQA1"><br>======================<br><b>생성계정 : Shin, Jake Jaewook</b><br><b>생성날짜 : 2018-12-04T06:58:43Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-04T07:06:19Z</b><br><br>안녕하세요,<br><br>Red Hat Global Support Services 를 이용해주셔서 감사합니다.<br><br>저는 Platform Support 를 담당하고 있는<br>Senior Technical Support Engineer 신재욱입니다.<br><br>올려주신 sosreport 에 대해 확인한 결과를 말씀드리면 다음과 같습니다.<br><br>현재 2호기의 로그가 없어 정확한 펜싱 여부가 확인되진 않습니다만 1호기의 로그상, 12:08:01 직후에 리부팅이 발생한 것으로 보입니다.<br>====<br>Dec  4 12:07:20 PCTAAL01SL su: pam_unix(su-l:session): session closed for user ctmag901<br>Dec  4 12:07:20 PCTAAL01SL su: pam_unix(su-l:session): session closed for user ctmag901<br>Dec  4 12:07:20 PCTAAL01SL su: pam_unix(su-l:session): session closed for user ctmag901<br>Dec  4 12:07:26 PCTAAL01SL crmd[5321]:  notice: High CPU load detected: 26.000000<br>Dec  4 12:08:01 PCTAAL01SL CROND[19094]: (root) CMD (/usr/lib64/sa/sa1 1 1)<br>Dec  4 12:08:01 PCTAAL01SL CROND[19095]: (root) CMD (/sysadmin/nfs_monitor/nfsiostat.sh &gt; /dev/null 2&gt;&amp;1)<br>Dec  4 12:08:01 PCTAAL01SL CROND[19096]: (root) CMD (/bin/sh /sysadmin/syscheck/healthcheck_dstat.sh &gt; /dev/null 2&gt;&amp;1)<br>Dec  4 12:13:29 PCTAAL01SL kernel: Initializing cgroup subsys cpuset<br>Dec  4 12:13:29 PCTAAL01SL kernel: Initializing cgroup subsys cpu<br>Dec  4 12:13:29 PCTAAL01SL kernel: Initializing cgroup subsys cpuacct<br>Dec  4 12:13:29 PCTAAL01SL kernel: Linux version 3.10.0-327.53.1.el7.x86_64 (mockbuild@x86-037.build.eng.bos.redhat.com) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) ) #1 SMP Tue Mar 14 10:49:09 EDT 2017<br>Dec  4 12:13:29 PCTAAL01SL kernel: Command line: BOOT_IMAGE=/vmlinuz-3.10.0-327.53.1.el7.x86_64 root=UUID=b1424063-2343-4510-b6ab-e6e979f9724b ro crashkernel=512M rhgb quiet LANG=en_US.UTF-8 nmi_watchdog=0 transparent_hugepage=never elevator=deadline rdloaddriver=hpsa rdloaddriver=qla2xxx nosoftlockup intel_idle.max_cstate=0 processor.max_cstate=1 intel_pstate=disable mce=ignore_ce<br>Dec  4 12:13:29 PCTAAL01SL kernel: e820: BIOS-provided physical RAM map:<br>Dec  4 12:13:29 PCTAAL01SL kernel: BIOS-e820: [mem 0x0000000000000000-0x0000000000093fff] usable<br>Dec  4 12:13:29 PCTAAL01SL kernel: BIOS-e820: [mem 0x0000000000094000-0x000000000009ffff] reserved<br>Dec  4 12:13:29 PCTAAL01SL kernel: BIOS-e820: [mem 0x00000000000e0000-0x00000000000fffff] reserved<br>Dec  4 12:13:29 PCTAAL01SL kernel: BIOS-e820: [mem 0x0000000000100000-0x000000005a344fff] usable<br>Dec  4 12:13:29 PCTAAL01SL kernel: BIOS-e820: [mem 0x000000005a345000-0x000000005a384fff] reserved<br>====<br><br>또한, 이슈 발생시점에 시스템 부하가 몇차례 확인됩니다.<br>====<br>Dec 04 12:06:56 [5321] PCTAAL01SL       crmd:   notice: throttle_handle_load:<br>High CPU load detected: 29.770000<br>Dec 04 12:07:26 [5321] PCTAAL01SL       crmd:   notice: throttle_handle_load:<br>High CPU load detected: 26.000000<br>====<br><br>해당 메시지에 대해서 다음과 같이 보고되어 있습니다.<br><br>What does the following pacemaker message mean? &quot;crmd: notice: throttle_handle_load: High CPU load detected&quot; <br>  https://access.redhat.com/solutions/2206831<br><br>즉, 현재와 같이 시스템 부하가 발생할 경우 토큰 통신이 실패할 수 있으며 결과로서 현재와 같이 시스템 리부팅이 발생할 수 있습니다.<br>펜싱에 의한 시스템 리부팅의 경우, fence_kdump 가 설정되어 있으면 해당 시점의 시스템 상황을 파악할 수 있는 vmcore 가 수집됩니다.<br><br>현재 클러스터 설정을 보면 다음과 같이 fence_kdump 설정은 확인됩니다.<br>====<br>Stonith Devices:<br> Resource: kdump_stonith (class=stonith type=fence_kdump)<br>  Attributes: pcmk_host_check=static-list pcmk_monitor_action=metadata pcmk_status_action=metadata pcmk_reboot_action=off pcmk_host_list=&quot;PCTAAL01SL-HB PCTAAL02SL-HB&quot; pcmk_off_timeout=120s timeout=120s<br>  Operations: monitor interval=60s (kdump_stonith-monitor-interval-60s)<br> Resource: ipmilan_stonith1 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_list=PCTAAL01SL-HB ipaddr=172.18.24.103 login=Redhat passwd=nl!con00 lanplus=on auth=password delay=15 action=reboot<br>  Operations: monitor interval=60s (ipmilan_stonith1-monitor-interval-60s)<br> Resource: ipmilan_stonith2 (class=stonith type=fence_ipmilan)<br>  Attributes: pcmk_host_list=PCTAAL02SL-HB ipaddr=172.18.24.104 login=Redhat passwd=nl!con00 lanplus=on auth=password action=reboot<br>  Operations: monitor interval=60s (ipmilan_stonith2-monitor-interval-60s)<br>Fencing Levels:<br> Node: PCTAAL01SL-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith1<br> Node: PCTAAL02SL-HB<br>  Level 1 - kdump_stonith<br>  Level 2 - ipmilan_stonith2<br>==== <br><br>또한,kdump.conf 설정을 보면 시스템이 펜싱될 경우에 vmcore 가 다음 경로에 저장되도록 설정되어 있습니다. <br>====<br>  kdump.conf:<br>    raw /dev/vg9/DUMP<br>====<br><br>따라서, 우선 원인 파악을 위해 상기 디바이스에 vmcore 가 저장되어 있는지 확인후, 저장되어 있을 경우<br>업로드해 주시기 바랍니다.<br><br>참고로 다음 링크를 통해 vmcore 의 업로드 방법을 확인하실 수 있습니다.(일반적으로 FTP 부분을 참조하시면 됩니다.)<br><br>How to provide files to Red Hat Support (vmcore, rhev logcollector, sosreports, heap dumps, log files, etc.)<br>  https://access.redhat.com/solutions/2112<br><br>마지막으로 이슈 발생시점에 고객사에서 시스템 부하를 초래할 작업이 있었는지도 확인해보실 필요가 있습니다.<br>만약 시스템에 걸리는 부하가 시스템의 성능을 초과한다면 스케일업 또는 작업개선을 통해 시스템 부하를 낮출 필요가 있습니다.<br><br>감사합니다.<br><br><publishedDate>2018-12-04T06:58:43Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OPeSEQA1"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-04T05:45:23Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-04T05:45:23Z</b><br><br>안녕하세요. 드랍박스 sosreport 링크 부탁 드립니다.<br>감사합니다.<br><br><publishedDate>2018-12-04T05:45:23Z</publishedDate><createdByType>Customer</createdByType><br>======================<br><comment id="a0a2K00000OPeERQA1"><br>======================<br><b>생성계정 : Huh, Kyung</b><br><b>생성날짜 : 2018-12-04T05:22:57Z</b><br><b>마지막 답변자 : Huh, Kyung</b><br><b>마지막 수정 일자 : 2018-12-04T05:22:56Z</b><br><br>안녕하세요? 레드햇 허경입니다.<br><br>올려주신 sosreport 파일 내용 확인 후 업데이트 드리겠습니다.<br><br>고맙습니다.<br>허 경 드림.<br><br><publishedDate>2018-12-04T05:22:56Z</publishedDate><createdByType>Associate</createdByType><br>======================<br><comment id="a0a2K00000OPdvEQAT"><br>======================<br><b>생성계정 : 타임게이트, 타임게이트</b><br><b>생성날짜 : 2018-12-04T04:48:13Z</b><br><b>마지막 답변자 : 타임게이트, 타임게이트</b><br><b>마지막 수정 일자 : 2018-12-04T04:48:13Z</b><br><br>해당 시당대 메세지 입니다.<br><br>Dec  4 12:06:06 PCTAAL01SL lrmd[5318]: warning: PCTA1_vip_monitor_5000 process (PID 15907) timed out<br>Dec  4 12:06:16 PCTAAL01SL lrmd[5318]: warning: ping_monitor_5000 process (PID 15940) timed out<br>Dec  4 12:06:16 PCTAAL01SL lrmd[5318]: warning: ping_monitor_5000:15940 - timed out after 20000ms<br>Dec  4 12:06:16 PCTAAL01SL crmd[5321]:   error: Result of monitor operation for ping on PCTAAL01SL-HB: Timed Out #007 call=54 key=ping_monitor_5000 timeout=20000ms<br>Dec  4 12:06:41 PCTAAL01SL systemd-logind: Failed to abandon session scope: Connection timed out<br>Dec  4 12:06:50 PCTAAL01SL lrmd[5318]: warning: PCTA1_vip_monitor_5000:15907 - timed out after 10000ms<br>Dec  4 12:06:50 PCTAAL01SL lrmd[5318]: warning: VG_SRPCTAAP3VG_monitor_10000 process (PID 15908) timed out<br>Dec  4 12:06:50 PCTAAL01SL lrmd[5318]: warning: LV_APP_con_ctmag901_monitor_5000 process (PID 15926) timed out<br>Dec  4 12:06:50 PCTAAL01SL crmd[5321]:   error: Result of monitor operation for PCTA1_vip on PCTAAL01SL-HB: Timed Out #007 call=60 key=PCTA1_vip_monitor_5000 timeout=10000ms<br>Dec  4 12:06:50 PCTAAL01SL su: pam_rootok(su-l:auth): root check succeeded<br>Dec  4 12:06:50 PCTAAL01SL su: (to ctmag901) root on none<br>Dec  4 12:06:50 PCTAAL01SL su: pam_unix(su-l:session): session opened for user ctmag901 by (uid=0)<br>Dec  4 12:06:50 PCTAAL01SL cib[5316]:   error: Error in connection setup (5316-16086-14): Broken pipe (32)<br>Dec  4 12:06:50 PCTAAL01SL lrmd[5318]: warning: VG_SRPCTAAP3VG_monitor_10000:15908 - timed out after 30000ms<br>Dec  4 12:06:50 PCTAAL01SL lrmd[5318]: warning: LV_APP_con_ctmag901_monitor_5000:15926 - timed out after 30000ms<br>Dec  4 12:06:50 PCTAAL01SL crmd[5321]:   error: Result of monitor operation for VG_SRPCTAAP3VG on PCTAAL01SL-HB: Timed Out #007 call=56 key=VG_SRPCTAAP3VG_monitor_10000 timeout=30000ms<br>Dec  4 12:06:51 PCTAAL01SL crmd[5321]:  notice: PCTAAL01SL-HB-VG_SRPCTAAP3VG_monitor_10000:56 [ volume_list=[&quot;vg0&quot;,&quot;vg9&quot;,&quot;LRPCTAAPPVG&quot;,&quot;@PCTAAL01SL-HB&quot;]\n ]<br>Dec  4 12:06:51 PCTAAL01SL crmd[5321]:   error: Result of monitor operation for LV_APP_con_ctmag901 on PCTAAL01SL-HB: Timed Out #007 call=58 key=LV_APP_con_ctmag901_monitor_5000 timeout=30000ms<br>Dec  4 12:06:51 PCTAAL01SL stonith-ng[5317]:  notice: Child process 16044 performing action 'monitor' timed out with signal 9<br>Dec  4 12:06:51 PCTAAL01SL stonith-ng[5317]:  notice: Operation 'monitor' [16044] for device 'ipmilan_stonith2' returned: -62 (Timer expired)<br>Dec  4 12:06:51 PCTAAL01SL su: pam_rootok(su-l:auth): root check succeeded<br>Dec  4 12:06:51 PCTAAL01SL crmd[5321]:   error: Result of monitor operation for ipmilan_stonith2 on PCTAAL01SL-HB: Timed Out #007 call=53 key=ipmilan_stonith2_monitor_60000 timeout=20000ms<br>Dec  4 12:07:08 PCTAAL01SL crmd[5321]:  notice: High CPU load detected: 29.770000<br>Dec  4 12:07:08 PCTAAL01SL lrmd[5318]: warning: PCTA1_vip_monitor_5000 process (PID 16126) timed out<br>Dec  4 12:07:08 PCTAAL01SL lrmd[5318]: warning: PCTA1_vip_monitor_5000:16126 - timed out after 10000ms<br>Dec  4 12:07:08 PCTAAL01SL lrmd[5318]: warning: ping_stop_0 process (PID 16065) timed out<br>Dec  4 12:07:14 PCTAAL01SL lrmd[5318]:    crit: ping_stop_0 process (PID 16065) will not die!<br>Dec  4 12:07:20 PCTAAL01SL lrmd[5318]: warning: ping_stop_0:16065 - timed out after 20000ms<br>Dec  4 12:07:20 PCTAAL01SL crmd[5321]:   error: Result of stop operation for ping on PCTAAL01SL-HB: Timed Out #007 call=77 key=ping_stop_0 timeout=20000ms<br><br><publishedDate>2018-12-04T04:48:13Z</publishedDate><createdByType>Customer</createdByType><br>======================<br></comments><br>